<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch-data-and-programming-foundations">
  <title>Data and Programming Foundations</title>

  <introduction>
    <p>This unit introduces the data foundations as well as the computational tools in programming needed to work effectively with data. You'll learn the fundamentals of R programming using the Web-R platform, including data structures, importing and exploring datasets, and techniques for data transformation.</p>
  </introduction>

 
  <!-- TOPIC 5: INTRODUCTION TO WEB-R -->
  <section xml:id="sec-intro-to-web-r">
    <title>Introduction to Web-R</title>
    
    <!-- Session 9: Getting started with R -->
    <subsection xml:id="subsec-r-environment">
      <title>R environment overview and basic syntax</title>
      
      <p>R is a powerful programming language and environment specifically designed for statistical computing and data analysis. Web-R provides browser-based access to R without requiring local installation.</p>
      
      <p>Key components of the R environment include:</p>
      <ul>
        <li>
          <p><term>Console</term>: Where commands are entered and executed</p>
        </li>
        <li>
          <p><term>Script Editor</term>: For writing and saving R code files</p>
        </li>
        <li>
          <p><term>Environment</term>: Shows variables and objects in memory</p>
        </li>
        <li>
          <p><term>Packages</term>: Extensions that add functionality to base R</p>
        </li>
        <li>
          <p><term>Help</term>: Documentation and assistance for R functions</p>
        </li>
      </ul>
      
      <p>R syntax fundamentals:</p>
      <ul>
        <li>
          <p>Case sensitivity: R distinguishes between uppercase and lowercase letters</p>
        </li>
        <li>
          <p>Assignment: Use <c>&lt;-</c> to assign values to variables (though <c>=</c> also works)</p>
        </li>
        <li>
          <p>Comments: Begin with <c>#</c> and are not executed</p>
        </li>
        <li>
          <p>Function calls: <c>function_name(arg1, arg2)</c></p>
        </li>
        <li>
          <p>Indexing: Use square brackets <c>[]</c> to access elements</p>
        </li>
      </ul>
      
      <sage language="r">
        <input>
          # This is a comment - R ignores everything after the # symbol
          
          # Variable assignment
          x &lt;- 10   # Assign the value 10 to variable x
          y &lt;- 5    # Assign the value 5 to variable y

          # Basic arithmetic operations
          2 + 3    # Addition
          10 - 4   # Subtraction
          5 * 6    # Multiplication
          20 / 4   # Division
          2^3      # Exponentiation
          10 %% 3  # Modulo (remainder)
            #Note that if you click Execute or Run, 
            #no output will appear because they weren't told to print!
            #So that you understand what the operators are doing,
            #assign each line a different variable (ex:a,b,c...)
            #then underneath tell it to print(insert variable)

          
          # Perform operations with variables
          x + y
          x * y
          x / y
          
          # Creating a more complex expression
          result &lt;- (x + y) * 2
          result
          
          # Boolean operations
          x &gt; y    # Greater than
          x &lt; y    # Less than
          x == y   # Equal to
          x != y   # Not equal to
          
          # Getting help
          # To see the below execute, you need to uncomment them!
          # ?mean   # Opens help for the mean function
        </input>
      </sage>
      
      <exercise xml:id="exercise-r-syntax-matching">
  <title>R Syntax Fundamentals</title>
  <statement>
    <p>Match each R code example on the left with its correct description on the right.</p>
  </statement>
  <cardsort>
  <match>
  <premise order="6">Assignment operator creating a variable x with value 10</premise>
  <response>x&lt;-10</response>
  </match>
  <match>
  <premise order="3">Accessing the third element of a vector or list called names</premise>
  <response>names[3]</response>
  </match>
  <match>
  <premise order="1">A comment that R ignores during execution</premise>
  <response>#Calculate Mean</response>
  </match>
  <match>
  <premise order="10">Testing if two variables have equal values</premise>
  <response>total==sum</response>
  </match>
  <match>
  <premise order="5">An operation that will return a remainder of (1)</premise>
  <response>5%%2</response>
  </match>
  <match>
  <premise order="9">Creating a complex expression that adds two variables and multiplies by 2</premise>
  <response>&lt;- (x + y) * 2</response>
  </match>
  <match>
  <premise order="4">Function call with two arguments to calculate an average while removing missing values</premise>
  <response>mean(data, na.rm=TRUE)</response>
  </match>
  <match>
  <premise order="2">Testing if a variable is not equal to zero</premise>
  <response>Value != </response>
  </match>
  <match>
  <premise order="8">Requesting help documentation for a function</premise>
  <response>?mean</response>
  </match>
  <match>
  <premise order="7">Creating two different variables due to case sensitivity</premise>
  <response>Count &lt;- count</response>
  </match>
  </cardsort>   
  <solution>
    <p>Correct matches:</p>
    <ul>
      <li>x &lt;- 10: Assignment operator creating a variable x with value 10</li>
      <li>names[3]: Accessing the third element of a vector or list called names</li>
      <li># Calculate average: A comment that R ignores during execution</li>
      <li>total == sum: Testing if two variables have equal values</li>
      <li>5 %% 2: Modulo operation returning the remainder (1)</li>
      <li>result &lt;- (x + y) * 2: Creating a complex expression that adds two variables and multiplies by 2</li>
      <li>mean(data, na.rm=TRUE): Function call with two arguments to calculate an average while removing missing values</li>
      <li>Value != 0: Testing if a variable is not equal to zero</li>
      <li>?mean: Requesting help documentation for a function</li>
      <li>Count &lt;- count: Creating two different variables due to case sensitivity</li>
    </ul>
  </solution>
</exercise>
   </subsection>
   
   <!-- Variables, data types, and basic operations -->
   <subsection xml:id="subsec-variables-datatypes">
     <title>Variables, data types, and basic operations</title>
     
     <p>R supports various data types for different kinds of information:</p>
     
     <table xml:id="table-r-data-types">
       <title>Common R Data Types</title>
       <tabular halign="center">
         <row header="yes">
           <cell>Data Type</cell>
           <cell>Description</cell>
           <cell>Example</cell>
         </row>
         <row>
           <cell>Numeric</cell>
           <cell>Real numbers</cell>
           <cell><c>42.5</c></cell>
         </row>
         <row>
           <cell>Integer</cell>
           <cell>Whole numbers</cell>
           <cell><c>42L</c> (the L specifies integer)</cell>
         </row>
         <row>
           <cell>Character</cell>
           <cell>Text strings</cell>
           <cell><c>"Hello"</c> or <c>'World'</c></cell>
         </row>
         <row>
           <cell>Logical</cell>
           <cell>Boolean values</cell>
           <cell><c>TRUE</c> or <c>FALSE</c></cell>
         </row>
         <row>
           <cell>Factor</cell>
           <cell>Categorical variables</cell>
           <cell><c>factor(c("small", "medium", "large"))</c></cell>
         </row>
         <row>
           <cell>Date</cell>
           <cell>Calendar dates</cell>
           <cell><c>as.Date("2025-05-10")</c></cell>
         </row>
       </tabular>
     </table>
     
     <p>Key operations and functions for working with variables include:</p>
     
     <ul>
       <li>
         <p><term>Type checking</term>: <c>class()</c>, <c>typeof()</c>, <c>is.numeric()</c>, <c>is.character()</c>, etc.</p>
       </li>
       <li>
         <p><term>Type conversion</term>: <c>as.numeric()</c>, <c>as.character()</c>, <c>as.factor()</c>, etc.</p>
       </li>
       <li>
         <p><term>Variable information</term>: <c>length()</c>, <c>str()</c>, <c>summary()</c></p>
       </li>
       <li>
         <p><term>Mathematical operations</term>: <c>+</c>, <c>-</c>, <c>*</c>, <c>/</c>, <c>^</c>, <c>%%</c> (modulo), <c>%/%</c> (integer division)</p>
       </li>
       <li>
         <p><term>Logical operations</term>: <c>==</c>, <c>!=</c>, <c>&lt;</c>, <c>&gt;</c>, <c>&lt;=</c>, <c>&gt;=</c>, <c>&amp;</c> (and), <c>|</c> (or), <c>!</c> (not)</p>
       </li>
     </ul>
     
     <sage language="r">
      <input>
         # Creating variables of different data types
# Numeric
num_var &lt;- 42.5
print(class(num_var))

# Integer
int_var &lt;- 42L  
print(class(int_var))  
is.integer(int_var)  

# Character
char_var &lt;- "Hello, R!"
print(class(char_var))  
nchar(char_var)  

# Logical
log_var &lt;- TRUE
print(class(log_var))  
!log_var  # Logical NOT

# Creating a factor (categorical variable)
sizes &lt;- factor(c("small", "medium", "large", "medium", "small"))
print(class(sizes))  
print(levels(sizes))  
print(table(sizes))  # Shows frequency of each category

# Date
date_var &lt;- as.Date("2025-05-10")
print(class(date_var))  
print(format(date_var, "%A, %B %d, %Y"))

# Type conversion examples
# Character to numeric
char_num &lt;- "42.5"
print(class(char_num))  # Displays: "character"
num_converted &lt;- as.numeric(char_num)
print(class(num_converted))  # Displays: "numeric"

# Numeric to character
num_char &lt;- 42.5
char_converted &lt;- as.character(num_char)
print(char_converted)  
print(class(char_converted)) 

# Vector operations
vec1 &lt;- c(1, 3, 5, 7, 9, 11)
print(vec1) 
print(mean(vec1))  
print(sum(vec1))  
print(length(vec1))  # Number of elements

# Logical operations
x &lt;- 10
y &lt;- 5
print(paste("x =", x, "and y =", y))  # Shows values for context

# Is x greater than y?
result1 &lt;- x &gt; y
print(paste("Is x &gt; y?", result1))  

# Is x equal to 2 times y?
result2 &lt;- x == 2*y
print(paste("Is x == 2*y?", result2)) 

# Both conditions must be TRUE
result3 &lt;- (x &gt; 5) &amp;&amp; (y &lt; 10)  
print(paste("Is (x &gt; 5) AND (y &lt; 10)?", result3))

# Simple function example
square &lt;- function(x) {
  return(x^2)
}
print(square(4)) 
print(square(1:5))  # Applies to each element within set

# Demonstrating if statements
test_number &lt;- function(x) {
  if (x &gt; 0) {
    return("Positive")
  } else if (x &lt; 0) {
    return("Negative")
  } else {
    return("Zero")
  }
}

print(test_number(10)) 
print(test_number(-5))
print(test_number(0))
       </input>
      </sage>
     
     <exercise xml:id="ex-data-types">
       <title>Data Types Practice</title>
       <statement>
         <p>For each of the following R expressions, predict what data type the result will be:</p>
       </statement>
       <matches>
         <match>
           <premise><c>"42" + 8</c></premise>
           <response>error (can't add a number to a character string)</response>
         </match>
         <match>
           <premise><c>as.numeric("42") + 8</c></premise>
           <response>numeric</response>
         </match>
         <match>
           <premise><c>10 &gt; 5</c></premise>
           <response>logical</response>
         </match>
         <match>
           <premise><c>c("red", "green", "blue")</c></premise>
           <response>character vector</response>
         </match>
         <match>
           <premise><c>as.Date("2025-01-01") + 30</c></premise>
           <response>Date</response>
         </match>
       </matches>
     </exercise>
   </subsection>
   
   <!-- Hands-on practice: Simple calculations and variable assignments -->
   <subsection xml:id="subsec-r-practice">
     <title>Hands-on practice: Simple calculations and variable assignments</title>
     
     <p>Let's practice some fundamental R operations through hands-on exercises.</p>
     
     <activity xml:id="activity-basic-calculations">
       <title>Basic R Calculations</title>
       <statement>
         <p>Work through the following exercises in Web-R:</p>
         <ol>
           <li>
             <p>Calculate the area of a circle with radius 5 (remember: area = π × r²)</p>
           </li>
           <li>
             <p>Convert a temperature of 75°F to Celsius using the formula: C = (F - 32) × 5/9</p>
           </li>
           <li>
             <p>Calculate the average of the following five numbers: 18, 24, 32, 21, 15</p>
           </li>
           <li>
             <p>Determine whether the average you calculated is greater than 20</p>
           </li>
         </ol>
       </statement>
     </activity>
     
     <sage language="r">
       <input>
         # Exercise 1: Calculate the area of a circle with radius 5
         radius &lt;- 5
         # Insert your code here
         
         
         # Exercise 2: Convert 75°F to Celsius
         fahrenheit &lt;- 75
         # Insert your code here
         
         
         # Exercise 3: Calculate the average of five numbers
         numbers &lt;- c(18, 24, 32, 21, 15)
         # Insert your code here
         
         
         # Exercise 4: Determine if the average is greater than 20
         # Insert your code here
         
       </input>
      </sage>
     
     <exercise xml:id="ex-debugging-practice">
       <title>Debugging Practice</title>
       <statement>
         <p>The following R code contains several errors. Find and fix all the errors to make the code run correctly:</p>
         <pre>
           # Calculate the final price of an item with tax
           item_price = 29.99
           tax_rate = 0.08
           
           # Calculate tax amount
           tax_amount &lt;- item_price x tax_rate
           
           # Calculate final price
           final_price &lt;- item_price + tax_Amount
           
           # Check if the price is below budget
           budget &lt;- 35
           is_affordable = final price &lt; Budget
           
           # Print results
           print("Tax amount:" tax_amount)
           print("Final price:" final_price)
           print("Affordable?", is_affordable)
         </pre>
       </statement>
       <answer>
         <pre>
           # Calculate the final price of an item with tax
           item_price &lt;- 29.99  # Proper syntax is to use &lt;- to assign variables
           tax_rate &lt;- 0.08      #though = technically still works
           
           # Calculate tax amount
           tax_amount &lt;- item_price * tax_rate  # Changed 'x' to '*'
           
           # Calculate final price
           final_price &lt;- item_price + tax_amount  # Fixed capitalization of tax_amount
           
           # Check if the price is below budget
           budget &lt;- 35
           is_affordable &lt;- final_price &lt; budget  # Added space and fixed capitalization
           
           # Print results
           print(paste("Tax amount:", tax_amount))  # Added paste function
           print(paste("Final price:", final_price))  # Added paste function
           print(paste("Affordable?", is_affordable))  # Added paste function
         </pre>
       </answer>
     </exercise>
   </subsection>
 </section>

<section xml:id="sec-data-structures">
   <title>Working with data structures</title>
   
   <!-- Vectors, lists, and data frames -->
   <subsection xml:id="subsec-r-data-structures">
     <title>Lists and Dataframes</title>
     
     <p>R provides several data structures for organizing and manipulating dat, but we will be primarily working with:</p>
     
         <definition xml:id="def-vector">
       <statement>
         <p>A <term>vector</term> is a one-dimensional collection of elements of the same data type. Vectors are created using the <c>c()</c> function.</p>
       </statement>
     </definition>

     <definition xml:id="def-list">
       <statement>
         <p>A <term>list</term> is a flexible collection that can contain elements of different data types, including other lists. Lists are created using the <c>list()</c> function.</p>
       </statement>
     </definition>
     
     <definition xml:id="def-dataframe">
       <statement>
         <p>A <term>data frame</term> is a two-dimensional table-like structure where each column can contain a different data type. Data frames are the most common structure for datasets in R.</p>
       </statement>
     </definition>
     
     <sage language="r">
       <input>
        # Lists 
        # Creating a list
         my_list &lt;- list(
           numbers = c(1, 2, 3),
           text = "Hello, R!",
           logical = c(TRUE, FALSE),
           nested = list(a = 1, b = 2)
         )
         
         # Accessing list elements
         my_list$numbers      # By name
         my_list[[1]]         # By position (returns the element)
         my_list[1]           # By position (returns a sublist)
         my_list$nested$a     # Accessing nested elements
         
         # Data Frames
         # Creating a data frame
         student_df &lt;- data.frame(
           name = c("Alice", "Bob", "Charlie", "Diana"),
           age = c(21, 22, 20, 23),
           gpa = c(3.8, 3.2, 3.9, 3.5),
           international = c(FALSE, FALSE, TRUE, TRUE)
         )
         
         # Examining a data frame
         student_df           # Print the data frame
         str(student_df)      # Structure of the data frame
         summary(student_df)  # Summary statistics
         
         # Accessing data frame elements
         student_df$name      # Access a column by name
         student_df[, 2]      # Access a column by position
         student_df[3, ]      # Access a row
         student_df[2, 3]     # Access a specific element

       </input>
      </sage>
   </subsection>
   
   <!-- Creating and manipulating basic data structures -->
   <subsection xml:id="subsec-manipulating-structures">
     <title>Creating and manipulating basic data structures</title>
     
     <p>Let's explore common operations for creating and manipulating data structures in R. Here are some common data frame operations:</p>
     <ul>
       <li>
         <p><term>Creating</term>: <c>data.frame()</c>, <c>as.data.frame()</c></p>
       </li>
       <li>
         <p><term>Selecting columns</term>: <c>df$column</c>, <c>df[, "column"]</c>, <c>df[, 1]</c></p>
       </li>
       <li>
         <p><term>Selecting rows</term>: <c>df[1:5, ]</c>, <c>df[df$x &gt; 10, ]</c></p>
       </li>
       <li>
         <p><term>Adding columns</term>: <c>df$new_col &lt;- values</c>, <c>cbind(df, new_col = values)</c></p>
       </li>
       <li>
         <p><term>Adding rows</term>: <c>rbind(df, new_row)</c></p>
       </li>
       <li>
         <p><term>Information functions</term>: <c>nrow()</c>, <c>ncol()</c>, <c>dim()</c>, <c>names()</c>, <c>str()</c>, <c>summary()</c></p>
       </li>
     </ul>
     
     <sage language="r">
       <input>
         # Vector creation and manipulation
         
         # Creating sequences
         seq1 &lt;- 1:10  # Integer sequence
         seq1
         
         seq2 &lt;- seq(0, 1, by = 0.1)  # Sequence with specific increment
         seq2
         
         seq3 &lt;- rep(c("A", "B"), times = 3)  # Repeating values
         seq3
         
         # Vector functions
         numbers &lt;- c(15, 8, 23, 42, 7)
         
         length(numbers)  # Number of elements
         sum(numbers)     # Sum of elements
         mean(numbers)    # Average
         max(numbers)     # Maximum value
         min(numbers)     # Minimum value
         sort(numbers)    # Sort in ascending order
         
         # Vector subsetting
         numbers[3]        # Third element
         numbers[c(1, 4)]  # First and fourth elements
         numbers[numbers &gt; 20]  # Elements greater than 20
         numbers[-2]       # All except the second element
         
         # Data frame manipulation
         
         # Create a sample data frame
         students &lt;- data.frame(
           id = 1:5,
           name = c("Alice", "Bob", "Charlie", "Diana", "Elijah"),
           score = c(85, 92, 78, 90, 88),
           passed = c(TRUE, TRUE, TRUE, TRUE, TRUE)
         )
         
         # Viewing the data frame
         students
         
         # Basic information
         dim(students)      # Dimensions (rows, columns)
         nrow(students)     # Number of rows
         ncol(students)     # Number of columns
         names(students)    # Column names
         str(students)      # Structure
         summary(students)  # Summary statistics
         
         # Selecting columns
         students$name       # Name column
         students[, "score"] # Score column
         students[, 3]       # Third column
         students[, c(2, 3)] # Second and third columns
         
         # Selecting rows
         students[2, ]            # Second row
         students[1:3, ]          # First three rows
         students[students$score &gt;= 90, ]  # Rows where score is at least 90
         
         # Adding a new column
         students$grade &lt;- c("B", "A", "C", "A", "B")
         students
         
         # Adding a calculated column
         students$normalized &lt;- students$score / max(students$score) * 100
         students
         
         # Creating a new row
         new_student &lt;- data.frame(
           id = 6,
           name = "Fiona",
           score = 95,
           passed = TRUE,
           grade = "A",
           normalized = 95 / max(students$score) * 100
         )
         
         # Adding the new row
         students &lt;- rbind(students, new_student)
         students
       </input>
      </sage>
   </subsection>
   
   <!-- Exercise: Building and transforming data structures -->
   <subsection xml:id="subsec-exercise-data-structures">
     <title>Exercise: Building and transforming data structures</title>
     
     <activity xml:id="activity-build-dataset">
       <title>Building a Student Dataset</title>
       <statement>
         <p>In this exercise, you will create and manipulate a dataset of student information.</p>
         
         <p>Instructions:</p>
         <ol>
           <li>
             <p>Create a data frame containing information for 5 students with the following columns:</p>
             <ul>
               <li><p>last_four_digits_student_id (numeric)</p></li>
               <li><p>name (character)</p></li>
               <li><p>age (numeric)</p></li>
               <li><p>major (character)</p></li>
               <li><p>gpa (numeric)   Note: you can use any GPA if you're not comfortable sharing</p></li>
             </ul>
           </li>
           <li>
             <p>Add a new column called 'year' indicating the year of study (1, 2, 3, or 4) for each student</p>
           </li>
           <li>
             <p>Add a new column called 'honors' that contains TRUE for students with GPA ≥ 3.5 and FALSE otherwise</p>
           </li>
           <li>
             <p>Create a subset of the data frame that includes only honors students</p>
           </li>
           <li>
             <p>Calculate and display the average GPA for each major</p>
           </li>
         </ol>
       </statement>
     </activity>
     
     <sage language="r">
       <input>
         # Step 1: Create the student data frame
         # Your code here
         
         
         # Step 2: Add a 'year' column
         # Your code here
         
         
         # Step 3: Add an 'honors' column
         # Your code here
         
         
         # Step 4: Create a subset of honors students
         # Your code here
         
         
         # Step 5: Calculate average GPA by major
         # Your code here
         
       </input>
      </sage>
     
    <exercise xml:id="ex-student-dataset-answer">
     <title>Answer</title>
     <program interactive="webr" xml:id="prog-build-dataset-solution">
       <code>
         # Step 1: Create the student data frame
         students &lt;- data.frame(
           student_id = 1:5,
           name = c("Alex", "Beatrice", "Carlos", "Diana", "Eli"),
           age = c(19, 20, 21, 19, 22),
           major = c("Computer Science", "Biology", "Computer Science", "Mathematics", "Biology"),
           gpa = c(3.7, 3.4, 3.2, 3.9, 3.6)
         )
         
         # Display the data frame
         students
         
         # Step 2: Add a 'year' column
         students$year &lt;- c(2, 3, 4, 1, 4)
         students
         
         # Step 3: Add an 'honors' column
         students$honors &lt;- students$gpa &gt;= 3.5
         students
         
         # Step 4: Create a subset of honors students
         honors_students &lt;- students[students$honors, ]
         honors_students
         
         # Step 5: Calculate average GPA by major
         # Using aggregate function
         avg_gpa_by_major &lt;- aggregate(gpa ~ major, data = students, FUN = mean)
         avg_gpa_by_major
       </code>
     </program>
     </exercise>
     
     <exercise xml:id="ex-challenge-task">
       <title>Challenge Task</title>
       <statement>
         <p>Using the student dataset from the exercise above, complete the following tasks:</p>
         <ol>
           <li>
             <p>Find the student with the highest GPA</p>
           </li>
           <li>
             <p>Calculate the percentage of students who are on the honors list</p>
           </li>
           <li>
             <p>Create a new column called 'age_group' that categorizes students as 'Under 20', '20-21', or '22+' based on their age</p>
           </li>
         </ol>
       </statement>
       <answer>
         <pre>
           # Find the student with the highest GPA
           top_student &lt;- students[which.max(students$gpa), ]
           top_student
           
           # Calculate percentage of honors students
           percent_honors &lt;- mean(students$honors) * 100
           percent_honors
           
           # Create age_group column
           students$age_group &lt;- cut(students$age, 
                    breaks = c(0, 20, 22, Inf), 
                    labels = c("Under 20", "20-21", "22+"))
           students
         </pre>
       </answer>
     </exercise>
   </subsection>
 </section>


  <!--TOPIC 5: Data Cleaning and Organization-->
  <section xml:id="sec-data-cleaning">
  <title>Data Cleaning and Organizing</title>

  <introduction>
    <p>
      Real-world data rarely comes in a perfect, analysis-ready format. Data cleaning (sometimes called data wrangling or data preprocessing) is the process of detecting and correcting errors, inconsistencies, and inaccuracies in datasets. It's often said that data scientists spend up to 80% of their time cleaning and organizing data, making this a critical skill in the data science workflow.
    </p>
  </introduction>

  <subsection xml:id="subsec-why-clean-data">
    <title>Why Clean Data Matters</title>
    
    <p>
      Clean data is a prerequisite for reliable analysis. Working with messy data can lead to:
    </p>
    
    <ul>
      <li>Incorrect conclusions or predictions</li>
      <li>Wasted time debugging analytical procedures</li>
      <li>Difficulties reproducing results</li>
      <li>Loss of credibility when presenting findings</li>
    </ul>
    
    <p>
      The concept of "garbage in, garbage out" is especially relevant in data science—even the most sophisticated analysis methods will produce flawed results if applied to poor-quality data.
    </p>
  </subsection>

  <subsection xml:id="subsec-common-data-issues">
    <title>Common Data Quality Issues</title>
    
    <p>
      Before cleaning data, it's important to recognize common quality issues:
    </p>
    
    <dl>
      <li>
        <title>Missing values</title>
        <p>Data points that are absent from the dataset, often represented as NA in R</p>
      </li>
      <li>
        <title>Duplicate records</title>
        <p>Identical or nearly identical rows that appear multiple times</p>
      </li>
      <li>
        <title>Outliers</title>
        <p>Values that differ significantly from other observations and may represent errors</p>
      </li>
      <li>
        <title>Inconsistent formatting</title>
        <p>The same data represented differently (e.g., "New York" vs. "NY" vs. "N.Y.")</p>
      </li>
      <li>
        <title>Incorrect data types</title>
        <p>Data stored in inappropriate formats (e.g., dates stored as text)</p>
      </li>
      <li>
        <title>Structural issues</title>
        <p>Problems with how the data is organized (e.g., multiple values in a single cell)</p>
      </li>
    </dl>
  </subsection>
  
  <subsection xml:id="subsec-data-cleaning-steps">
    <title>Data Cleaning Process</title>
    
    <p>
      A systematic approach to data cleaning typically includes:
    </p>
    
    <ol>
    <li>Inspect the data to identify issues (using <c>str()</c>, <c>summary()</c>, etc.)</li>
    <li>Handle missing values (deletion or imputation)</li>
    <li>Identify and remove duplicates</li>
    <li>Standardize text and categorical data</li>
    <li>Convert data types appropriately</li>
    <li>Standardize date and time formats</li>
    <li>Document the cleaning process for reproducibility</li>
    </ol>
    
    <p>
      Let's explore how to perform these tasks in R.
    </p>

    <introduction>
  <p>
    Before diving into our data cleaning example, let's review some key functions and techniques commonly used in R for data cleaning:
  </p>
</introduction>

<paragraphs>
  <title>Key Data Cleaning Functions in R</title>
  
  <p><c>sapply()</c>: Applies a function to each element of a list or each column of a data frame. The "s" stands for "simplified," meaning it attempts to return results in the simplest possible data structure.</p>
  
  <program language="r">
    <input>
# Example: Count missing values in each column of a data frame
sapply(df, function(x) sum(is.na(x)))
    </input>
  </program>
  
  <p><c>is.na()</c>: Tests for missing values (NA) in a vector or data frame. Returns a logical vector of the same length as the input, with TRUE for missing values.</p>
  
  <p><c>na.omit()</c>: Removes rows with any missing values from a data frame. This can be useful but may result in significant data loss if many rows have at least one missing value.</p>
  
  <program language="r">
    <input>
# Remove all rows containing any NA values
clean_df &lt;- na.omit(df)
    </input>
  </program>
  
  <p><c>duplicated()</c>: Returns a logical vector indicating which elements of a vector or data frame are duplicates of elements with smaller subscripts.</p>
  
  <program language="r">
    <input>
# Identify duplicate rows based on all columns
duplicated(df)

# Identify duplicate values in a specific column
duplicated(df$id)

# Remove duplicate rows
df[!duplicated(df), ]
    </input>
  </program>
  
  <p><c>gsub()</c>: Performs pattern matching and replacement in strings using regular expressions. This function is particularly useful for standardizing text data.</p>
  
  <program language="r">
    <input>
# Convert first letter of each word to uppercase
gsub("\\b([a-z])", "\\U\\1", "john smith", perl=TRUE)
    </input>
  </program>
  
  <p><c>as.numeric()</c>, <c>as.character()</c>, etc.: Type conversion functions that change the data type of an object. Often used in combination when cleaning data.</p>
  
  <program language="r">
    <input>
# Convert a character column to numeric
df$age &lt;- as.numeric(as.character(df$age))
    </input>
  </program>
  
  <p><c>lubridate</c> package: A comprehensive set of functions for working with dates and times in R. Functions like <c>ymd()</c>, <c>mdy()</c>, and <c>dmy()</c> parse dates in different formats.</p>
  
  <program language="r">
    <input>
library(lubridate)
# Parse dates in different formats
ymd("2023-01-15")
mdy("01/15/2023")
dmy("15-Jan-2023")
    </input>
  </program>
  
  <p><c>Conditional indexing</c>: Using logical conditions to select and modify specific elements of a data frame is a fundamental technique for data cleaning.</p>
  
  <program language="r">
    <input>
# Replace specific values in a column
df$city[df$city == "NY"] &lt;- "New York"

# Fill missing values conditionally
df$name[is.na(df$name)] &lt;- "Unknown"
    </input>
  </program>
</paragraphs>

  </subsection>

  <subsection xml:id="subsec-data-cleaning-R">
    <title>Data Cleaning in R</title>
    
    <p>
      R provides numerous functions and packages for data cleaning. Here are some fundamental techniques:
    </p>
    
    <sage language="r">
      <input>
    # Load a sample dataset with quality issues
    # For this example, we'll create one
    messy_data &lt;- data.frame(
    ID = c(1, 2, 2, 3, 4, NA),
    Name = c("John Smith", "Jane doe", "Jane Doe", "bob wilson", NA, "Sarah Lee"),
    Age = c(25, "thirty", 30, 22, 41, 38),
    City = c("New York", "NY", "New York", "Los Angeles", "Chicago", ""),
    Date = c("2023-01-15", "01/15/2023", "2023-01-15", "15-Jan-23", "2023/01/15", NA)
      )

# View the messy data
print(messy_data)

# Step 1: Inspecting the data
str(messy_data)
summary(messy_data)

# Step 2: Handling missing values
# Count missing values in each column
sapply(messy_data, function(x) sum(is.na(x)))

# Remove rows with any missing values
clean_data1 &lt;- na.omit(messy_data)

# Or, fill missing values (imputation)
messy_data$ID[is.na(messy_data$ID)] &lt;- 0
messy_data$Name[is.na(messy_data$Name)] &lt;- "Unknown"

# Step 3: Removing duplicates
# Identify duplicates
duplicated(messy_data$ID)

# Remove duplicate rows based on ID column
clean_data2 &lt;- messy_data[!duplicated(messy_data$ID),]

# Step 4: Standardize text data
# Convert names to proper case
proper_case &lt;- function(x) {
  return(gsub("\\b([a-z])", "\\U\\1", tolower(x), perl=TRUE))
}

messy_data$Name &lt;- proper_case(messy_data$Name)

# Standardize city names
messy_data$City[messy_data$City == "NY"] &lt;- "New York"
messy_data$City[messy_data$City == ""] &lt;- NA

# Step 5: Convert data types
# Convert Age to numeric
messy_data$Age &lt;- as.numeric(as.character(messy_data$Age))
# This will produce a warning about NAs introduced by coercion

# Step 6: Standardize date formats
# This requires a more complex approach, using lubridate package
# install.packages("lubridate")
library(lubridate)

# Parse dates in different formats
parse_date &lt;- function(date_str) {
  if (is.na(date_str)) return(NA)
  
  # Try different formats
  date &lt;- try(ymd(date_str), silent = TRUE)
  if (inherits(date, "try-error") || is.na(date)) date &lt;- try(mdy(date_str), silent = TRUE)
  if (inherits(date, "try-error") || is.na(date)) date &lt;- try(dmy(date_str), silent = TRUE)
  
  return(date)
}

# Apply the function to standardize dates
messy_data$Date &lt;- sapply(messy_data$Date, parse_date)

# Final look at the cleaned data
print(messy_data)

# Document changes for reproducibility (in practice, you would save this to a file)
cleaning_log &lt;- c(
  "1. Replaced missing IDs with 0",
  "2. Replaced missing names with 'Unknown'",
  "3. Standardized case for names",
  "4. Standardized city names (NY → New York)",
  "5. Converted Age column to numeric",
  "6. Standardized date formats using lubridate"
)

print(cleaning_log)
      </input>
    </sage>
    
    <p>
      The above example demonstrates basic data cleaning techniques. For more complex datasets, specialized packages like <c>tidyr</c>, <c>dplyr</c>, and <c>janitor</c> offer powerful tools.
    </p>
  </subsection>
  
  <subsection xml:id="subsec-tidy-data">
    <title>Tidy Data Principles</title>
    
    <p>
      "Tidy data" is a standard way of organizing data that makes analysis easier. According to Hadley Wickham, tidy data has the following characteristics:
    </p>
    
    <ol>
      <li>Each variable forms a column</li>
      <li>Each observation forms a row</li>
      <li>Each type of observational unit forms a table</li>
    </ol>
    
    <p>
      The <c>tidyr</c> package helps reshape data to meet these principles:
    </p>
    
    <sage language="r">
      <input>
# Load tidyr package
library(tidyr)

# Create a messy dataset where columns represent years
untidy_data &lt;- data.frame(
  country = c("USA", "Canada", "Mexico"),
  `2018` = c(100, 90, 85),
  `2019` = c(105, 92, 87),
  `2020` = c(98, 88, 84)
)

print(untidy_data)

# Convert to tidy format - each row is a country-year observation
tidy_data &lt;- pivot_longer(
  untidy_data, 
  cols = c(`2018`, `2019`, `2020`), 
  names_to = "year", 
  values_to = "score"
)

print(tidy_data)

# The reverse operation - from tidy to wide format
wide_data &lt;- pivot_wider(
  tidy_data,
  names_from = year,
  values_from = score
)

print(wide_data)
      </input>
    </sage>
  </subsection>
  
  <subsection xml:id="subsec-data-validation">
    <title>Data Validation</title>
    
    <p>
      After cleaning, it's essential to validate your data to ensure it meets quality standards. Data validation is a process of verifying and ensuring the accuracy, consistency, and reliability of data before it's used for any purpose.
    </p>
    
    <sage language="r">
      <input>
# Create a simple cleaned dataset for validation
clean_data &lt;- data.frame(
  ID = 1:5,
  Age = c(25, 30, 22, 41, 38),
  Income = c(45000, 52000, 0, 63000, 58000)
)

# Validation checks
validation_results &lt;- list(
  # Check for missing values
  missing_values = any(is.na(clean_data)),
  
  # Check for duplicates
  duplicates = any(duplicated(clean_data$ID)),
  
  # Check value ranges
  age_in_range = all(clean_data$Age &gt;= 0 &amp; clean_data$Age &lt; 120),
  
  # Check for suspicious values
  suspicious_income = any(clean_data$Income == 0)
)

print(validation_results)

# Create a validation report
for (check in names(validation_results)) {
  status &lt;- if(check == "suspicious_income") {
    ifelse(validation_results[[check]], "WARNING", "PASS")
  } else {
    ifelse(validation_results[[check]], "FAIL", "PASS")
  }
  
  cat(check, ": ", status, "\n", sep="")
}
      </input>
    </sage>
  </subsection>
  
  <conclusion>
    <p>
      Data cleaning is an iterative process that requires careful attention to detail and domain knowledge. While it may not be the most glamorous part of data science, it is undoubtedly one of the most important. Clean, well-organized data sets the foundation for all subsequent analysis and modeling tasks.
    </p>
  </conclusion>
  
  <exercise xml:id="ex-data-cleaning">
    <title>Data Cleaning Practice</title>
    <statement>
      <p>
        Given the following dataset of student records with various issues, apply data cleaning techniques to address the problems:
      </p>
      <program language="r">
        <input>
# Student records with quality issues
students &lt;- data.frame(
  ID = c(101, 102, 102, 103, NA, 105),
  Name = c("john smith", "JANE DOE", "Jane Doe", "Sam Brown", "Alex Johnson", ""),
  Score = c(85, "A", 92, 78, 65, 90),
  Grade = c("B", "A", "A-", "C+", "D", "A-"),
  Date_Enrolled = c("2023-09-01", "09/01/2023", "2023-09-01", "Sept 1, 2023", NA, "01-09-2023")
)

# Your cleaning tasks:
# 1. Handle duplicate student IDs (keep the higher score)
# 2. Fill in missing or empty values
# 3. Standardize name formats (proper case)
# 4. Ensure Score is numeric (convert "A" to 95)
# 5. Standardize date formats
# 6. Validate the cleaned data
        </input>
      </program>
    </statement>
    <answer>
      <program language="r">
        <input>
# Load required packages
library(dplyr)
library(lubridate)

# First look at the data
print(students)
str(students)

# Define a proper case function
proper_case &lt;- function(x) {
  return(gsub("\\b([a-z])", "\\U\\1", tolower(x), perl=TRUE))
}

# Clean the data
cleaned_students &lt;- students %&gt;%
  # Convert Score to numeric (A = 95)
  mutate(Score = case_when(
    Score == "A" ~ 95,
    TRUE ~ as.numeric(as.character(Score))
  )) %&gt;%
  # Handle duplicates - keep row with higher score
  group_by(ID) %&gt;%
  slice_max(Score, with_ties = FALSE) %&gt;%
  ungroup() %&gt;%
  # Standardize names
  mutate(Name = proper_case(Name)) %&gt;%
  # Fill missing/empty values
  mutate(
    ID = if_else(is.na(ID), 999, ID),
    Name = if_else(Name == "", "Unknown", Name)
  )

# Parse and standardize dates
parse_date &lt;- function(date_str) {
  if (is.na(date_str) || date_str == "") return(as.Date("2023-09-01"))
  
  # Try different formats
  date &lt;- try(ymd(date_str), silent = TRUE)
  if (inherits(date, "try-error") || is.na(date)) date &lt;- try(mdy(date_str), silent = TRUE)
  if (inherits(date, "try-error") || is.na(date)) date &lt;- try(dmy(date_str), silent = TRUE)
  if (inherits(date, "try-error") || is.na(date)) {
    # Handle text dates like "Sept 1, 2023"
    if (grepl("Sept", date_str)) {
      date &lt;- as.Date("2023-09-01")
    }
  }
  
  return(date)
}

# Apply date parsing
cleaned_students$Date_Enrolled &lt;- sapply(students$Date_Enrolled, parse_date)

# Validate the cleaned data
validation_checks &lt;- list(
  no_missing_values = !any(is.na(cleaned_students)),
  no_duplicates = !any(duplicated(cleaned_students$ID)),
  all_scores_numeric = is.numeric(cleaned_students$Score),
  all_dates_standardized = all(class(cleaned_students$Date_Enrolled) == "Date")
)

# Print validation results
print(validation_checks)

# Display the cleaned dataset
print(cleaned_students)
        </input>
      </program>
    </answer>
  </exercise>
  
<exercise xml:id="chk-data-cleaning">
  <title>Data Cleaning Concepts</title>
  <statement>
    <p>
      Which of the following are key principles of "tidy data"? Select all that apply.
    </p>
  </statement>
  <choices multiple-correct="yes">
    <choice correct="yes">
      <statement>
        <p>Each variable forms a column</p>
      </statement>
      <feedback>
        <p>Correct! In tidy data, each variable forms a column.</p>
      </feedback>
    </choice>
    <choice>
      <statement>
        <p>Missing values should be replaced with zeros</p>
      </statement>
      <feedback>
        <p>Incorrect. Tidy data doesn't require missing values to be replaced with zeros. In fact, this could introduce bias in analyses.</p>
      </feedback>
    </choice>
    <choice correct="yes">
      <statement>
        <p>Each observation forms a row</p>
      </statement>
      <feedback>
        <p>Correct! In tidy data, each observation forms a row.</p>
      </feedback>
    </choice>
    <choice>
      <statement>
        <p>Variable names should be in uppercase</p>
      </statement>
      <feedback>
        <p>Incorrect. The case of variable names is a formatting preference and not a principle of tidy data.</p>
      </feedback>
    </choice>
    <choice correct="yes">
      <statement>
        <p>Each type of observational unit forms a table</p>
      </statement>
      <feedback>
        <p>Correct! In tidy data, each type of observational unit forms a table.</p>
      </feedback>
    </choice>
  </choices>
</exercise>
  
</section>
 
<!-- TOPIC 6: DATA IMPORT AND EXPLORATION -->
  <section xml:id="sec-data-import">
    <title>Data Import and Exploration</title>
    
    <!-- Session 11: Importing data into R -->
    <subsection xml:id="subsec-importing-data">
      <title>Reading CSV files and other data formats</title>
      
      <p>R can import data from various file formats. The most common is the CSV (Comma-Separated Values) format, but R also supports Excel files, JSON, XML, and database connections.</p>
      
      <p>Basic functions for importing data include:</p>
      <ul>
        <li>
          <p><term>read.csv()</term>: For comma-separated files</p>
        </li>
        <li>
          <p><term>read.table()</term>: For more general text files with flexible delimiter options</p>
        </li>
        <li>
          <p><term>read.delim()</term>: For tab-delimited files</p>
        </li>
        <li>
          <p><term>readxl::read_excel()</term>: For Excel files (requires the readxl package)</p>
        </li>
        <li>
          <p><term>jsonlite::fromJSON()</term>: For JSON files (requires the jsonlite package)</p>
        </li>
      </ul>
      
      <p>Key parameters for the read.csv() function:</p>
      <ul>
        <li>
          <p><term>file</term>: File path or URL</p>
        </li>
        <li>
          <p><term>header</term>: TRUE if the first row contains column names</p>
        </li>
        <li>
          <p><term>sep</term>: Field separator character</p>
        </li>
        <li>
          <p><term>dec</term>: Decimal point character</p>
        </li>
        <li>
          <p><term>stringsAsFactors</term>: Should character columns be converted to factors?</p>
        </li>
        <li>
          <p><term>na.strings</term>: Strings to be treated as NA values</p>
        </li>
      </ul>
      <p>What we are going to practice first is creating data within the R environment, saving as a .csv file, and doing some basic functions with it:</p>
    
      <sage language="r">
        <input>
          # First, let's create a sample CSV file to use
          # Create a sample dataset
          sample_data &lt;- data.frame(
            ID = 1:5,
            Name = c("Alice", "Bob", "Charlie", "Diana", "Elijah"),
            Age = c(25, 32, 28, 41, 35),
            Salary = c(55000, 62000, 58000, 75000, 68000),
            Department = c("HR", "IT", "Marketing", "Finance", "IT")
          )
          
          # Save it as a CSV file
          write.csv(sample_data, "employee_data.csv", row.names = FALSE)
          
          # Verify the file was created
          list.files(pattern = "*.csv")
          
          # Now let's import the CSV file
          # Basic import
          employees &lt;- read.csv("employee_data.csv")
          head(employees)
          
          # Import with some options
          employees2 &lt;- read.csv("employee_data.csv", 
            stringsAsFactors = TRUE,  # Convert strings to factors
            na.strings = c("NA", "N/A", ""))  # Define NA values
          
          # Compare the structure of the two imports
          str(employees)
          str(employees2)
          
          # Creating a CSV with missing values
          sample_data_na &lt;- data.frame(
            ID = 1:5,
            Name = c("Alice", "Bob", "Charlie", "", "Elijah"),
            Age = c(25, NA, 28, 41, 35),
            Salary = c(55000, 62000, NA, 75000, 68000),
            Department = c("HR", "IT", "N/A", "Finance", "IT")
          )
          
          # Save it as a CSV file
          write.csv(sample_data_na, "employee_data_na.csv", row.names = FALSE)
          
          # Import handling missing values
          employees_na &lt;- read.csv("employee_data_na.csv", 
                                 na.strings = c("NA", "N/A", ""))
          
          # Check for missing values
          employees_na
          is.na(employees_na)
          colSums(is.na(employees_na))  # Count NAs in each column
        </input>
      </sage>
      
      <p>The R environment has several built in datasets that you can also access as you learn R. A few examples are in the code box, uncomment the dataset that most appeals to you.</p>
      
      <sage language="r">
        <input>
          # Importing data from a URL
          # Note: This URL is a placeholder and may not work in the actual Web-R environment
          
          #Dataset about cars
          #data(mtcars)
          #head(mtcars)
          
          # Dataset about penguins (note that for this one you have to a few installation steps first)
          # install.packages("palmerpenguins")
          # library(palmerpenguins)
          # data(penguins)
          # head(penguins)
          
          # Dataset about iris flowers
          #data(iris)
          #head(iris)
          #str(iris)

          # Dataset about air quality from NYC in 1973
          #data(airquality)
          #head(airquality)

          # Dataset about arrests made in the US in 1973
          #data(USArrests)
          #head(USArrests)
        </input>
      </sage>
      
      <exercise xml:id="ex-import-practice">
        <title>Data Import Practice</title>
        <statement>
          <p>Answer the following questions about data import in R:</p>
        </statement>
        <choices>
          <choice correct="yes">
            <statement>
              <p>Which parameter would you set to ensure empty strings are treated as missing values?</p>
            </statement>
            <answer>na.strings = ""</answer>
          </choice>
          <choice correct="no">
            <statement>
              <p>Which function would you use to import an Excel file?</p>
            </statement>
            <answer>read.csv()</answer>
            <feedback>
              <p>read.csv() is for CSV files. For Excel files, you would use read_excel() from the readxl package.</p>
            </feedback>
          </choice>
          <choice correct="yes">
            <statement>
              <p>What is the default behavior of read.csv() regarding the first row of data?</p>
            </statement>
            <answer>It treats the first row as column headers (header = TRUE)</answer>
          </choice>
          <choice correct="yes">
            <statement>
              <p>How would you check the number of missing values in each column of a data frame called 'df'?</p>
            </statement>
            <answer>colSums(is.na(df))</answer>
          </choice>
        </choices>
      </exercise>
      </subsection>
    
    <!-- Basic data inspection functions -->
    <subsection xml:id="subsec-data-inspection">
      <title>Basic data inspection functions</title>
      
      <p>After importing data, the next step is to inspect and understand the structure and content of your dataset. R provides several functions for this purpose:</p>
      
      <table xml:id="table-inspection-functions">
        <title>Common Data Inspection Functions</title>
        <tabular halign="center">
          <row header="yes">
            <cell>Function</cell>
            <cell>Description</cell>
          </row>
          <row>
            <cell><c>head()</c></cell>
            <cell>Display the first few rows (default: 6)</cell>
          </row>
          <row>
            <cell><c>tail()</c></cell>
            <cell>Display the last few rows (default: 6)</cell>
          </row>
          <row>
            <cell><c>View()</c></cell>
            <cell>Open data in a spreadsheet-like viewer (not available in Web-R)</cell>
          </row>
          <row>
            <cell><c>str()</c></cell>
            <cell>Display the structure, including data types of each column</cell>
          </row>
          <row>
            <cell><c>summary()</c></cell>
            <cell>Generate summary statistics for each column</cell>
          </row>
          <row>
            <cell><c>dim()</c></cell>
            <cell>Show dimensions (rows, columns)</cell>
          </row>
          <row>
            <cell><c>names()</c></cell>
            <cell>List column names</cell>
          </row>
          <row>
            <cell><c>glimpse()</c></cell>
            <cell>From dplyr package; similar to str() but more readable</cell>
          </row>
          <row>
            <cell><c>class()</c></cell>
            <cell>Show the class of an object</cell>
          </row>
          <row>
            <cell><c>table()</c></cell>
            <cell>Create frequency tables of categorical variables</cell>
          </row>
          <row>
            <cell><c>unique()</c></cell>
            <cell>shows distinct types in the dataset and is useful for classification tasks</cell>
          </row>
        </tabular>
      </table>
      <p>Now let's use some of these functions to learn something about whichever of the preloaded datasets talked about above that most appeals to you! Take this opportunity to try and learn something new.</p>
      <sage language="r">
        <input>
          # Load the built-in dataset of your choice
          #Put your code here
          
          # See the first few rows
          #Put your code here
          
          # Display column names
          #Put your code here
          
          # Get the structure
          #Put your code here
          
          # Generate summary statistics
          #Put your code here
          
          # Class of the object
          #Put your code here
          
          # Check for missing values and count how many, if so
          #Put your code here
          
          # Have fun diving more into the data and experimenting with other code!
          
        </input>
      </sage>
      
      <exercise xml:id="ex-inspection-matching">
        <title>Match Inspection Functions</title>
        <statement>
          <p>Match each data inspection task with the most appropriate R function:</p>
        </statement>
        <matches>
          <match>
            <premise>Check how many rows and columns are in a dataset</premise>
            <response>dim()</response>
          </match>
          <match>
            <premise>Get a quick overview of the data types in each column</premise>
            <response>str()</response>
          </match>
          <match>
            <premise>Calculate basic statistics like mean, median, min, and max for numerical columns</premise>
            <response>summary()</response>
          </match>
          <match>
            <premise>Count how many observations fall into each category of a variable</premise>
            <response>table()</response>
          </match>
          <match>
            <premise>Check for missing values in a dataset</premise>
            <response>is.na()</response>
          </match>
        </matches>
      </exercise>
    </subsection>
    
    <!-- Activity: Import and explore a dataset of interest -->
    <subsection xml:id="subsec-activity-import-explore">
      <title>Activity: Import and explore a dataset of interest</title>
      
      <activity xml:id="activity-import-explore">
        <title>Dataset Exploration</title>
        <statement>
          <p>In this activity, you will import and explore the mtcars dataset to practice your data inspection skills.</p>
          
          <p>Instructions:</p>
          <ol>
            <li>
              <p>Load the mtcars dataset using <c>data(mtcars)</c></p>
            </li>
            <li>
              <p>Perform a comprehensive exploration of the dataset using the inspection functions you've learned</p>
            </li>
            <li>
              <p>Answer the following questions:</p>
              <ul>
                <li>
                  <p>How many rows and columns are in the dataset?</p>
                </li>
                <li>
                  <p>What are the variables (column names) in the dataset?</p>
                </li>
                <li>
                  <p>What is the average miles per gallon (mpg) in the dataset?</p>
                </li>
                <li>
                  <p>Which car has the highest horsepower (hp)?</p>
                </li>
                <li>
                  <p>How many cars have 4 cylinders (cyl)?</p>
                </li>
                <li>
                  <p>Is there a relationship between miles per gallon (mpg) and weight (wt)?</p>
                </li>
              </ul>
            </li>
          </ol>
        </statement>
      </activity>
      
      <sage language="r">
        <input>
          # Step 1: Load the mtcars dataset
          #Your code here
          
          # Continue inputting your code to answer the questions
          # Practice using the comment lines saying what the code is doing
          
        </input>
      </sage>
      
      <exercise xml:id="ex-mtcarsex-solution">
        <title>Solution</title>
      <program interactive="webr" xml:id="prog-explore-mtcars-solution">
        <code>
          # Step 1: Load the mtcars dataset
          data(mtcars)
          
          # Step 2: Comprehensive exploration
          # Basic dimensions
          dim(mtcars)
          
          # Column names
          names(mtcars)
          
          # First few rows
          head(mtcars)
          
          # Structure
          str(mtcars)
          
          # Summary statistics
          summary(mtcars)
          
          # Check for missing values
          any(is.na(mtcars))
          
          # Step 3: Answer the questions
          # How many rows and columns are in the dataset?
          cat("Rows:", nrow(mtcars), "Columns:", ncol(mtcars), "\n")
          
          # What are the variables (column names) in the dataset?
          cat("Variables:", paste(names(mtcars), collapse=", "), "\n")
          
          # What is the average miles per gallon (mpg) in the dataset?
          cat("Average MPG:", mean(mtcars$mpg), "\n")
          
          # Which car has the highest horsepower (hp)?
          highest_hp_index &lt;- which.max(mtcars$hp)
          cat("Car with highest horsepower:", rownames(mtcars)[highest_hp_index], 
              "with", mtcars$hp[highest_hp_index], "hp\n")
          
          # How many cars have 4 cylinders (cyl)?
          cat("Number of cars with 4 cylinders:", sum(mtcars$cyl == 4), "\n")
          
          # Is there a relationship between miles per gallon (mpg) and weight (wt)?
          cor_mpg_wt &lt;- cor(mtcars$mpg, mtcars$wt)
          cat("Correlation between MPG and weight:", cor_mpg_wt, "\n")
          
          # Visualize the relationship
          plot(mtcars$wt, mtcars$mpg, 
               main="Relationship between Weight and MPG",
               xlab="Weight (1000 lbs)",
               ylab="Miles Per Gallon",
               pch=19,
               col="blue")
          
          # Add a regression line
          abline(lm(mpg ~ wt, data=mtcars), col="red", lwd=2)
        </code>
      </program>
      </exercise>
      </subsection>
  </section>

  <!-- Session 12: Data exploration techniques -->
  <section xml:id="sec-data-exploration">
    <title>Data exploration techniques</title>
    
    <!-- Summary statistics and data summaries -->
    <subsection xml:id="subsec-summary-statistics">
      <title>Summary statistics and data summaries</title>
      
      <p>Summary statistics provide a concise way to understand the central tendencies, dispersion, and shape of your data. Key summary statistics include:</p>
      
      <ul>
        <li>
          <p><term>Measures of central tendency</term>: mean, median, mode</p>
        </li>
        <li>
          <p><term>Measures of dispersion</term>: range, variance, standard deviation, interquartile range (IQR)</p>
        </li>
        <li>
          <p><term>Measures of shape</term>: skewness, kurtosis</p>
        </li>
        <li>
          <p><term>Quantiles</term>: min, max, quartiles, percentiles</p>
        </li>
      </ul>
      
      <p>R functions for calculating summary statistics:</p>
      <ul>
        <li>
          <p><term>summary()</term>: General summary statistics for each column in a data frame</p>
        </li>
        <li>
          <p><term>mean(), median(), range()</term>: Basic statistical functions</p>
        </li>
        <li>
          <p><term>sd(), var()</term>: Standard deviation and variance</p>
        </li>
        <li>
          <p><term>quantile()</term>: Calculate quantiles</p>
        </li>
        <li>
          <p><term>IQR()</term>: Interquartile range</p>
        </li>
        <li>
          <p><term>cor()</term>: Correlation between variables</p>
        </li>
        <li>
          <p><term>aggregate()</term>: Calculate statistics by group</p>
        </li>
      </ul>
      
      <program interactive="webr" xml:id="prog-summary-stats">
        <code>
          # Load the mtcars dataset
          data(mtcars)
          
          # Basic summary statistics
          summary(mtcars)
          
          # Calculate specific statistics for mpg
          mean(mtcars$mpg)         # Mean
          median(mtcars$mpg)       # Median
          min(mtcars$mpg)          # Minimum
          max(mtcars$mpg)          # Maximum
          range(mtcars$mpg)        # Range (min and max)
          var(mtcars$mpg)          # Variance
          sd(mtcars$mpg)           # Standard deviation
          IQR(mtcars$mpg)          # Interquartile range
          
          # Calculate quartiles
          quantile(mtcars$mpg)     # Default quartiles (0%, 25%, 50%, 75%, 100%)
          quantile(mtcars$mpg, probs = seq(0, 1, 0.1))  # Deciles (0%, 10%, 20%, ..., 100%)
          
          # Correlation matrix for numeric variables
          cor_matrix &lt;- cor(mtcars)
          round(cor_matrix, 2)  # Round to 2 decimal places for better readability
          
          # Correlation between specific variables
          cor(mtcars$mpg, mtcars$wt)
          
          # Grouped summary statistics
          # Average mpg by number of cylinders
          aggregate(mpg ~ cyl, data = mtcars, FUN = mean)
          
          # Multiple statistics by cylinder
          aggregate(mpg ~ cyl, data = mtcars, FUN = function(x) c(mean = mean(x), 
                                                               sd = sd(x), 
                                                               min = min(x), 
                                                               max = max(x)))
          
          # Multiple variables and grouping factors
          aggregate(cbind(mpg, hp) ~ cyl + am, data = mtcars, FUN = mean)
          
          # Alternative using tapply
          tapply(mtcars$mpg, mtcars$cyl, mean)
          
          # Create a simple statistical summary function
          my_stats &lt;- function(x) {
            c(mean = mean(x),
              median = median(x),
              sd = sd(x),
              IQR = IQR(x),
              min = min(x),
              max = max(x),
              n = length(x))
          }
          
          # Apply to mpg
          my_stats(mtcars$mpg)
          
          # Apply to multiple groups
          by(mtcars$mpg, mtcars$cyl, my_stats)
        </code>
      </program>
      
      <exercise xml:id="ex-summary-stats">
        <title>Summary Statistics Practice</title>
        <statement>
          <p>Using the following data about student exam scores, answer the questions below:</p>
          <pre>
            scores &lt;- c(65, 72, 83, 85, 68, 92, 78, 95, 69, 74, 88, 73)
          </pre>
        </statement>
        <exercise>
          <statement>
            <p>Calculate the mean, median, minimum, and maximum of the scores.</p>
          </statement>
          <answer>
            <pre>
              mean(scores)    # Result: 78.5
              median(scores)  # Result: 76
              min(scores)     # Result: 65
              max(scores)     # Result: 95
            </pre>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Calculate the standard deviation and interquartile range (IQR).</p>
          </statement>
          <answer>
            <pre>
              sd(scores)      # Result: 10.13
              IQR(scores)     # Result: 16.25
            </pre>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Determine the quartiles (25th, 50th, and 75th percentiles).</p>
          </statement>
          <answer>
            <pre>
              quantile(scores)
              # Result: 
              # 0%   25%   50%   75%  100% 
              # 65.0  69.5  76.0  85.8  95.0 
            </pre>
          </answer>
        </exercise>
      </exercise>
    </subsection>
    
    <!-- Initial data visualization -->
    <subsection xml:id="subsec-initial-visualization">
      <title>Initial data visualization</title>
      
      <p>Visualization is an essential tool for data exploration. Basic R plotting functions allow you to quickly visualize patterns and relationships in your data.</p>
      
      <p>Common plot types for initial data exploration:</p>
      <ul>
        <li>
          <p><term>Histograms</term>: Visualize the distribution of a single numerical variable</p>
        </li>
        <li>
          <p><term>Boxplots</term>: Show the five-number summary and identify outliers</p>
        </li>
        <li>
          <p><term>Scatter plots</term>: Examine relationships between two numerical variables</p>
        </li>
        <li>
          <p><term>Bar plots</term>: Visualize frequencies of categorical variables</p>
        </li>
        <li>
          <p><term>Line plots</term>: Show trends over time or ordered categories</p>
        </li>
        <li>
          <p><term>Density plots</term>: Smooth approximation of the distribution of a numerical variable</p>
        </li>
      </ul>
      
      <program interactive="webr" xml:id="prog-basic-plots">
        <code>
          # Load the iris dataset
          data(iris)
          
          # Histogram of Sepal.Length
          hist(iris$Sepal.Length, 
               main = "Distribution of Sepal Length",
               xlab = "Sepal Length (cm)",
               col = "lightblue",
               breaks = 10)
          
          # Add a density curve to a histogram
          hist(iris$Sepal.Length, 
               main = "Distribution of Sepal Length with Density Curve",
               xlab = "Sepal Length (cm)",
               col = "lightblue",
               freq = FALSE)  # Use density instead of frequency
          lines(density(iris$Sepal.Length), col = "red", lwd = 2)
          
          # Boxplot of Sepal.Width
          boxplot(iris$Sepal.Width,
                main = "Boxplot of Sepal Width",
                ylab = "Sepal Width (cm)",
                col = "lightgreen")
          
          # Boxplot by group
          boxplot(Sepal.Width ~ Species, data = iris,
                main = "Sepal Width by Species",
                xlab = "Species",
                ylab = "Sepal Width (cm)",
                col = c("lightpink", "lightblue", "lightgreen"))
          
          # Scatter plot of Sepal.Length vs. Sepal.Width
          plot(iris$Sepal.Length, iris$Sepal.Width,
               main = "Sepal Length vs. Sepal Width",
               xlab = "Sepal Length (cm)",
               ylab = "Sepal Width (cm)",
               pch = 19,
               col = "darkblue")
          
          # Scatter plot with color by species
          plot(iris$Sepal.Length, iris$Sepal.Width,
               main = "Sepal Length vs. Sepal Width by Species",
               xlab = "Sepal Length (cm)",
               ylab = "Sepal Width (cm)",
               pch = 19,
               col = as.numeric(iris$Species)) # Color points by species
          legend("topright", legend = levels(iris$Species), 
                 col = 1:3, pch = 19)
          
          # Bar plot of species counts
          species_counts &lt;- table(iris$Species)
          barplot(species_counts,
                main = "Count of Iris Flowers by Species",
                xlab = "Species",
                ylab = "Count",
                col = c("coral", "skyblue", "palegreen"))
          
          # Multiple plots in one figure
          par(mfrow = c(2, 2))  # 2x2 grid of plots
          
          # Plot 1: Histogram
          hist(iris$Sepal.Length, 
               main = "Sepal Length",
               xlab = "cm",
               col = "lightblue")
          
          # Plot 2: Boxplot
          boxplot(Sepal.Width ~ Species, data = iris,
                main = "Sepal Width by Species",
                xlab = "",
                col = c("pink", "lightblue", "lightgreen"))
          
          # Plot 3: Scatter plot
          plot(iris$Petal.Length, iris$Petal.Width,
               main = "Petal Length vs. Width",
               xlab = "Length (cm)",
               ylab = "Width (cm)",
               pch = 19,
               col = as.numeric(iris$Species))
          
          # Plot 4: Bar plot
          barplot(table(iris$Species),
                main = "Species Count",
                col = c("coral", "skyblue", "palegreen"))
          
          # Reset to single plot
          par(mfrow = c(1, 1))
          
          # Pairs plot (scatter plot matrix)
          pairs(iris[, 1:4], 
                main = "Iris Data Scatter Plot Matrix",
                pch = 21,
                bg = c("red", "green", "blue")[unclass(iris$Species)])
        </code>
      </program>
      
      <p>Additional plots for specific analysis needs:</p>
      
      <program interactive="webr" xml:id="prog-advanced-plots">
        <code>
          # Load datasets
          data(mtcars)
          data(iris)
          
          # Density plot
          plot(density(mtcars$mpg), 
               main = "Density Plot of MPG",
               xlab = "Miles Per Gallon",
               col = "blue",
               lwd = 2)
          
          # Overlaid density plots by group
          # First, split the data by cylinder
          mpg_by_cyl &lt;- split(mtcars$mpg, mtcars$cyl)
          
          # Create an empty plot with appropriate limits
          plot(density(mtcars$mpg), 
               main = "MPG Density by Number of Cylinders",
               xlab = "Miles Per Gallon",
               col = "white",  # Make the line invisible
               lwd = 2)
          
          # Add density curves for each cylinder group
          colors &lt;- c("red", "green", "blue")
          labels &lt;- names(mpg_by_cyl)
          
          for (i in 1:length(mpg_by_cyl)) {
            lines(density(mpg_by_cyl[[i]]), col = colors[i], lwd = 2)
          }
          
        # Add a legend
         legend("topright", 
               legend = paste(labels, "cylinders"),
               col = colors, 
               lwd = 2)
         
         # Correlation plot
         # Create correlation matrix
         cor_matrix &lt;- cor(mtcars)
         
         # Simple visualization of correlation matrix
         image(cor_matrix, 
               main = "Correlation Matrix Heatmap",
               axes = FALSE)
         axis(1, at = seq(0, 1, length.out = ncol(mtcars)), labels = colnames(mtcars), las = 2)
         axis(2, at = seq(0, 1, length.out = ncol(mtcars)), labels = colnames(mtcars), las = 2)
         
         # Add correlation values
         text(expand.grid(seq(0, 1, length.out = ncol(mtcars)), 
                          seq(0, 1, length.out = ncol(mtcars))), 
              labels = round(c(cor_matrix), 2),
              cex = 0.7)
         
         # Violin plot (combination of boxplot and density)
         # We'll create a simple version since violin plots are typically made with ggplot2
         
         # First, create side-by-side boxplots
         boxplot(Sepal.Length ~ Species, data = iris,
               main = "Sepal Length by Species",
               xlab = "Species",
               ylab = "Sepal Length (cm)",
               col = c("lightpink", "lightblue", "lightgreen"))
         
         # Now add density curves on the sides (simplified violin plot)
         for (i in 1:length(levels(iris$Species))) {
           species_data &lt;- iris$Sepal.Length[iris$Species == levels(iris$Species)[i]]
           # Calculate density
           dens &lt;- density(species_data)
           # Scale density to appropriate width
           scaled_dens &lt;- dens$y / max(dens$y) * 0.4
           # Plot the density curve
           lines(i + scaled_dens, dens$x, col = "darkred", lwd = 2)
           lines(i - scaled_dens, dens$x, col = "darkred", lwd = 2)
         }
        </code>
     </program>
     
     <exercise xml:id="ex-visualization-tasks">
       <title>Data Visualization Tasks</title>
       <statement>
         <p>Using the iris dataset, create the following visualizations:</p>
       </statement>
       <exercise>
         <statement>
           <p>Create a histogram of Petal.Length with an appropriate title, labels, and coloring.</p>
         </statement>
         <answer>
           <pre>
             hist(iris$Petal.Length, 
                  main = "Distribution of Petal Length",
                  xlab = "Petal Length (cm)",
                  ylab = "Frequency",
                  col = "lightgreen",
                  breaks = 12)
           </pre>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>Create a scatter plot of Petal.Length (x-axis) vs. Petal.Width (y-axis), with points colored by Species.</p>
         </statement>
         <answer>
           <pre>
             plot(iris$Petal.Length, iris$Petal.Width,
                  main = "Petal Length vs. Petal Width by Species",
                  xlab = "Petal Length (cm)",
                  ylab = "Petal Width (cm)",
                  pch = 19,
                  col = as.numeric(iris$Species))
             legend("topleft", 
                    legend = levels(iris$Species), 
                    col = 1:3, 
                    pch = 19,
                    title = "Species")
           </pre>
         </answer>
       </exercise>
     </exercise>
   </subsection>
   
   <!-- Exercise: Exploratory analysis of a dataset -->
   <subsection xml:id="subsec-exploratory-analysis">
     <title>Exercise: Exploratory analysis of a dataset</title>
     
     <activity xml:id="activity-exploratory-analysis">
       <title>Comprehensive Exploratory Data Analysis</title>
       <statement>
         <p>In this exercise, you will conduct a comprehensive exploratory data analysis (EDA) of the ToothGrowth dataset, which shows the effect of Vitamin C on tooth growth in guinea pigs.</p>
         
         <p>Instructions:</p>
         <ol>
           <li>
             <p>Load the ToothGrowth dataset using <c>data(ToothGrowth)</c></p>
           </li>
           <li>
             <p>Perform a thorough exploration of the dataset structure and summary statistics</p>
           </li>
           <li>
             <p>Create appropriate visualizations to understand:</p>
             <ul>
               <li>
                 <p>The distribution of tooth length</p>
               </li>
               <li>
                 <p>The relationship between dose and tooth length</p>
               </li>
               <li>
                 <p>The effect of supplement type on tooth length</p>
               </li>
               <li>
                 <p>The combined effect of dose and supplement type</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Document your findings and insights from the analysis</p>
           </li>
         </ol>
       </statement>
     </activity>
     
     <program interactive="webr" xml:id="prog-exploratory-analysis">
       <code>
         # Step 1: Load the dataset
         data(ToothGrowth)
         
         # Step 2: Data exploration
         # Your code here
         
         
         # Step 3: Create visualizations
         # Your code here
         
         
         # Step 4: Document findings
         # Your insights here
         
       </code>
     </program>
     
     <program interactive="webr" xml:id="prog-exploratory-analysis-solution">
       <code>
         # Step 1: Load the dataset
         data(ToothGrowth)
         
         # Step 2: Data exploration
         # Examine the structure
         str(ToothGrowth)
         
         # Look at the first few rows
         head(ToothGrowth)
         
         # Summary statistics
         summary(ToothGrowth)
         
         # Convert 'dose' to a factor for better analysis and visualization
         ToothGrowth$dose &lt;- as.factor(ToothGrowth$dose)
         
         # Check for missing values
         any(is.na(ToothGrowth))
         
         # Group summary statistics
         # Mean tooth length by supplement type
         aggregate(len ~ supp, data = ToothGrowth, FUN = mean)
         
         # Mean tooth length by dose
         aggregate(len ~ dose, data = ToothGrowth, FUN = mean)
         
         # Mean tooth length by supplement type and dose
         aggregate(len ~ supp + dose, data = ToothGrowth, FUN = mean)
         
         # Step 3: Create visualizations
         # Distribution of tooth length
         hist(ToothGrowth$len, 
              main = "Distribution of Tooth Length",
              xlab = "Tooth Length",
              col = "lightblue",
              breaks = 10)
         
         # Add density curve
         hist(ToothGrowth$len, 
              main = "Distribution of Tooth Length with Density Curve",
              xlab = "Tooth Length",
              col = "lightblue",
              freq = FALSE)
         lines(density(ToothGrowth$len), col = "red", lwd = 2)
         
         # Boxplot of tooth length by supplement type
         boxplot(len ~ supp, data = ToothGrowth,
               main = "Tooth Length by Supplement Type",
               xlab = "Supplement Type",
               ylab = "Tooth Length",
               col = c("orange", "lightskyblue"))
         
         # Boxplot of tooth length by dose
         boxplot(len ~ dose, data = ToothGrowth,
               main = "Tooth Length by Vitamin C Dose",
               xlab = "Dose (mg/day)",
               ylab = "Tooth Length",
               col = "lightgreen")
         
         # Boxplot of tooth length by supplement type and dose
         boxplot(len ~ supp:dose, data = ToothGrowth,
               main = "Tooth Length by Supplement Type and Dose",
               xlab = "Supplement Type and Dose",
               ylab = "Tooth Length",
               col = c("orange", "lightskyblue"))
         
         # Create a more readable combined boxplot
         par(mfrow=c(1,2))
         
         # OJ supplement
         boxplot(len ~ dose, data = ToothGrowth[ToothGrowth$supp == "OJ",],
               main = "Orange Juice (OJ)",
               xlab = "Dose (mg/day)",
               ylab = "Tooth Length",
               col = "orange")
         
         # VC supplement
         boxplot(len ~ dose, data = ToothGrowth[ToothGrowth$supp == "VC",],
               main = "Ascorbic Acid (VC)",
               xlab = "Dose (mg/day)",
               ylab = "Tooth Length",
               col = "lightskyblue")
         
         # Reset plot settings
         par(mfrow=c(1,1))
         
         # Interaction plot
         interaction.plot(x.factor = ToothGrowth$dose,
                         trace.factor = ToothGrowth$supp,
                         response = ToothGrowth$len,
                         fun = mean,
                         type = "b",
                         legend = TRUE,
                         trace.label = "Supplement Type",
                         xlab = "Dose (mg/day)",
                         ylab = "Mean Tooth Length",
                         main = "Interaction Plot: Dose and Supplement Type",
                         col = c("red", "blue"),
                         lwd = 2,
                         pch = c(16, 17))
         
         # Step 4: Document findings
         cat("\nKey Findings from Exploratory Data Analysis:\n\n")
         
         cat("1. The dataset contains", nrow(ToothGrowth), "observations of tooth growth in guinea pigs.\n")
         cat("2. Variables include tooth length (len), supplement type (supp: OJ or VC), and dose (0.5, 1, or 2 mg/day).\n")
         
         cat("\n3. Overall tooth length statistics:\n")
         cat("   - Mean:", mean(ToothGrowth$len), "\n")
         cat("   - Median:", median(ToothGrowth$len), "\n")
         cat("   - Range:", min(ToothGrowth$len), "to", max(ToothGrowth$len), "\n")
         
         cat("\n4. Effect of supplement type on tooth length:\n")
         supp_means &lt;- aggregate(len ~ supp, data = ToothGrowth, FUN = mean)
         cat("   - OJ (Orange Juice) mean:", supp_means[1, 2], "\n")
         cat("   - VC (Ascorbic Acid) mean:", supp_means[2, 2], "\n")
         
         cat("\n5. Effect of dose on tooth length:\n")
         dose_means &lt;- aggregate(len ~ dose, data = ToothGrowth, FUN = mean)
         cat("   - 0.5 mg mean:", dose_means[1, 2], "\n")
         cat("   - 1.0 mg mean:", dose_means[2, 2], "\n")
         cat("   - 2.0 mg mean:", dose_means[3, 2], "\n")
         
         cat("\n6. Key insights:\n")
         cat("   - Higher doses consistently lead to greater tooth length\n")
         cat("   - Orange juice (OJ) appears more effective than ascorbic acid (VC) at lower doses\n")
         cat("   - At the highest dose (2.0 mg), both supplements show similar effectiveness\n")
         cat("   - There appears to be an interaction between supplement type and dose\n")
       </code>
     </program>
     
     <exercise xml:id="ex-eda-interpretation">
       <title>EDA Interpretation Questions</title>
       <statement>
         <p>Based on the exploratory analysis of the ToothGrowth dataset, answer the following questions:</p>
       </statement>
       <exercise>
         <statement>
           <p>What appears to be the relationship between vitamin C dose and tooth growth?</p>
         </statement>
         <answer>
           <p>There is a clear positive relationship between vitamin C dose and tooth growth. As the dose increases from 0.5 mg to 1.0 mg to 2.0 mg, the average tooth length consistently increases as well. This suggests that higher doses of vitamin C promote greater tooth growth in guinea pigs, regardless of the delivery method (supplement type).</p>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>How does the effectiveness of the two supplement types (OJ and VC) compare at different doses?</p>
         </statement>
         <answer>
           <p>The effectiveness of the two supplements varies by dose:</p>
           <ul>
             <li><p>At the lowest dose (0.5 mg), orange juice (OJ) appears significantly more effective than ascorbic acid (VC) for promoting tooth growth</p></li>
             <li><p>At the medium dose (1.0 mg), orange juice still shows a noticeable advantage over ascorbic acid</p></li>
             <li><p>At the highest dose (2.0 mg), the difference between the two supplements becomes minimal, with both achieving similar tooth growth results</p></li>
           </ul>
           <p>This suggests that orange juice might be a more efficient delivery method for vitamin C at lower doses, but this advantage disappears at higher concentrations.</p>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>What additional analyses or visualizations might help further explore this dataset?</p>
         </statement>
         <answer>
           <p>Additional analyses and visualizations that could provide further insights include:</p>
           <ol>
             <li><p>Statistical tests (such as t-tests or ANOVA) to determine if the observed differences between groups are statistically significant</p></li>
             <li><p>A linear regression model to quantify the relationship between dose and tooth length, potentially including an interaction term between dose and supplement type</p></li>
             <li><p>Density plots for each combination of supplement type and dose to better visualize the distribution of tooth lengths in each group</p></li>
             <li><p>Residual plots to check for any patterns or outliers that might influence the conclusions</p></li>
             <li><p>If additional variables were available (such as guinea pig age, weight, or sex), including these in the analysis to control for potential confounding factors</p></li>
           </ol>
         </answer>
       </exercise>
     </exercise>
   </subsection>
 </section>

 <!-- TOPIC 7: DATA TRANSFORMATION -->
 <section xml:id="sec-data-transformation">
   <title>Data Transformation</title>
   
   <!-- Session 13: Filtering and selecting data -->
   <subsection xml:id="subsec-filtering-selecting">
     <title>Filtering and selecting data</title>
     
     <p>Data transformation involves manipulating and reshaping your data to prepare it for analysis. Two fundamental transformation operations are filtering (selecting rows) and selecting (choosing columns).</p>
     
     <p>In base R, you can filter and select data using indexing operations:</p>
     
     <ul>
       <li>
         <p><term>Filtering rows</term>: <c>df[condition, ]</c> - Select rows that meet a condition</p>
       </li>
       <li>
         <p><term>Selecting columns</term>: <c>df[, c("col1", "col2")]</c> or <c>df[, c(1, 3)]</c> - Select specific columns</p>
       </li>
       <li>
         <p><term>Combined operations</term>: <c>df[condition, c("col1", "col2")]</c> - Filter rows and select columns</p>
       </li>
       <li>
         <p><term>Subset function</term>: <c>subset(df, condition, select = c("col1", "col2"))</c> - More readable alternative</p>
       </li>
     </ul>
     
     <program interactive="webr" xml:id="prog-filter-select">
       <code>
         # Load the mtcars dataset
         data(mtcars)
         
         # Add rownames as a column for better demonstration
         mtcars$car_name &lt;- rownames(mtcars)
         
         # Basic row filtering
         # Cars with mpg &gt; 20
         high_mpg_cars &lt;- mtcars[mtcars$mpg &gt; 20, ]
         head(high_mpg_cars)
         
         # Cars with 6 cylinders
         six_cyl_cars &lt;- mtcars[mtcars$cyl == 6, ]
         head(six_cyl_cars)
         
         # Multiple conditions with AND (&amp;)
         efficient_six_cyl &lt;- mtcars[mtcars$mpg &gt; 20 &amp; mtcars$cyl == 6, ]
         efficient_six_cyl
         
         # Multiple conditions with OR (|)
         high_mpg_or_low_weight &lt;- mtcars[mtcars$mpg &gt; 25 | mtcars$wt &lt; 2, ]
         high_mpg_or_low_weight
         
         # Selecting specific columns
         # By column name
         mpg_hp &lt;- mtcars[, c("mpg", "hp", "car_name")]
         head(mpg_hp)
         
         # By column position
         first_three_cols &lt;- mtcars[, 1:3]
         head(first_three_cols)
         
         # Excluding columns with negative indexing
         without_rowname &lt;- mtcars[, -12]  # Exclude the car_name column
         head(without_rowname)
         
         # Combined filtering and selection
         efficient_cars_subset &lt;- mtcars[mtcars$mpg &gt; 25, c("mpg", "wt", "car_name")]
         efficient_cars_subset
         
         # Using the subset() function
         # Filter rows
         efficient_cars_alt &lt;- subset(mtcars, mpg &gt; 25)
         efficient_cars_alt$car_name
         
         # Select columns
         car_details &lt;- subset(mtcars, select = c("mpg", "hp", "wt", "car_name"))
         head(car_details)
         
         # Combined operation
         efficient_details &lt;- subset(mtcars, mpg &gt; 25, select = c("mpg", "hp", "wt", "car_name"))
         efficient_details
         
         # Filtering with %in% operator for multiple values
         specific_cyl &lt;- mtcars[mtcars$cyl %in% c(4, 8), ]
         table(specific_cyl$cyl)
         
         # Filtering with logical functions
         # Find rows with missing values (not applicable in mtcars, but showing syntax)
         # missing_rows &lt;- mtcars[!complete.cases(mtcars), ]
         
         # Filtering with text patterns (using grepl)
         merc_cars &lt;- mtcars[grepl("Merc", mtcars$car_name), ]
         merc_cars$car_name
       </code>
     </program>
     
     <exercise xml:id="ex-filter-select">
       <title>Filtering and Selecting Practice</title>
       <statement>
         <p>Using the iris dataset, complete the following filtering and selection tasks:</p>
       </statement>
       <exercise>
         <statement>
           <p>Select all flowers of the species 'virginica' with petal length greater than 5.5 cm.</p>
         </statement>
         <answer>
           <pre>
             # Load the iris dataset
             data(iris)
             
             # Filter for virginica species with petal length &gt; 5.5
             selected_flowers &lt;- iris[iris$Species == "virginica" &amp; iris$Petal.Length &gt; 5.5, ]
             selected_flowers
             
             # Alternative using subset()
             selected_flowers_alt &lt;- subset(iris, Species == "virginica" &amp; Petal.Length &gt; 5.5)
             selected_flowers_alt
           </pre>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>Create a new dataset containing only the sepal measurements (Sepal.Length and Sepal.Width) for flowers with sepal width less than 3.0 cm, regardless of species.</p>
         </statement>
         <answer>
           <pre>
             # Filter for sepal width &lt; 3.0 and select only sepal measurements
             sepal_data &lt;- iris[iris$Sepal.Width &lt; 3.0, c("Sepal.Length", "Sepal.Width")]
             head(sepal_data)
             
             # Alternative using subset()
             sepal_data_alt &lt;- subset(iris, Sepal.Width &lt; 3.0, select = c("Sepal.Length", "Sepal.Width"))
             head(sepal_data_alt)
           </pre>
         </answer>
       </exercise>
     </exercise>
   </subsection>
   
   <!-- Subsetting data frames -->
   <subsection xml:id="subsec-subsetting-dataframes">
     <title>Subsetting data frames</title>
     
     <p>Subsetting refers to extracting specific portions of a data frame based on various criteria. This is an extension of filtering and selecting, with some additional techniques.</p>
     
     <ul>
       <li>
         <p><term>Random sampling</term>: Selecting a random subset of rows</p>
       </li>
       <li>
         <p><term>Top/bottom rows</term>: Getting the first or last n rows</p>
       </li>
       <li>
         <p><term>Stratified sampling</term>: Sampling proportionally from different groups</p>
       </li>
       <li>
         <p><term>Logical indexing</term>: Using vectors of TRUE/FALSE to select elements</p>
       </li>
     </ul>
     
     <program interactive="webr" xml:id="prog-subsetting">
       <code>
         # Load datasets
         data(iris)
         
         # Random sampling
         # Set seed for reproducibility
         set.seed(123)
         
         # Sample 10 random rows
         random_sample &lt;- iris[sample(nrow(iris), 10), ]
         random_sample
         
         # Get top/bottom rows
         # First 5 rows (same as head())
         top_rows &lt;- iris[1:5, ]
         top_rows
         
         # Last 5 rows (same as tail())
         bottom_rows &lt;- iris[(nrow(iris)-4):nrow(iris), ]
         bottom_rows
         
         # Stratified sampling
         # Sample 10 rows proportionally from each species
         # First, split the data by species
         iris_by_species &lt;- split(iris, iris$Species)
         
         # Then sample from each group and combine
         set.seed(456)
         stratified_sample &lt;- do.call(rbind, lapply(iris_by_species, function(x) {
           x[sample(nrow(x), 10), ]
         }))
         
         # Check the distribution of species in the sample
         table(stratified_sample$Species)
         
         # Logical indexing for complex conditions
         # Create a logical vector
         is_large_sepal &lt;- iris$Sepal.Length &gt; 6.5
         is_wide_sepal &lt;- iris$Sepal.Width &gt; 3.0
         is_setosa &lt;- iris$Species == "setosa"
         
         # Combine logical conditions
         combined_condition &lt;- (is_large_sepal and is_wide_sepal) | is_setosa
         
         # Use the logical vector to subset
         complex_subset &lt;- iris[combined_condition, ]
         head(complex_subset)
         
         # Count how many rows match each individual condition
         sum(is_large_sepal)
         sum(is_wide_sepal)
         sum(is_setosa)
         
         # Count how many rows match the combined condition
         sum(combined_condition)
         
         # Subsetting using %in% for selecting multiple categories
         selected_species &lt;- iris[iris$Species %in% c("setosa", "virginica"), ]
         table(selected_species$Species)
         
         # Subsetting with which() - returns indices of TRUE values
         large_sepal_indices &lt;- which(iris$Sepal.Length &gt; 7)
         large_sepal_flowers &lt;- iris[large_sepal_indices, ]
         large_sepal_flowers
         
         # Finding the index of the maximum value
         max_petal_length_index &lt;- which.max(iris$Petal.Length)
         iris[max_petal_length_index, ]
         
         # Finding the index of the minimum value
         min_petal_length_index &lt;- which.min(iris$Petal.Length)
         iris[min_petal_length_index, ]
        </code>
     </program>
     
     <p>Real-world example of subsetting to create a training and testing set:</p>
     
     <program interactive="webr" xml:id="prog-train-test-split">
       <code>
         # Create a simple train/test split using subsetting
         data(iris)
         
         # Set seed for reproducibility
         set.seed(42)
         
         # Create a random index for training data (80% of data)
         train_indices &lt;- sample(1:nrow(iris), size = round(0.8 * nrow(iris)))
         
         # Create training and testing sets
         train_data &lt;- iris[train_indices, ]
         test_data &lt;- iris[-train_indices, ]  # The '-' means "not in train_indices"
         
         # Check the dimensions
         cat("Original data dimensions:", dim(iris), "\n")
         cat("Training data dimensions:", dim(train_data), "\n")
         cat("Testing data dimensions:", dim(test_data), "\n")
         
         # Verify species distribution
         cat("\nSpecies distribution in original data:\n")
         print(table(iris$Species))
         
         cat("\nSpecies distribution in training data:\n")
         print(table(train_data$Species))
         
         cat("\nSpecies distribution in testing data:\n")
         print(table(test_data$Species))
         
         # Example: Stratified sampling for train/test split
         # Split the data by species
         iris_by_species &lt;- split(iris, iris$Species)
         
         # Function to create a stratified split
         create_stratified_split &lt;- function(data_list, train_prop = 0.8) {
           train_data &lt;- list()
           test_data &lt;- list()
           
           for (i in 1:length(data_list)) {
             group_data &lt;- data_list[[i]]
             n_train &lt;- round(train_prop * nrow(group_data))
             
             # Random indices for training
             train_idx &lt;- sample(1:nrow(group_data), size = n_train)
             
             # Split the data
             train_data[[i]] &lt;- group_data[train_idx, ]
             test_data[[i]] &lt;- group_data[-train_idx, ]
           }
           
           # Combine all groups
           return(list(
             train = do.call(rbind, train_data),
             test = do.call(rbind, test_data)
           ))
         }
         
         # Create stratified split
         set.seed(42)
         strat_split &lt;- create_stratified_split(iris_by_species, 0.8)
         
         # Check dimensions
         cat("\nStratified sampling:\n")
         cat("Training data dimensions:", dim(strat_split$train), "\n")
         cat("Testing data dimensions:", dim(strat_split$test), "\n")
         
         # Verify species distribution
         cat("\nSpecies distribution in stratified training data:\n")
         print(table(strat_split$train$Species))
         
         cat("\nSpecies distribution in stratified testing data:\n")
         print(table(strat_split$test$Species))
        </code>
     </program>
     
     <exercise xml:id="ex-subsetting-practice">
       <title>Subsetting Practice</title>
       <statement>
         <p>Using the mtcars dataset, complete the following subsetting tasks:</p>
       </statement>
       <exercise>
         <statement>
           <p>Create a random sample of 8 cars from the dataset.</p>
         </statement>
         <answer>
           <pre>
             # Load dataset
             data(mtcars)
             
             # Set seed for reproducibility
             set.seed(123)
             
             # Sample 8 random cars
             car_sample &lt;- mtcars[sample(nrow(mtcars), 8), ]
             car_sample
           </pre>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>Find the car with the best fuel efficiency (highest mpg) and the car with the most horsepower (hp).</p>
         </statement>
         <answer>
           <pre>
             # Find the car with the highest mpg
             most_efficient_index &lt;- which.max(mtcars$mpg)
             most_efficient_car &lt;- mtcars[most_efficient_index, ]
             rownames(most_efficient_car)  # Toyota Corolla
             
             # Find the car with the most horsepower
             most_powerful_index &lt;- which.max(mtcars$hp)
             most_powerful_car &lt;- mtcars[most_powerful_index, ]
             rownames(most_powerful_car)  # Maserati Bora
           </pre>
         </answer>
       </exercise>
     </exercise>
   </subsection>
   
   <!-- Activity: Extract meaningful subsets from datasets -->
   <subsection xml:id="subsec-activity-extract-subsets">
     <title>Activity: Extract meaningful subsets from datasets</title>
     
     <activity xml:id="activity-meaningful-subsets">
       <title>Extracting Insights from Data Subsets</title>
       <statement>
         <p>In this activity, you will extract and analyze meaningful subsets from the mpg dataset (from the ggplot2 package, but we'll provide a version).</p>
         
         <p>The mpg dataset contains fuel economy data for 38 popular car models from 1999 to 2008, including variables such as manufacturer, model, year, engine displacement, number of cylinders, transmission type, and fuel economy ratings.</p>
         
         <p>Instructions:</p>
         <ol>
           <li>
             <p>Create the mpg dataset using the code provided</p>
           </li>
           <li>
             <p>Extract and analyze the following subsets:</p>
             <ul>
               <li>
                 <p>All compact cars (class = "compact")</p>
               </li>
               <li>
                 <p>All cars manufactured by Toyota</p>
               </li>
               <li>
                 <p>All 4-cylinder cars with highway fuel economy (hwy) greater than 30 mpg</p>
               </li>
               <li>
                 <p>Compare the average city fuel economy (cty) for cars with 4, 6, and 8 cylinders</p>
               </li>
               <li>
                 <p>Create a random sample of 15 cars and calculate their average fuel economy</p>
               </li>
             </ul>
           </li>
           <li>
             <p>For each subset, create appropriate summary statistics and visualizations</p>
           </li>
           <li>
             <p>Document your findings and insights from each subset</p>
           </li>
         </ol>
       </statement>
     </activity>
     
     <program interactive="webr" xml:id="prog-create-mpg">
       <code>
       # Create a simplified version of the mpg dataset
         mpg &lt;- data.frame(
           manufacturer = c("audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "audi", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "chevrolet", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "dodge", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "ford", "honda", "honda", "honda", "honda", "honda", "honda", "honda", "honda", "honda", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "hyundai", "jeep", "jeep", "jeep", "jeep", "jeep", "jeep", "jeep", "jeep", "land rover", "land rover", "land rover", "land rover", "lincoln", "lincoln", "lincoln", "mercury", "mercury", "mercury", "mercury", "nissan", "nissan", "nissan", "nissan", "nissan", "nissan", "nissan", "nissan", "nissan", "nissan", "nissan", "nissan", "nissan", "pontiac", "pontiac", "pontiac", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "subaru", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "toyota", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen", "volkswagen"),
           model = c("a4", "a4", "a4", "a4", "a4", "a4", "a4", "a4", "a4 quattro", "a4 quattro", "a4 quattro", "a4 quattro", "a4 quattro", "a4 quattro", "a4 quattro", "a4 quattro", "a6 quattro", "a6 quattro", "c1500 suburban 2wd", "c1500 suburban 2wd", "corvette", "corvette", "corvette", "corvette", "k1500 tahoe 4wd", "k1500 tahoe 4wd", "malibu", "malibu", "malibu", "malibu", "malibu", "malibu", "malibu", "caravan 2wd", "caravan 2wd", "caravan 2wd", "caravan 2wd", "caravan 2wd", "caravan 2wd", "caravan 2wd", "dakota pickup 4wd", "dakota pickup 4wd", "dakota pickup 4wd", "durango 4wd", "durango 4wd", "durango 4wd", "durango 4wd", "durango 4wd", "durango 4wd", "ram 1500 pickup 4wd", "ram 1500 pickup 4wd", "ram 1500 pickup 4wd", "ram 1500 pickup 4wd", "ram 1500 pickup 4wd", "ram 1500 pickup 4wd", "ram 1500 pickup 4wd", "ram 1500 pickup 4wd", "expedition 2wd", "expedition 2wd", "expedition 2wd", "explorer 4wd", "explorer 4wd", "explorer 4wd", "explorer 4wd", "explorer 4wd", "explorer 4wd", "f150 pickup 4wd", "f150 pickup 4wd", "f150 pickup 4wd", "f150 pickup 4wd", "f150 pickup 4wd", "f150 pickup 4wd", "f150 pickup 4wd", "f150 pickup 4wd", "f150 pickup 4wd", "mustang", "mustang", "mustang", "mustang", "mustang", "mustang", "mustang", "mustang", "mustang", "civic", "civic", "civic", "civic", "civic", "civic", "civic", "civic", "civic", "sonata", "sonata", "sonata", "sonata", "sonata", "sonata", "sonata", "sonata", "sonata", "tiburon", "tiburon", "tiburon", "tiburon", "tiburon", "grand cherokee 4wd", "grand cherokee 4wd", "grand cherokee 4wd", "grand cherokee 4wd", "grand cherokee 4wd", "grand cherokee 4wd", "grand cherokee 4wd", "grand cherokee 4wd", "range rover", "range rover", "range rover", "range rover", "navigator 2wd", "navigator 2wd", "navigator 2wd", "mountaineer 4wd", "mountaineer 4wd", "mountaineer 4wd", "mountaineer 4wd", "altima", "altima", "altima", "altima", "maxima", "maxima", "maxima", "maxima", "pathfinder 4wd", "pathfinder 4wd", "pathfinder 4wd", "pathfinder 4wd", "pathfinder 4wd", "grand prix", "grand prix", "grand prix", "forester awd", "forester awd", "forester awd", "forester awd", "impreza awd", "impreza awd", "impreza awd", "impreza awd", "impreza awd", "impreza awd", "impreza awd", "impreza awd", "impreza awd", "impreza awd", "4runner 4wd", "4runner 4wd", "4runner 4wd", "4runner 4wd", "4runner 4wd", "4runner 4wd", "4runner 4wd", "4runner 4wd", "camry", "camry", "camry", "camry", "camry", "camry", "camry", "camry", "camry", "camry", "camry solara", "camry solara", "camry solara", "camry solara", "camry solara", "camry solara", "corolla", "corolla", "corolla", "corolla", "corolla", "corolla", "land cruiser wagon 4wd", "land cruiser wagon 4wd", "toyota tacoma 4wd", "toyota tacoma 4wd", "toyota tacoma 4wd", "gti", "gti", "jetta", "jetta", "jetta", "jetta", "jetta", "jetta", "jetta", "jetta", "new beetle", "new beetle", "new beetle", "new beetle", "new beetle", "new beetle", "new beetle", "passat", "passat", "passat", "passat", "passat", "passat", "passat", "passat", "passat"),
           displ = c(1.8, 1.8, 2, 2, 2.8, 2.8, 3.1, 3.1, 1.8, 1.8, 2, 2, 2.8, 2.8, 3.1, 3.1, 2.8, 4.2, 5.3, 5.3, 5.7, 5.7, 6.2, 6.2, 5.3, 5.3, 2.4, 2.4, 3.1, 3.1, 3.5, 3.5, 3.5, 2.4, 2.4, 2.4, 3.3, 3.3, 3.3, 3.8, 3.7, 3.7, 4.7, 3.7, 3.7, 4.7, 4.7, 5.7, 5.9, 4.7, 4.7, 4.7, 5.7, 5.7, 5.7, 5.7, 5.9, 4.6, 5.4, 5.4, 4, 4, 4, 4.6, 4.6, 5, 4.2, 4.2, 4.6, 4.6, 4.6, 5.4, 5.4, 5.4, 5.4, 3.8, 3.8, 4, 4, 4.6, 4.6, 5.4, 5.4, 5.4, 1.6, 1.6, 1.6, 1.6, 1.8, 1.8, 1.8, 2, 2, 2.4, 2.4, 2.5, 2.5, 3.3, 3.3, 3.3, 3.3, 2, 2, 2, 2.7, 2.7, 3, 3.7, 3.7, 4, 4, 4.7, 4.7, 4.7, 4.7, 4.7, 4, 4, 4, 4, 4.4, 4.4, 5.4, 5.4, 5.4, 4, 4, 4, 4, 2.4, 2.4, 2.4, 2.4, 3, 3, 3, 3, 3.3, 3.5, 3.8, 3.8, 3.8, 5.7, 5.7, 3.8, 2.5, 2.5, 2.5, 2.5, 2.2, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.5, 2.7, 2.7, 3.4, 3.4, 4, 4, 4, 4.7, 2.2, 2.2, 2.2, 2.4, 2.4, 2.4, 2.4, 2.4, 2.4, 3, 2.2, 2.2, 2.4, 2.4, 3, 3, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 4.7, 4.7, 2.7, 3.4, 4, 1.8, 2, 1.9, 1.9, 2, 2, 2, 2, 2.5, 2.5, 1.8, 1.8, 1.9, 1.9, 2, 2, 2.5, 1.8, 1.8, 1.8, 2, 2, 2, 2.8, 2.8, 3.6),
           year = c(1999, 1999, 2008, 2008, 1999, 1999, 2008, 2008, 1999, 1999, 2008, 2008, 1999, 1999, 2008, 2008, 1999, 2008, 1999, 2008, 1999, 2008, 2008, 2008, 1999, 2008, 1999, 1999, 1999, 1999, 2008, 2008, 2008, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2008, 1999, 1999, 1999, 2008, 1999, 1999, 1999, 1999, 2008, 1999, 1999, 2008, 2008, 2008, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2008, 2008, 1999, 1999, 1999, 1999, 2008, 1999, 1999, 2008, 2008, 1999, 1999, 1999, 2008, 1999, 2008, 1999, 2008, 2008, 1999, 1999, 1999, 1999, 1999, 1999, 2008, 2008, 2008, 1999, 1999, 1999, 1999, 1999, 1999, 2008, 2008, 1999, 1999, 2008, 2008, 1999, 1999, 1999, 1999, 2008, 2008, 2008, 1999, 1999, 1999, 1999, 2008, 2008, 2008, 2008, 1999, 1999, 1999, 2008, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2008, 2008, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2008, 2008, 2008, 2008, 1999, 1999, 1999, 1999, 1999, 2008, 2008, 2008, 2008, 2008, 1999, 1999, 1999, 2008, 2008, 2008, 1999, 1999, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 1999, 2008),
           cyl = c(4, 4, 4, 4, 6, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 4, 6, 6, 6, 6, 6, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 8, 8, 8, 6, 6, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 6, 8, 8, 8, 8, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 4, 4, 4, 4, 6, 6, 4, 4, 4, 4, 4, 4, 8, 8, 4, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 6, 6, 6),
           trans = c("auto(l5)", "manual(m5)", "manual(m6)", "auto(av)", "auto(l5)", "manual(m5)", "auto(av)", "manual(m6)", "auto(l5)", "manual(m5)", "manual(m6)", "auto(av)", "auto(l5)", "manual(m5)", "auto(av)", "manual(m6)", "auto(l5)", "auto(s6)", "auto(l4)", "auto(l4)", "manual(m6)", "manual(m6)", "manual(m6)", "auto(s6)", "auto(l4)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "auto(l6)", "auto(l6)", "auto(l3)", "auto(l4)", "manual(m5)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l5)", "auto(l4)", "manual(m5)", "auto(l4)", "auto(l5)", "auto(l4)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l5)", "auto(l4)", "manual(m5)", "auto(l5)", "manual(m6)", "auto(l5)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l5)", "manual(m5)", "auto(l5)", "auto(l5)", "auto(l5)", "auto(l5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l4)", "auto(l5)", "auto(l4)", "manual(m5)", "auto(l5)", "manual(m5)", "auto(l5)", "manual(m5)", "auto(l5)", "manual(m5)", "manual(m5)", "auto(l4)", "auto(l4)", "manual(m5)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l5)", "auto(l5)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l5)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "manual(m5)", "manual(m5)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l5)", "auto(l5)", "auto(l5)", "auto(l5)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l5)", "auto(l5)", "auto(l5)", "auto(l4)", "auto(l5)", "auto(l5)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l5)", "auto(l5)", "auto(l4)", "auto(l4)", "auto(l4)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(a5)", "auto(a6)", "auto(s5)", "auto(l5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(s6)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)", "manual(m5)", "auto(l4)"),
           drv = c("f", "f", "f", "f", "f", "f", "f", "f", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "r", "r", "r", "r", "r", "r", "4", "4", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "r", "r", "r", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "r", "r", "r", "r", "r", "r", "r", "r", "r", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "r", "r", "r", "4", "4", "4", "4", "f", "f", "f", "f", "f", "f", "f", "f", "4", "4", "4", "4", "4", "r", "r", "r", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "4", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "4", "4", "4", "4", "4", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f", "f"),
           cty = c(18, 21, 20, 21, 16, 18, 18, 18, 18, 18, 20, 19, 15, 17, 17, 17, 15, 14, 11, 14, 16, 15, 16, 15, 14, 11, 19, 19, 18, 17, 17, 17, 17, 18, 18, 20, 17, 16, 16, 15, 14, 14, 13, 13, 13, 12, 12, 13, 12, 14, 12, 13, 13, 13, 13, 12, 12, 11, 11, 11, 14, 15, 15, 14, 11, 11, 12, 14, 13, 13, 13, 14, 13, 13, 12, 18, 18, 18, 16, 15, 15, 15, 15, 14, 28, 28, 27, 25, 25, 24, 25, 23, 24, 21, 21, 21, 20, 18, 18, 19, 19, 20, 19, 19, 20, 15, 15, 15, 14, 14, 13, 12, 12, 12, 12, 13, 12, 11, 11, 11, 12, 12, 12, 11, 17, 16, 16, 15, 19, 19, 19, 19, 19, 16, 16, 16, 16, 14, 14, 14, 15, 15, 15, 9, 9, 9, 22, 21, 21, 20, 20, 20, 21, 22, 21, 22, 22, 19, 19, 18, 18, 16, 15, 15, 12, 21, 21, 21, 21, 21, 31, 29, 31, 30, 26, 26, 22, 22, 22, 20, 21, 21, 27, 27, 18, 17, 19, 21, 21, 30, 31, 29, 26, 26, 28, 26, 26, 29, 29, 29, 28, 25, 24, 19, 25, 26, 26, 25, 25, 24, 21, 21, 18),
           hwy = c(29, 29, 31, 30, 26, 26, 27, 26, 26, 25, 28, 27, 25, 25, 25, 25, 24, 19, 15, 20, 26, 25, 26, 25, 17, 17, 27, 27, 26, 25, 26, 25, 26, 24, 24, 28, 22, 22, 23, 20, 17, 19, 17, 17, 18, 17, 16, 17, 15, 17, 18, 17, 15, 17, 15, 16, 15, 17, 17, 17, 17, 19, 19, 17, 17, 15, 15, 16, 16, 17, 16, 15, 15, 17, 15, 26, 27, 27, 25, 20, 20, 20, 20, 19, 33, 32, 32, 32, 32, 32, 34, 32, 30, 28, 27, 30, 28, 24, 24, 26, 27, 27, 28, 28, 25, 20, 18, 20, 19, 19, 17, 17, 16, 15, 15, 15, 18, 17, 19, 19, 17, 17, 16, 16, 24, 25, 23, 23, 26, 25, 26, 27, 26, 24, 25, 23, 25, 20, 20, 20, 20, 22, 20, 17, 15, 14, 29, 26, 27, 25, 27, 28, 28, 29, 29, 30, 30, 26, 26, 24, 24, 23, 20, 22, 17, 31, 30, 31, 32, 30, 35, 35, 35, 35, 29, 28, 24, 24, 24, 22, 28, 29, 31, 32, 24, 23, 26, 30, 28, 40, 38, 40, 38, 36, 40, 36, 35, 38, 35, 38, 38, 35, 32, 27, 35, 35, 35, 31, 32, 32, 27, 26, 26),  
           fl = c("p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "r", "r", "p", "p", "p", "p", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "e", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "d", "d", "d", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "r", "r", "p", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "p", "p", "d", "d", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p", "p"),
           class = c("compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "midsize", "midsize", "suv", "suv", "2seater", "2seater", "2seater", "2seater", "suv", "suv", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "minivan", "minivan", "minivan", "minivan", "minivan", "minivan", "minivan", "pickup", "pickup", "pickup", "suv", "suv", "suv", "suv", "suv", "suv", "pickup", "pickup", "pickup", "pickup", "pickup", "pickup", "pickup", "pickup", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "pickup", "pickup", "pickup", "pickup", "pickup", "pickup", "pickup", "pickup", "pickup", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "suv", "suv", "suv", "suv", "suv", "midsize", "midsize", "midsize", "compact", "compact", "compact", "compact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "suv", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "suv", "suv", "pickup", "pickup", "pickup", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "compact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "subcompact", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize", "midsize")
         )
         
         # Step 1: Explore the dataset
         # First, get a quick overview of the data
         str(mpg)
         head(mpg)
         summary(mpg)
         
         # Step 2: Extract and analyze the requested subsets
         
         # (a) All compact cars
         compact_cars &lt;- mpg[mpg$class == "compact", ]
         
         # Basic summary of compact cars
         cat("\n===== COMPACT CARS =====\n")
         cat("Number of compact cars:", nrow(compact_cars), "\n")
         
         # Summary statistics for compact cars
         compact_summary &lt;- summary(compact_cars[, c("cty", "hwy")])
         compact_summary
         
         # Distribution of manufacturers within compact cars
         table(compact_cars$manufacturer)
         
         # Visualize fuel economy of compact cars
         par(mfrow = c(1, 2))
         hist(compact_cars$cty, 
              main = "City MPG - Compact Cars",
              xlab = "MPG (City)",
              col = "lightblue")
         hist(compact_cars$hwy, 
              main = "Highway MPG - Compact Cars",
              xlab = "MPG (Highway)",
              col = "lightgreen")
         par(mfrow = c(1, 1))
         
         # (b) All cars manufactured by Toyota
         toyota_cars &lt;- mpg[mpg$manufacturer == "toyota", ]
         
         cat("\n===== TOYOTA CARS =====\n")
         cat("Number of Toyota cars:", nrow(toyota_cars), "\n")
         
         # Distribution of Toyota car types
         toyota_class_table &lt;- table(toyota_cars$class)
         toyota_class_table
         
         # Average fuel economy by car type for Toyota
         toyota_mpg_by_class &lt;- aggregate(cbind(cty, hwy) ~ class, data = toyota_cars, FUN = mean)
         toyota_mpg_by_class
         
         # Visualize Toyota models by class and MPG
         boxplot(hwy ~ class, data = toyota_cars,
                main = "Highway MPG by Toyota Vehicle Class",
                xlab = "Vehicle Class",
                ylab = "Highway MPG",
                col = "lightyellow")
         
         # (c) All 4-cylinder cars with highway fuel economy (hwy) greater than 30 mpg
         efficient_4cyl &lt;- mpg[mpg$cyl == 4 &amp; mpg$hwy &gt; 30, ]
         
         cat("\n===== EFFICIENT 4-CYLINDER CARS =====\n")
         cat("Number of 4-cylinder cars with hwy mpg &gt; 30:", nrow(efficient_4cyl), "\n")
         
         # Which manufacturers are producing these efficient cars?
         table(efficient_4cyl$manufacturer)
         
         # Average city and highway MPG for these efficient cars
         cat("Average city MPG:", mean(efficient_4cyl$cty), "\n")
         cat("Average highway MPG:", mean(efficient_4cyl$hwy), "\n")
         
         # Visualize these efficient cars
         plot(efficient_4cyl$displ, efficient_4cyl$hwy, 
              main = "Engine Displacement vs Highway MPG\nFor Efficient 4-Cylinder Cars",
              xlab = "Engine Displacement (liters)",
              ylab = "Highway MPG",
              pch = 19,
              col = "darkgreen")
         
         # (d) Compare average city fuel economy for cars with 4, 6, and 8 cylinders
         cyl_comparison &lt;- aggregate(cty ~ cyl, data = mpg, FUN = mean)
         cyl_comparison
         
         # Visualize the comparison
         barplot(cyl_comparison$cty, 
                names.arg = cyl_comparison$cyl,
                main = "Average City MPG by Number of Cylinders",
                xlab = "Number of Cylinders",
                ylab = "Average City MPG",
                col = "lightcoral",
                ylim = c(0, 25))
         
         # Additional analysis - relationship between cylinders and highway MPG
         hwy_comparison &lt;- aggregate(hwy ~ cyl, data = mpg, FUN = mean)
         
         # (e) Create a random sample of 15 cars
         set.seed(123)  # For reproducibility
         car_sample &lt;- mpg[sample(nrow(mpg), 15), ]
         
         cat("\n===== RANDOM SAMPLE OF 15 CARS =====\n")
         # Show the models in the sample
         car_sample$model
         
         # Calculate average fuel economy for the sample
         cat("Average city MPG in sample:", mean(car_sample$cty), "\n")
         cat("Average highway MPG in sample:", mean(car_sample$hwy), "\n")
         
         # Compare sample averages to population averages
         cat("\nComparison to overall averages:\n")
         cat("Overall average city MPG:", mean(mpg$cty), "\n")
         cat("Overall average highway MPG:", mean(mpg$hwy), "\n")
         
         # Step 3: Document findings and insights
         
         cat("\n\n===== SUMMARY OF FINDINGS =====\n")
         cat("1. Compact Cars:\n")
         cat("   - There are", nrow(compact_cars), "compact cars in the dataset\n")
         cat("   - Average city MPG:", mean(compact_cars$cty), "\n")
         cat("   - Average highway MPG:", mean(compact_cars$hwy), "\n")
         cat("   - Main manufacturers:", names(sort(table(compact_cars$manufacturer), decreasing = TRUE)[1:3]), "\n\n")
         
         cat("2. Toyota Vehicles:\n")
         cat("   - Toyota offers", length(toyota_class_table), "different vehicle classes\n")
         cat("   - Most common Toyota class:", names(which.max(toyota_class_table)), "\n")
         cat("   - Toyota's most fuel-efficient class (highway):", 
             toyota_mpg_by_class$class[which.max(toyota_mpg_by_class$hwy)], "\n\n")
         
         cat("3. Efficient 4-Cylinder Cars (MPG > 30):\n")
         cat("   - There are", nrow(efficient_4cyl), "4-cylinder cars with highway MPG > 30\n")
         cat("   - Top manufacturer of efficient cars:", names(which.max(table(efficient_4cyl$manufacturer))), "\n")
         cat("   - Average highway MPG:", mean(efficient_4cyl$hwy), "\n\n")
         
         cat("4. Cylinder Comparison:\n")
         cat("   - 4-cylinder cars average", cyl_comparison$cty[cyl_comparison$cyl == 4], "city MPG\n")
         cat("   - 6-cylinder cars average", cyl_comparison$cty[cyl_comparison$cyl == 6], "city MPG\n")
         cat("   - 8-cylinder cars average", cyl_comparison$cty[cyl_comparison$cyl == 8], "city MPG\n")
         cat("   - Each additional 2 cylinders reduces city MPG by approximately", 
             round((cyl_comparison$cty[cyl_comparison$cyl == 4] - cyl_comparison$cty[cyl_comparison$cyl == 8])/2, 1), 
             "MPG\n\n")
       </code>
     </program>
     
     <exercise xml:id="ex-mpg-insights">
       <title>Dataset Analysis Insights</title>
       <statement>
         <p>Based on the exploratory analysis of the mpg dataset, answer the following questions:</p>
       </statement>
       <exercise>
         <statement>
           <p>Why might car manufacturers continue to produce vehicles with more cylinders despite their lower fuel efficiency?</p>
         </statement>
         <answer>
           <p>Car manufacturers continue to produce vehicles with more cylinders despite lower fuel efficiency because:</p>
           <ul>
             <li><p>Higher cylinder engines typically provide more power and torque, which is necessary for larger vehicles like trucks and SUVs that need to haul heavy loads or tow trailers</p></li>
             <li><p>Many consumers prioritize performance, acceleration, and driving experience over fuel economy</p></li>
             <li><p>Vehicles with more cylinders often serve different market segments (luxury, performance, utility) where fuel efficiency is not the primary concern</p></li>
             <li><p>Some applications require the additional power that comes with more cylinders, such as off-road driving or commercial use</p></li>
           </ul>
           <p>The data shows that as cylinders increase, fuel economy decreases significantly, but this represents a design trade-off rather than a simple deficiency. For many use cases, the benefits of additional power outweigh the costs of reduced efficiency.</p>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>What patterns do you observe regarding the relationship between engine displacement, number of cylinders, and fuel economy?</p>
         </statement>
         <answer>
           <p>The analysis reveals several clear patterns regarding engine displacement, cylinders, and fuel economy:</p>
           <ol>
             <li><p>There is a strong negative correlation between engine displacement and fuel economy - as displacement increases, both city and highway MPG decrease</p></li>
             <li><p>The number of cylinders is directly related to engine displacement - vehicles with more cylinders typically have larger displacement engines</p></li>
             <li><p>4-cylinder engines show the best fuel economy, followed by 6-cylinder engines, with 8-cylinder engines being the least efficient</p></li>
             <li><p>Within the same cylinder count, smaller displacement engines tend to be more fuel-efficient than larger ones</p></li>
             <li><p>Efficient 4-cylinder cars (highway MPG > 30) almost exclusively have displacement below 2.5 liters</p></li>
             <li><p>The relationship appears to be non-linear - the MPG penalty for going from 4 to 6 cylinders is less severe than going from 6 to 8 cylinders</p></li>
           </ol>
           <p>These patterns reflect the fundamental engineering trade-offs in internal combustion engines, where greater displacement and more cylinders typically result in more power but reduced efficiency due to increased friction, weight, and fuel consumption.</p>
         </answer>
       </exercise>
     </exercise>
   </subsection>
 </section>

 <!-- TOPIC 7: DATA TRANSFORMATION (CONTINUED) -->
 <section xml:id="sec-creating-transforming-variables">
   <title>Creating and transforming variables</title>
   
   <!-- Session 14: Creating and transforming variables -->
   <subsection xml:id="subsec-adding-modifying-columns">
     <title>Adding and modifying columns</title>
     
     <p>Creating new variables and transforming existing ones are essential data preparation tasks. In R, you can easily add or modify columns in a data frame using various techniques.</p>
     
     <p>Common ways to add or modify columns:</p>
     <ul>
       <li>
         <p><term>Direct assignment</term>: <c>df$new_col &lt;- values</c></p>
       </li>
       <li>
         <p><term>Calculated columns</term>: <c>df$new_col &lt;- df$col1 + df$col2</c></p>
       </li>
       <li>
         <p><term>Applying functions</term>: <c>df$new_col &lt;- function(df$existing_col)</c></p>
       </li>
       <li>
         <p><term>Conditional assignment</term>: <c>df$new_col &lt;- ifelse(condition, value_if_true, value_if_false)</c></p>
       </li>
       <li>
         <p><term>Using transform()</term>: <c>df &lt;- transform(df, new_col = expression)</c></p>
       </li>
       <li>
         <p><term>Category creation</term>: <c>df$new_cat &lt;- cut(df$numeric_var, breaks, labels)</c></p>
       </li>
     </ul>
     
     <program interactive="webr" xml:id="prog-adding-columns">
       <code>
         # Create a sample dataset
         students &lt;- data.frame(
           id = 1:6,
           name = c("Alice", "Bob", "Charlie", "David", "Emma", "Frank"),
           math_score = c(85, 92, 78, 95, 88, 72),
           science_score = c(92, 88, 75, 89, 94, 77)
         )
         
         # Display the original data
         students
         
         # 1. Direct assignment - Add a new column
         students$english_score &lt;- c(88, 90, 82, 78, 95, 85)
         students
         
         # 2. Calculated columns - Calculate average score
         students$avg_score &lt;- (students$math_score + students$science_score + students$english_score) / 3
         students
         
         # 3. Using a function - Round the average score
         students$avg_score_rounded &lt;- round(students$avg_score)
         students
         
         # 4. Conditional assignment with ifelse() - Assign grades
         students$grade &lt;- ifelse(students$avg_score &gt;= 90, "A",
                              ifelse(students$avg_score &gt;= 80, "B",
                                     ifelse(students$avg_score &gt;= 70, "C",
                                            ifelse(students$avg_score &gt;= 60, "D", "F"))))
         students
         
         # 5. Using transform() - Add multiple columns at once
         students &lt;- transform(students,
                             math_pct = math_score / 100,
                             science_pct = science_score / 100,
                             english_pct = english_score / 100)
         students
         
         # 6. Creating categories with cut() - Group by performance
         students$performance_group &lt;- cut(students$avg_score,
                                        breaks = c(0, 70, 80, 90, 100),
                                        labels = c("Needs Improvement", "Satisfactory", "Good", "Excellent"),
                                        right = FALSE)
         students
         
         # 7. Creating a binary variable
         students$honor_roll &lt;- students$avg_score &gt;= 85
         students
         
         # 8. Modifying an existing column
         students$math_score &lt;- students$math_score + 2  # Curve everyone's math score
         students
         
         # 9. Creating an index or rank
         students$rank &lt;- rank(-students$avg_score)  # Negative for descending order
         students
         
         # 10. Using apply() to create a column
         # Calculate maximum score across all subjects
         students$max_score &lt;- apply(students[, c("math_score", "science_score", "english_score")], 1, max)
         students
       </code>
     </program>
     
     <p>More advanced transformations:</p>
     
     <program interactive="webr" xml:id="prog-advanced-transformations">
       <code>
         # Load the iris dataset
         data(iris)
         
         # Create a copy to work with
         iris_modified &lt;- iris
         
         # Z-score standardization (mean = 0, sd = 1)
         # For Sepal.Length
         iris_modified$Sepal.Length_z &lt;- (iris$Sepal.Length - mean(iris$Sepal.Length)) / sd(iris$Sepal.Length)
         
         # Check that the new variable has mean ≈ 0 and sd ≈ 1
         mean(iris_modified$Sepal.Length_z)
         sd(iris_modified$Sepal.Length_z)
         
         # Min-max scaling (range [0,1])
         # For Sepal.Width
         iris_modified$Sepal.Width_scaled &lt;- (iris$Sepal.Width - min(iris$Sepal.Width)) / 
                                         (max(iris$Sepal.Width) - min(iris$Sepal.Width))
         
         # Check that the range is [0,1]
         range(iris_modified$Sepal.Width_scaled)
         
         # Log transformation (useful for skewed data)
         # Create a sample skewed variable
         set.seed(123)
         skewed_data &lt;- data.frame(
           id = 1:100,
           value = rlnorm(100, meanlog = 2, sdlog = 1)  # Log-normal distribution
         )
         
         # Plot the original skewed data
         hist(skewed_data$value, 
              main = "Original Skewed Distribution",
              xlab = "Value",
              col = "lightblue")
         
         # Apply log transformation
         skewed_data$log_value &lt;- log(skewed_data$value)
         
         # Plot the transformed data
         hist(skewed_data$log_value, 
              main = "Log-Transformed Distribution",
              xlab = "Log(Value)",
              col = "lightgreen")
         
         # Square root transformation (another option for right-skewed data)
         skewed_data$sqrt_value &lt;- sqrt(skewed_data$value)
         
         # Plot the square root transformed data
         hist(skewed_data$sqrt_value, 
              main = "Square Root Transformed Distribution",
              xlab = "Sqrt(Value)",
              col = "lightpink")
         
         # Creating dummy variables (one-hot encoding)
         # For iris Species
         iris_dummy &lt;- iris
         
         # Create dummy variables for Species
         iris_dummy$is_setosa &lt;- as.integer(iris$Species == "setosa")
         iris_dummy$is_versicolor &lt;- as.integer(iris$Species == "versicolor")
         iris_dummy$is_virginica &lt;- as.integer(iris$Species == "virginica")
         
         # Check the dummy variables
         head(iris_dummy[, c("Species", "is_setosa", "is_versicolor", "is_virginica")])
         
         # Binning continuous variables
         # Bin Petal.Length into categories
         iris_dummy$petal_size &lt;- cut(iris$Petal.Length,
                                    breaks = c(0, 2, 4, 7),
                                    labels = c("small", "medium", "large"),
                                    include.lowest = TRUE)
         
         # Check the binning
         table(iris_dummy$petal_size, iris_dummy$Species)
         
         # String manipulations
         # Create a sample dataset with text
         text_data &lt;- data.frame(
           id = 1:5,
           full_name = c("John Smith", "Mary Johnson", "Robert Brown", "Linda Davis", "James Wilson")
         )
         
         # Extract first name
         text_data$first_name &lt;- sapply(strsplit(text_data$full_name, " "), `[`, 1)
         
         # Extract last name
         text_data$last_name &lt;- sapply(strsplit(text_data$full_name, " "), `[`, 2)
         
         # Create initials
         text_data$initials &lt;- paste0(substr(text_data$first_name, 1, 1), 
                                   substr(text_data$last_name, 1, 1))
         
         # Make names uppercase
         text_data$full_name_upper &lt;- toupper(text_data$full_name)
         
         text_data
       </code>
     </program>
     
     <exercise xml:id="ex-column-transformations">
       <title>Column Transformation Practice</title>
       <statement>
         <p>Using the mtcars dataset, perform the following transformations:</p>
       </statement>
       <exercise>
         <statement>
           <p>Create a new column called 'efficiency_ratio' that divides mpg by wt (miles per gallon per unit of weight).</p>
         </statement>
         <answer>
           <pre>
             data(mtcars)
             
             # Create efficiency ratio column
             mtcars$efficiency_ratio &lt;- mtcars$mpg / mtcars$wt
             
             # Check the results
             head(mtcars[, c("mpg", "wt", "efficiency_ratio")])
           </pre>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>Create a new categorical variable 'size_category' based on the 'wt' variable, with categories "Light" (less than 2.5), "Medium" (2.5 to 3.5), and "Heavy" (greater than 3.5).</p>
         </statement>
         <answer>
           <pre>
             # Create size category based on weight
             mtcars$size_category &lt;- cut(mtcars$wt, 
                                     breaks = c(0, 2.5, 3.5, Inf), 
                                     labels = c("Light", "Medium", "Heavy"))
             
             # Check the distribution of size categories
             table(mtcars$size_category)
           </pre>
         </answer>
       </exercise>
     </exercise>
   </subsection>
   
   <!-- Reshaping data (wide vs. long format) -->
   <subsection xml:id="subsec-reshaping-data">
     <title>Reshaping data (wide vs. long format)</title>
     
     <p>Data often needs to be reshaped between "wide" and "long" formats for different types of analysis or visualization. Understanding these formats and how to convert between them is an important skill in data science.</p>
     
     <p>Wide format vs. Long format:</p>
     <ul>
       <li>
         <p><term>Wide format</term>: Each subject/entity is in a single row, with multiple columns for different variables or time points</p>
       </li>
       <li>
         <p><term>Long format</term>: Each observation is in a separate row, with columns for ID variables, measurement variables, and values</p>
       </li>
     </ul>
     
     <p>In base R, reshaping can be done with the <c>reshape()</c> function:</p>
     
     <program interactive="webr" xml:id="prog-reshaping">
       <code>
         # Create a sample dataset in wide format
         # Student scores for different subjects
         student_scores_wide &lt;- data.frame(
           student_id = 1:5,
           name = c("Alice", "Bob", "Charlie", "Diana", "Eduardo"),
           math = c(85, 92, 78, 95, 88),
           science = c(92, 88, 75, 89, 94),
           english = c(88, 90, 82, 78, 95)
         )
         
         # Display the wide format data
         student_scores_wide
         
         # Reshape from wide to long format using reshape()
         student_scores_long &lt;- reshape(
           student_scores_wide,
           direction = "long",          # Reshape to long format
           varying = c("math", "science", "english"),  # Columns to reshape
           v.names = "score",           # Name for the values column
           timevar = "subject",         # Name for the variable names column
           idvar = c("student_id", "name"),  # ID columns to keep
           times = c("math", "science", "english"),  # Values for the timevar column
           new.row.names = 1:15         # New row names (5 students × 3 subjects)
         )
         
         # Display the long format data
         student_scores_long
         
         # Sort for better viewing
         student_scores_long &lt;- student_scores_long[order(student_scores_long$student_id), ]
         student_scores_long
         
         # Reshape back from long to wide format
         student_scores_wide2 &lt;- reshape(
           student_scores_long,
           direction = "wide",          # Reshape to wide format
           idvar = c("student_id", "name"),  # ID columns
           timevar = "subject"          # Variable that contains the column names
         )
         
         # Fix column names by removing "score." prefix
         names(student_scores_wide2) &lt;- gsub("score.", "", names(student_scores_wide2))
         
         # Display the result of reshaping back to wide format
         student_scores_wide2
         
         # Example with multiple measurement variables
         # Create a dataset with students' scores and absences in wide format
         student_data_wide &lt;- data.frame(
           student_id = 1:4,
           math_score = c(85, 92, 78, 95),
           science_score = c(92, 88, 75, 89),
           math_absences = c(2, 0, 3, 1),
           science_absences = c(1, 2, 4, 0)
         )
         
         student_data_wide
         
         # Reshape to long format with multiple measurement variables
         student_data_long &lt;- reshape(
           student_data_wide,
           direction = "long",
           varying = list(c("math_score", "science_score"), 
                         c("math_absences", "science_absences")),
           v.names = c("score", "absences"),
           timevar = "subject",
           times = c("math", "science"),
           idvar = "student_id"
         )
         
         # Sort for better viewing
         student_data_long &lt;- student_data_long[order(student_data_long$student_id), ]
         student_data_long
         
         # Another approach using melt function from reshape2 package (not run since we don't have the package in Web-R)
         # Commented out as it requires a package
         # install.packages("reshape2")
         # library(reshape2)
         # student_scores_long_alt &lt;- melt(student_scores_wide, 
         #                                id.vars = c("student_id", "name"),
         #                                measure.vars = c("math", "science", "english"),
         #                                variable.name = "subject",
         #                                value.name = "score")
         
        # Real-world example - Measuring multiple variables over time
         # Create a dataset of patient measurements over time
         patients &lt;- data.frame(
           patient_id = rep(1:3, each = 3),
           visit = rep(c("baseline", "month_3", "month_6"), 3),
           weight = c(70, 68, 67, 85, 83, 82, 62, 60, 61),
           blood_pressure = c(140, 135, 130, 150, 145, 140, 125, 120, 120),
           cholesterol = c(220, 200, 195, 240, 220, 210, 190, 185, 180)
         )
         
         # View the initial data (in wide format by visit)
         patients
         
         # Reshape to have all measurements for each patient-visit in a single row
         # We're already in long format for visit, but wide format for measurements
         
         # Let's reshape to have each measurement in its own row
         patients_long &lt;- reshape(
           patients,
           direction = "long",
           varying = c("weight", "blood_pressure", "cholesterol"),
           v.names = "value",
           timevar = "measurement",
           idvar = c("patient_id", "visit"),
           times = c("weight", "blood_pressure", "cholesterol")
         )
         
         # Sort for better viewing
         patients_long &lt;- patients_long[order(patients_long$patient_id, patients_long$visit), ]
         patients_long
         
         # Now let's reshape to have each patient in a single row with all visits and measurements
         # First, reshape to have measurements in columns
         patients_wide_meas &lt;- reshape(
           patients_long,
           direction = "wide",
           idvar = c("patient_id", "visit"),
           timevar = "measurement"
         )
         
         patients_wide_meas
         
         # Then reshape to have visits in columns
         patients_wide_all &lt;- reshape(
           patients_wide_meas,
           direction = "wide",
           idvar = "patient_id",
           timevar = "visit"
         )
         
         patients_wide_all
         
         # Cleaning up column names for better readability (not executed in this example)
         # Would need to rename columns like value.weight.baseline to baseline_weight
       </code>
     </program>
     
     <exercise xml:id="ex-reshaping-practice">
       <title>Data Reshaping Practice</title>
       <statement>
         <p>For each of the following scenarios, determine whether the data should be in wide or long format, and explain why:</p>
       </statement>
       <exercise>
         <statement>
           <p>Creating a line plot showing the trend of stock prices for multiple companies over time</p>
         </statement>
         <answer>
           <p>Long format would be most appropriate for this scenario.</p>
           <p>Reasons:</p>
           <ul>
             <li><p>Most plotting functions in R (especially ggplot2) work better with long-format data for this type of visualization</p></li>
             <li><p>In long format, each row would represent a single observation: one company's stock price on a specific date</p></li>
             <li><p>The data would have columns for: date, company name, and stock price</p></li>
             <li><p>This structure allows for easily grouping by company and creating lines for each company over time</p></li>
             <li><p>It also simplifies adding visual attributes like different colors for each company</p></li>
           </ul>
           <p>In contrast, wide format (with each company as a separate column) would make the plotting more difficult and would require reshaping before visualization.</p>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>Running a correlation analysis between different test scores (math, science, english) for a group of students</p>
         </statement>
         <answer>
           <p>Wide format would be most appropriate for this scenario.</p>
           <p>Reasons:</p>
           <ul>
             <li><p>Correlation analysis typically compares variables (columns) to each other</p></li>
             <li><p>In wide format, each row would represent one student, with separate columns for math score, science score, and English score</p></li>
             <li><p>Functions like <c>cor()</c> in R expect data in wide format, where each variable to be correlated is a separate column</p></li>
             <li><p>This structure allows for directly calculating the correlation matrix between all test scores</p></li>
             <li><p>Wide format is also more convenient for calculating descriptive statistics for each subject</p></li>
           </ul>
           <p>Using long format (with a single "score" column and a "subject" column) would require reshaping the data before performing the correlation analysis.</p>
         </answer>
       </exercise>
     </exercise>
   </subsection>
   
   <!-- Group work: Begin transforming a dataset for mid-term project -->
   <subsection xml:id="subsec-group-work-transformation">
     <title>Group work: Begin transforming a dataset for mid-term project</title>
     
     <activity xml:id="activity-dataset-transformation">
       <title>Dataset Transformation for Mid-Term Project</title>
       <statement>
         <p>In this activity, you will work with partners to prepare a dataset for your mid-term project. The goal is to apply the data transformation techniques you've learned to clean, reshape, and enhance a dataset for analysis.</p>
         
         <p>Instructions:</p>
         <ol>
           <li>
             <p>Form groups of 2-3 students</p>
           </li>
           <li>
             <p>Choose one of the following datasets:</p>
             <ul>
               <li>
                 <p>The built-in <c>airquality</c> dataset (daily air quality measurements in New York, 1973)</p>
               </li>
               <li>
                 <p>The built-in <c>mtcars</c> dataset (fuel consumption and design characteristics for car models)</p>
               </li>
               <li>
                 <p>The built-in <c>ChickWeight</c> dataset (weight versus age of chicks on different diets)</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Perform the following transformations on your chosen dataset:</p>
             <ul>
               <li>
                 <p>Clean any missing or problematic values</p>
               </li>
               <li>
                 <p>Create at least three new derived variables based on the existing data</p>
               </li>
               <li>
                 <p>Reshape the data as needed for your planned analysis</p>
               </li>
               <li>
                 <p>Create appropriate categorical variables using binning</p>
               </li>
               <li>
                 <p>Standardize or normalize numerical variables if appropriate</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Document your transformations and explain the reasoning behind each step</p>
           </li>
           <li>
             <p>Prepare a brief presentation of your transformed dataset and the insights it enables</p>
           </li>
         </ol>
       </statement>
     </activity>
     
     <program interactive="webr" xml:id="prog-dataset-transformation-example">
       <code>
         # Example transformation workflow for the airquality dataset
         # This is an example of what students might do in their group work
         
         # Load the dataset
         data(airquality)
         
         # Examine the structure
         str(airquality)
         
         # Look at the first few rows
         head(airquality)
         
         # Check for missing values
         summary(airquality)
         sum(is.na(airquality))
         colSums(is.na(airquality))
         
         # 1. Cleaning missing values
         # Method 1: Remove rows with any missing values
         airquality_complete &lt;- na.omit(airquality)
         dim(airquality)  # Original dimensions
         dim(airquality_complete)  # Dimensions after removing NAs
         
         # Method 2: Impute missing values with column means
         airquality_imputed &lt;- airquality
         for(col in names(airquality)) {
           airquality_imputed[is.na(airquality_imputed[, col]), col] &lt;- 
             mean(airquality[, col], na.rm = TRUE)
         }
         
         # Verify imputation
         colSums(is.na(airquality_imputed))
         
         # 2. Creating derived variables
         # Add a Date column by combining Month and Day
         airquality_imputed$Date &lt;- as.Date(paste("1973", airquality_imputed$Month, 
                                           airquality_imputed$Day, sep = "-"))
         
         # Create a Heat Index (simple version)
         airquality_imputed$HeatIndex &lt;- airquality_imputed$Temp + 0.05 * airquality_imputed$Temp * airquality_imputed$Wind
         
         # Create Air Quality Index (simplified version)
         airquality_imputed$AQI &lt;- (airquality_imputed$Ozone + airquality_imputed$Solar.R) / 2
         
         # Add a Weekend indicator
         # First, get the day of the week (1 = Monday, ... 7 = Sunday)
         airquality_imputed$Weekday &lt;- as.POSIXlt(airquality_imputed$Date)$wday
         # Create a binary weekend indicator (0 = weekday, 1 = weekend)
         airquality_imputed$Weekend &lt;- as.integer(airquality_imputed$Weekday %in% c(0, 6))
         
         # 3. Creating categorical variables
         # Bin temperature into categories
         airquality_imputed$TempCategory &lt;- cut(airquality_imputed$Temp, 
                                          breaks = c(50, 70, 80, 100), 
                                          labels = c("Cool", "Moderate", "Hot"),
                                          include.lowest = TRUE)
         
         # Categorize Ozone levels
         airquality_imputed$OzoneLevel &lt;- cut(airquality_imputed$Ozone,
                                        breaks = c(0, 30, 60, 100, 200),
                                        labels = c("Good", "Moderate", "Unhealthy for Sensitive Groups", "Unhealthy"),
                                        include.lowest = TRUE)
         
         # 4. Standardize numerical variables
         # Z-score standardization
         numeric_cols &lt;- c("Ozone", "Solar.R", "Wind", "Temp")
         for(col in numeric_cols) {
           col_z &lt;- paste0(col, "_z")
           airquality_imputed[, col_z] &lt;- scale(airquality_imputed[, col])
         }
         
         # 5. Reshape the data for time series analysis
         # First, prepare a dataset with just the variables of interest
         aq_ts &lt;- airquality_imputed[, c("Date", "Ozone", "Solar.R", "Wind", "Temp")]
         
         # Reshape to long format
         aq_long &lt;- reshape(
           aq_ts,
           direction = "long",
           varying = c("Ozone", "Solar.R", "Wind", "Temp"),
           v.names = "Value",
           timevar = "Measurement",
           idvar = "Date",
           times = c("Ozone", "Solar.R", "Wind", "Temp")
         )
         
         # Sort for better viewing
         aq_long &lt;- aq_long[order(aq_long$Date, aq_long$Measurement), ]
         
         # View the final transformed dataset
         head(airquality_imputed)
         
         # View the long format data
         head(aq_long)
         
         # 6. Analysis example: How do weather conditions affect air quality?
         # Correlation between variables
         cors &lt;- cor(airquality_imputed[, c("Ozone", "Solar.R", "Wind", "Temp")])
         cors
         
         # Average Ozone by temperature category
         aggregate(Ozone ~ TempCategory, data = airquality_imputed, FUN = mean)
         
         # Is air quality worse on weekends?
         aggregate(Ozone ~ Weekend, data = airquality_imputed, FUN = mean)
         
         # Visualize the relationship between temperature and ozone
         plot(airquality_imputed$Temp, airquality_imputed$Ozone,
              main = "Relationship Between Temperature and Ozone",
              xlab = "Temperature (F)",
              ylab = "Ozone (ppb)",
              pch = 19,
              col = "darkblue")
         abline(lm(Ozone ~ Temp, data = airquality_imputed), col = "red", lwd = 2)
        </code>
     </program>
     
     <exercise xml:id="ex-transformation-explanation">
       <title>Transformation Explanation</title>
       <statement>
         <p>For each of the following data transformations in the example above, explain why it might be useful for analysis:</p>
       </statement>
       <exercise>
         <statement>
           <p>Creating the Heat Index variable</p>
         </statement>
         <answer>
           <p>Creating the Heat Index variable is useful for analysis because:</p>
           <ul>
             <li><p>It combines temperature and wind data to create a measure of perceived temperature or thermal comfort</p></li>
             <li><p>This derived variable can be more relevant for understanding how weather conditions actually feel to humans than raw temperature alone</p></li>
             <li><p>It might correlate better with certain health outcomes or behavioral patterns than either temperature or wind speed individually</p></li>
             <li><p>It can be used to study how perceived temperature (rather than actual temperature) affects air quality metrics like ozone levels</p></li>
             <li><p>Creating composite indices like this often provides more intuitive and interpretable measures for analysis and visualization</p></li>
           </ul>
           <p>While the implementation in the example is simplified, it demonstrates how multiple variables can be combined to create more meaningful metrics for specific analytical purposes.</p>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>Categorizing Ozone levels into "Good", "Moderate", "Unhealthy for Sensitive Groups", and "Unhealthy"</p>
         </statement>
         <answer>
           <p>Categorizing Ozone levels is valuable for analysis because:</p>
           <ul>
             <li><p>It transforms a continuous numerical variable into meaningful categories that align with established air quality standards and health guidelines</p></li>
             <li><p>These categories have direct practical implications for public health recommendations and actions</p></li>
             <li><p>It simplifies communication of results to non-technical audiences, as descriptive categories like "Good" or "Unhealthy" are more intuitive than raw numerical values</p></li>
             <li><p>It facilitates certain types of analyses, such as contingency tables, chi-square tests, or categorical visualizations</p></li>
             <li><p>It can reveal non-linear relationships that might be obscured when using the continuous variable (e.g., certain effects might only appear at "Unhealthy" levels)</p></li>
             <li><p>It allows for comparisons with official air quality reporting formats, which typically use similar category systems</p></li>
           </ul>
           <p>This transformation bridges the gap between raw scientific measurements and meaningful health-related interpretations, making the data more useful for both analysis and decision-making.</p>
         </answer>
       </exercise>
     </exercise>
   </subsection>
 </section>

 <!-- TOPIC 8: WORKING WITH WEB DATA -->
 <section xml:id="sec-working-with-web-data">
   <title>Working with Web Data</title>
   
   <!-- Session 15: Introduction to APIs -->
   <subsection xml:id="subsec-intro-to-apis">
     <title>Introduction to APIs</title>
     
     <p>APIs (Application Programming Interfaces) allow different software applications to communicate with each other. In data science, APIs are often used to access data from web services, databases, or other applications.</p>
     
     <p>Key concepts about APIs:</p>
     <ul>
       <li>
         <p><term>API</term>: A set of rules and protocols for building and interacting with software applications</p>
       </li>
       <li>
         <p><term>RESTful API</term>: A common architectural style for web APIs that uses HTTP methods</p>
       </li>
       <li>
         <p><term>Endpoints</term>: Specific URLs where API requests are sent</p>
       </li>
       <li>
         <p><term>HTTP Methods</term>: GET, POST, PUT, DELETE, etc., used to perform different operations</p>
       </li>
       <li>
         <p><term>Parameters</term>: Values sent to the API to customize requests</p>
       </li>
       <li>
         <p><term>Authentication</term>: Methods to verify user identity (API keys, OAuth, etc.)</p>
       </li>
       <li>
         <p><term>Rate Limiting</term>: Restrictions on how many requests can be made in a given time period</p>
       </li>
       <li>
         <p><term>Response Formats</term>: Common formats include JSON, XML, CSV, etc.</p>
       </li>
     </ul>
     
     <p>In R, there are several packages for working with APIs, including <c>httr</c> for making HTTP requests and <c>jsonlite</c> for parsing JSON data.</p>
     
     <program interactive="webr" xml:id="prog-api-basics">
       <code>
         # Basic API concepts and demonstration
         # Note: Some code is commented out since we can't install packages or make actual API calls in Web-R
         
         # Example of how to make a basic API request using httr (not run)
         # install.packages("httr")
         # install.packages("jsonlite")
         # library(httr)
         # library(jsonlite)
         
         # Simple GET request (not run)
         # response &lt;- GET("https://api.example.com/data")
         
         # Check status code
         # status_code(response)
         
         # View headers
         # headers(response)
         
         # Extract content
         # content &lt;- content(response, "text")
         
         # Parse JSON
         # data &lt;- fromJSON(content)
         
         # Example of API request with parameters
         # params &lt;- list(
         #   param1 = "value1",
         #   param2 = "value2"
         # )
         # response_with_params &lt;- GET("https://api.example.com/data", query = params)
         
         # Example with authentication (API key)
         # api_key &lt;- "your_api_key_here"
         # auth_response &lt;- GET(
         #   "https://api.example.com/protected",
         #   add_headers(Authorization = paste("Bearer", api_key))
         # )
         
         # Instead of making real API calls, let's simulate the basic structure
         # This helps understand the concepts even without internet access
         
         # Simulate a JSON response
         simulated_json &lt;- '{
           "data": [
             {"id": 1, "name": "Item 1", "value": 42},
             {"id": 2, "name": "Item 2", "value": 73},
             {"id": 3, "name": "Item 3", "value": 29}
           ],
           "metadata": {
             "count": 3,
             "page": 1,
             "total_pages": 5
           }
         }'
         
         # Parse the simulated JSON
         # In a real scenario, this would come from an API response
         parsed_data &lt;- jsonlite::fromJSON(simulated_json)
         
         # Explore the structure
         str(parsed_data)
         
         # Access specific data
         parsed_data$data
         parsed_data$metadata$count
         
         # Convert the data part to a data frame
         # This is often done with API responses
         data_df &lt;- as.data.frame(parsed_data$data)
         data_df
         
         # Calculate summary statistics
         summary(data_df$value)
         
         # Examples of common data formats in APIs
         
         # JSON (JavaScript Object Notation)
         json_example &lt;- '{
           "person": {
             "name": "John Doe",
             "age": 30,
             "address": {
               "street": "123 Main St",
               "city": "Anytown",
               "zip": "12345"
             },
             "phoneNumbers": [
               {"type": "home", "number": "555-1234"},
               {"type": "work", "number": "555-5678"}
             ]
           }
         }'
         
         parsed_json &lt;- jsonlite::fromJSON(json_example)
         str(parsed_json)
         <![CDATA[
         # XML (eXtensible Markup Language)
         # In a real scenario, you would use the xml2 package
         xml_example &lt;- '<?xml version="1.0" encoding="UTF-8"?>
         <person>
           <name>John Doe</name>
           <age>30</age>
           <address>
             <street>123 Main St</street>
             <city>Anytown</city>
             <zip>12345</zip>
           </address>
           <phoneNumbers>
             <phone type="home">555-1234</phone>
             <phone type="work">555-5678</phone>
           </phoneNumbers>
         </person>'

         ]]>
         
         # CSV (Comma-Separated Values)
         csv_example &lt;- 'id,name,value
         1,Item 1,42
         2,Item 2,73
         3,Item 3,29'
         
         # Parse CSV string
         parsed_csv &lt;- read.csv(text = csv_example)
         parsed_csv
        </code>
     </program>
     
     <p>Examples of popular APIs for data science:</p>
     <ul>
       <li>
         <p><term>Weather APIs</term>: OpenWeatherMap, Weather Underground, National Weather Service</p>
       </li>
       <li>
         <p><term>Financial APIs</term>: Alpha Vantage, Yahoo Finance, FRED (Federal Reserve Economic Data)</p>
       </li>
       <li>
         <p><term>Social Media APIs</term>: Twitter API, Reddit API, YouTube Data API</p>
       </li>
       <li>
         <p><term>Government Data APIs</term>: Data.gov, Census API, WHO Data API</p>
       </li>
       <li>
         <p><term>Mapping APIs</term>: Google Maps, OpenStreetMap, Mapbox</p>
       </li>
     </ul>
     
     <exercise xml:id="ex-api-concepts">
       <title>API Concepts</title>
       <statement>
         <p>Match each API term with its correct description:</p>
       </statement>
       <matches>
         <match>
           <premise>REST API</premise>
           <response>An architectural style that uses HTTP requests to GET, PUT, POST, and DELETE data</response>
         </match>
         <match>
           <premise>Endpoint</premise>
           <response>A specific URL where API requests are sent</response>
         </match>
         <match>
           <premise>API Key</premise>
           <response>A unique identifier used to authenticate requests to the API</response>
         </match>
         <match>
           <premise>JSON</premise>
           <response>A common data format used for API responses, organized as key-value pairs</response>
         </match>
         <match>
           <premise>Rate Limiting</premise>
           <response>Restrictions on how many API requests can be made in a given time period</response>
         </match>
         <match>
           <premise>HTTP GET</premise>
           <response>A request method used to retrieve data from a server without modifying any resources</response>
         </match>
       </matches>
     </exercise>
   </subsection>
   
   <!-- What are APIs and how they work -->
   <subsection xml:id="subsec-using-r-with-apis">
     <title>Using R to access web data</title>
     
     <p>R provides several packages and functions for accessing web data, whether through APIs or direct web scraping. Here, we'll explore how to use R to access web data, focusing on APIs.</p>
     
     <p>Key packages for working with web data in R:</p>
     <ul>
       <li>
         <p><term>httr</term>: For making HTTP requests</p>
       </li>
       <li>
         <p><term>jsonlite</term>: For parsing JSON data</p>
       </li>
       <li>
         <p><term>xml2</term>: For parsing XML data</p>
       </li>
       <li>
         <p><term>rvest</term>: For web scraping HTML pages</p>
       </li>
       <li>
         <p><term>curl</term>: For making low-level HTTP requests</p>
       </li>
     </ul>
     
     <program interactive="webr" xml:id="prog-r-web-data">
       <code>
         # Working with web data in R
         # Note: Most code is commented out as we can't make actual web requests in Web-R
         
         # Basic structure of API requests in R
         
         # GET request (not run)
         # library(httr)
         # response &lt;- GET("https://api.example.com/data")
         
         # POST request with body data (not run)
         # post_data &lt;- list(name = "value", otherParam = "otherValue")
         # post_response &lt;- POST("https://api.example.com/create", 
         #                      body = post_data, 
         #                      encode = "json")
         
         # Working with query parameters (not run)
         # params &lt;- list(param1 = "value1", param2 = "value2")
         # param_response &lt;- GET("https://api.example.com/search", 
         #                      query = params)
         
         # Authentication methods
         
         # Basic authentication (not run)
         # basic_auth_response &lt;- GET("https://api.example.com/secure",
         #                           authenticate("username", "password"))
         
         # API key in header (not run)
         # key_response &lt;- GET("https://api.example.com/data",
         #                    add_headers(Authorization = "Bearer YOUR_API_KEY"))
         
         # API key as a parameter (not run)
         # key_param_response &lt;- GET("https://api.example.com/data",
         #                          query = list(api_key = "YOUR_API_KEY"))
         
         # Handling API responses
         
         # Check status code (not run)
         # if (status_code(response) == 200) {
         #   # Process successful response
         # } else {
         #   # Handle error
         #   print(paste("Error:", status_code(response)))
         # }
         
         # Parse JSON response (not run)
         # library(jsonlite)
         # content &lt;- content(response, "text")
         # data &lt;- fromJSON(content)
         
         # Parse XML response (not run)
         # library(xml2)
         # xml_content &lt;- content(response, "text")
         # xml_data &lt;- read_xml(xml_content)
         # xml_values &lt;- xml_find_all(xml_data, "//path/to/element")
         
         # Example of error handling (not run)
         # tryCatch({
         #   response &lt;- GET("https://api.example.com/data")
         #   if (status_code(response) != 200) {
         #     stop(paste("API request failed with status code:", status_code(response)))
         #   }
         #   content &lt;- content(response, "text")
         #   data &lt;- fromJSON(content)
         #   # Process data...
         # }, error = function(e) {
         #   message("Error: ", e$message)
         # }, warning = function(w) {
         #   message("Warning: ", w$message)
         # })
         
         # Working with paginated responses (not run)
         # all_data &lt;- list()
         # page &lt;- 1
         # more_pages &lt;- TRUE
         # 
         # while(more_pages) {
         #   response &lt;- GET("https://api.example.com/data",
         #                  query = list(page = page, per_page = 100))
         #   
         #   if (status_code(response) != 200) {
         #     stop("API request failed")
         #   }
         #   
         #   content &lt;- content(response, "text")
         #   data &lt;- fromJSON(content)
         #   
         #   all_data &lt;- c(all_data, list(data$results))
         #   
         #   # Check if there are more pages
         #   if (data$metadata$page &lt; data$metadata$total_pages) {
         #     page &lt;- page + 1
         #   } else {
         #     more_pages &lt;- FALSE
         #   }
         # }
         # 
         # # Combine all pages of data
         # final_data &lt;- do.call(rbind, all_data)
         
         # Web scraping example (not run)
         # library(rvest)
         # 
         # url &lt;- "https://example.com/table_page"
         # page &lt;- read_html(url)
         # 
         # # Extract tables
         # tables &lt;- html_table(page)
         # first_table &lt;- tables[[1]]  # Get the first table on the page
         # 
         # # Extract specific elements
         # titles &lt;- page %&gt;% 
         #   html_nodes("h2.title") %&gt;%
         #   html_text()
         # 
         # links &lt;- page %&gt;%
         #   html_nodes("a") %&gt;%
         #   html_attr("href")
         
         # Instead of making real web requests, let's create a simulated example
         # to demonstrate the workflow of processing API data
         
         # Simulate multiple pages of API data
         simulate_api_page &lt;- function(page, items_per_page = 5) {
           # Create simulated data
           start_id &lt;- (page - 1) * items_per_page + 1
           end_id &lt;- page * items_per_page
           
           # Generate simulated items
           items &lt;- data.frame(
             id = start_id:end_id,
             name = paste("Item", start_id:end_id),
             value = sample(1:100, items_per_page, replace = TRUE)
           )
           
           # Return the simulated page data
           list(
             results = items,
             metadata = list(
               page = page,
               per_page = items_per_page,
               total_items = 25,
               total_pages = ceiling(25 / items_per_page)
             )
           )
         }
         
         # Simulate the API workflow
         all_data &lt;- list()
         page &lt;- 1
         more_pages &lt;- TRUE
         
         while(more_pages) {
           # Simulate an API call
           data &lt;- simulate_api_page(page)
           
           # Store the results
           all_data &lt;- c(all_data, list(data$results))
           
           # Check if there are more pages
           if (data$metadata$page &lt; data$metadata$total_pages) {
             page &lt;- page + 1
           } else {
             more_pages &lt;- FALSE
           }
           
           # Print progress
           cat("Retrieved page", data$metadata$page, "of", data$metadata$total_pages, "\n")
         }
         
         # Combine all pages of data
         final_data &lt;- do.call(rbind, all_data)
         
         # View the combined data
         final_data
         
         # Analyze the data
         summary(final_data$value)
         
         # Create a simple visualization
         hist(final_data$value, 
              main = "Distribution of Item Values",
              xlab = "Value",
              col = "lightblue",
              breaks = 10)
        </code>
     </program>
     
     <exercise xml:id="ex-api-workflow">
       <title>API Workflow</title>
       <statement>
         <p>Put the following steps in the correct order for retrieving and processing data from an API:</p>
       </statement>
       <orderedlist>
         <item>Make an HTTP request to the API endpoint</item>
         <item>Convert the JSON response to a data frame</item>
         <item>Check the HTTP status code to ensure the request was successful</item>
         <item>Perform data analysis and visualization</item>
         <item>Extract the response content as text</item>
         <item>Parse the text into a structured format (e.g., JSON)</item>
       </orderedlist>
       <answer>
         <orderedlist>
           <item>Make an HTTP request to the API endpoint</item>
           <item>Check the HTTP status code to ensure the request was successful</item>
           <item>Extract the response content as text</item>
           <item>Parse the text into a structured format (e.g., JSON)</item>
           <item>Convert the JSON response to a data frame</item>
           <item>Perform data analysis and visualization</item>
         </orderedlist>
       </answer>
     </exercise>
   </subsection>
   
   <!-- Demonstration: Extracting data from a simple API -->
   <subsection xml:id="subsec-api-demonstration">
     <title>Demonstration: Extracting data from a simple API</title>
     
     <p>In this section, we'll walk through a detailed example of how to extract and analyze data from a public API. Since we can't make actual API calls in Web-R, we'll simulate the process using realistic data structures and workflows.</p>
     
     <p>We'll use a simulated weather API as our example, as weather data is commonly used in data science projects and is readily available through many public APIs.</p>
     
     <program interactive="webr" xml:id="prog-weather-api-example">
       <code>
         # Simulated Weather API Example
         
         # Function to simulate weather API response for a city
         simulate_weather_api &lt;- function(city, days = 7) {
           # Simulate different base temperatures and conditions based on city
           city_info &lt;- list(
             "New York" = list(base_temp = 20, conditions = c("Clear", "Partly Cloudy", "Cloudy", "Rain", "Thunderstorm")),
             "Los Angeles" = list(base_temp = 25, conditions = c("Clear", "Partly Cloudy", "Sunny", "Fog")),
             "Chicago" = list(base_temp = 18, conditions = c("Clear", "Partly Cloudy", "Cloudy", "Rain", "Snow")),
             "Miami" = list(base_temp = 28, conditions = c("Clear", "Partly Cloudy", "Sunny", "Rain", "Thunderstorm")),
             "Denver" = list(base_temp = 15, conditions = c("Clear", "Partly Cloudy", "Sunny", "Rain", "Snow"))
           )
           
           # Default to New York if city not found
           if (!city %in% names(city_info)) {
             city &lt;- "New York"
           }
           
           # Get city base info
           base_temp &lt;- city_info[[city]]$base_temp
           possible_conditions &lt;- city_info[[city]]$conditions
           
           # Create dates
           current_date &lt;- Sys.Date()
           forecast_dates &lt;- current_date + 0:(days-1)
           
           # Generate random weather data
           forecast &lt;- data.frame(
             date = as.character(forecast_dates),
             day_of_week = weekdays(forecast_dates),
             temp_high = base_temp + sample(-3:7, days, replace = TRUE),
             temp_low = base_temp - sample(5:10, days, replace = TRUE),
             humidity = sample(40:90, days, replace = TRUE),
             wind_speed = round(runif(days, 0, 15), 1),
             precipitation = round(rexp(days, 1/2) * 10, 1),
             conditions = sample(possible_conditions, days, replace = TRUE),
             stringsAsFactors = FALSE
           )
           
           # Create an API-like response structure
           response &lt;- list(
             location = list(
               city = city,
               country = "United States",
               latitude = runif(1, 25, 49),
               longitude = runif(1, -125, -70)
             ),
             forecast = forecast,
             metadata = list(
               generated_at = Sys.time(),
               days = days,
               units = list(
                 temperature = "Celsius",
                 wind_speed = "km/h",
                 precipitation = "mm"
               )
             )
           )
           
           return(response)
         }
         
         # Simulate API calls for multiple cities
         cities &lt;- c("New York", "Los Angeles", "Chicago", "Miami", "Denver")
         weather_data &lt;- list()
         
         for (city in cities) {
           # Simulate API call
           cat("Fetching weather data for", city, "\n")
           response &lt;- simulate_weather_api(city)
           weather_data[[city]] &lt;- response
         }
         
         # Function to extract forecast data from API responses
         extract_forecast &lt;- function(weather_data) {
           result &lt;- data.frame()
           
           for (city_name in names(weather_data)) {
             city_data &lt;- weather_data[[city_name]]
             forecast &lt;- city_data$forecast
             
             # Add city name to forecast data
             forecast$city &lt;- city_name
             
             # Combine with previous results
             result &lt;- rbind(result, forecast)
           }
           
           return(result)
         }
         
         # Extract and combine forecast data from all cities
         all_forecasts &lt;- extract_forecast(weather_data)
         
         # Convert date strings to Date objects for proper sorting
         all_forecasts$date &lt;- as.Date(all_forecasts$date)
         
         # View the combined dataset
         head(all_forecasts)
         
         # Basic data analysis
         
         # Summary statistics for temperature by city
         city_temp_summary &lt;- aggregate(
           cbind(temp_high, temp_low) ~ city, 
           data = all_forecasts, 
           FUN = function(x) c(mean = mean(x), min = min(x), max = max(x))
         )
         city_temp_summary
         
         # Average precipitation by city
         city_precip &lt;- aggregate(
           precipitation ~ city, 
           data = all_forecasts, 
           FUN = mean
         )
         city_precip
         
         # Find the city with the highest average temperature
         city_with_highest_temp &lt;- city_temp_summary[which.max(city_temp_summary$temp_high[, "mean"]), "city"]
         cat("City with highest average temperature:", city_with_highest_temp, "\n")
         
         # Find days with extreme weather conditions
         extreme_days &lt;- all_forecasts[all_forecasts$precipitation &gt; 15 | 
                                    all_forecasts$wind_speed &gt; 10, ]
         if (nrow(extreme_days) &gt; 0) {
           cat("Days with extreme weather conditions:\n")
           print(extreme_days[, c("city", "date", "conditions", "precipitation", "wind_speed")])
         } else {
           cat("No days with extreme weather conditions found\n")
         }
         
         # Visualize the data
         
         # Temperature comparison by city
         boxplot(temp_high ~ city, data = all_forecasts,
                main = "High Temperature Comparison by City",
                xlab = "City",
                ylab = "Temperature (°C)",
                col = "lightblue")
         
         # Visualization of temperature range by city
         plot(1:nrow(city_temp_summary), city_temp_summary$temp_high[, "mean"],
              type = "n",  # Empty plot
              xlim = c(0.5, nrow(city_temp_summary) + 0.5),
              ylim = c(min(city_temp_summary$temp_low[, "min"]) - 2, 
                      max(city_temp_summary$temp_high[, "max"]) + 2),
              xlab = "City",
              ylab = "Temperature (°C)",
              main = "Temperature Range by City",
              xaxt = "n")  # No x-axis labels initially
         
         # Add x-axis labels
         axis(1, at = 1:nrow(city_temp_summary), labels = city_temp_summary$city)
         
         # Add temperature ranges
         for (i in 1:nrow(city_temp_summary)) {
           # Draw line for temperature range
           lines(c(i, i), 
                c(city_temp_summary$temp_low[i, "min"], city_temp_summary$temp_high[i, "max"]),
                lwd = 2, col = "darkgray")
           
           # Add points for min, mean, and max temperatures
           points(i, city_temp_summary$temp_low[i, "min"], pch = 25, bg = "blue", cex = 1.2)
           points(i, city_temp_summary$temp_low[i, "mean"], pch = 22, bg = "lightblue", cex = 1.2)
           points(i, city_temp_summary$temp_high[i, "mean"], pch = 22, bg = "pink", cex = 1.2)
           points(i, city_temp_summary$temp_high[i, "max"], pch = 24, bg = "red", cex = 1.2)
         }
         
         # Add legend
         legend("topright", 
                legend = c("Min", "Mean Low", "Mean High", "Max"), 
                pch = c(25, 22, 22, 24),
                pt.bg = c("blue", "lightblue", "pink", "red"),
                title = "Temperature (°C)")
       </code>
     </program>
     
     <p>In a real-world scenario, you would need to consider several additional factors when working with APIs:</p>
     
     <ul>
       <li>
         <p><term>API Documentation</term>: Always consult the API documentation to understand available endpoints, parameters, and response formats</p>
       </li>
       <li>
         <p><term>Authentication</term>: Most production APIs require authentication through API keys or OAuth</p>
       </li>
       <li>
         <p><term>Rate Limiting</term>: Be mindful of rate limits and implement appropriate delays or retry mechanisms</p>
       </li>
       <li>
         <p><term>Error Handling</term>: Implement robust error handling to deal with network issues, API errors, and unexpected responses</p>
       </li>
       <li>
         <p><term>Data Validation</term>: Validate the data received before processing it</p>
       </li>
       <li>
         <p><term>Caching</term>: Consider caching responses to reduce API calls and improve performance</p>
       </li>
     </ul>
     
     <exercise xml:id="ex-api-usage">
       <title>API Usage</title>
       <statement>
         <p>Consider a scenario where you need to analyze weather data for 100 cities over a period of one year. What challenges might you face when using a weather API for this purpose, and how would you address them?</p>
       </statement>
       <answer>
         <p>When analyzing weather data for 100 cities over one year using a weather API, several challenges would arise:</p>
         
         <p><strong>1. Rate Limiting and API Quotas:</strong></p>
         <ul>
           <li><p><em>Challenge</em>: Most weather APIs impose rate limits (e.g., 1000 calls/day or 60 calls/minute)</p></li>
           <li><p><em>Solution</em>: Implement throttling to stay within limits (pause between requests), batch requests where possible, and spread data collection over multiple days</p></li>
         </ul>
         
         <p><strong>2. Data Volume Management:</strong></p>
         <ul>
           <li><p><em>Challenge</em>: 100 cities × 365 days = 36,500 requests, potentially generating gigabytes of data</p></li>
           <li><p><em>Solution</em>: Create an efficient storage structure (database or optimized files), implement incremental processing, and consider data compression techniques</p></li>
         </ul>
         
         <p><strong>3. Cost Considerations:</strong></p>
         <ul>
           <li><p><em>Challenge</em>: Many weather APIs charge for high-volume usage</p></li>
           <li><p><em>Solution</em>: Compare pricing plans across providers, consider historical bulk data purchases instead of individual requests, and optimize query structure to minimize calls</p></li>
         </ul>
         
         <p><strong>4. Handling Missing or Erroneous Data:</strong></p>
         <ul>
           <li><p><em>Challenge</em>: API outages, missing weather stations, or corrupted records</p></li>
           <li><p><em>Solution</em>: Implement robust error handling, data validation checks, imputation strategies for missing values, and possibly use multiple data sources for validation</p></li>
         </ul>
         
         <p><strong>5. API Changes or Deprecation:</strong></p>
         <ul>
           <li><p><em>Challenge</em>: APIs may change structure or endpoints during a year-long project</p></li>
           <li><p><em>Solution</em>: Build an abstraction layer between API and analysis code, monitor for API announcements, and design for easy adaptation</p></li>
         </ul>
         
         <p><strong>6. Consistency in Data Collection:</strong></p>
         <ul>
           <li><p><em>Challenge</em>: Ensuring consistent timing and parameters across all requests</p></li>
           <li><p><em>Solution</em>: Develop automated scripts with logging, implement data quality checks, and standardize request parameters</p></li>
         </ul>
         
         <p><strong>7. Time Zone Management:</strong></p>
         <ul>
           <li><p><em>Challenge</em>: Cities across different time zones complicating temporal analysis</p></li>
           <li><p><em>Solution</em>: Store timestamps in UTC with time zone information, and implement time zone conversion functions for analysis</p></li>
         </ul>
         
         <p>An effective implementation would involve creating a robust data pipeline with error handling, rate limiting, logging, and storage optimization, potentially running on a scheduled basis to collect data incrementally.</p>
       </answer>
     </exercise>
   </subsection>
   
   <!-- Web data project work -->
   <subsection xml:id="subsec-web-project-work">
     <title>Web data project work</title>
     
     <activity xml:id="activity-web-data-project">
       <title>Web Data Analysis Project</title>
       <statement>
         <p>In this activity, you will work with your partner to design and plan a data analysis project that utilizes web data. Since we can't make actual API calls in Web-R, the implementation will be simulated, but the planning process will follow real-world practices.</p>
         
         <p>Instructions:</p>
         <ol>
           <li>
             <p>Work with a partner to select one of the following project scenarios:</p>
             <ul>
               <li>
                 <p>Analyzing weather patterns and their relationship with energy consumption</p>
               </li>
               <li>
                 <p>Tracking stock prices and financial news sentiment for selected companies</p>
               </li>
               <li>
                 <p>Analyzing social media trends around specific topics or events</p>
               </li>
               <li>
                 <p>Comparing economic indicators across different regions or countries</p>
               </li>
               <li>
                 <p>Analyzing public health data in relation to environmental factors</p>
               </li>
             </ul>
           </li>
           <li>
             <p>For your chosen scenario, design a data collection and analysis plan that includes:</p>
             <ul>
               <li>
                 <p>The specific research question(s) you want to answer</p>
               </li>
               <li>
                 <p>The APIs or web data sources you would use</p>
               </li>
               <li>
                 <p>The data collection strategy (frequency, scope, parameters)</p>
               </li>
               <li>
                 <p>The data storage approach</p>
               </li>
               <li>
                 <p>The data transformations and analyses you would perform</p>
               </li>
               <li>
                 <p>The expected insights and how they would be presented</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Create a rough pseudocode or flowchart outlining the steps in your data pipeline</p>
           </li>
           <li>
             <p>Identify potential challenges and how you would address them</p>
           </li>
           <li>
             <p>Design a sample visualization that you would create with the collected data</p>
           </li>
         </ol>
       </statement>
     </activity>
     
     <program interactive="webr" xml:id="prog-project-example">
       <code>
         # Example of a simulated data analysis project: Weather and Energy Consumption
         # This is a simplified simulation of what a real project might involve
         
         # 1. Define project parameters
         cities &lt;- c("New York", "Chicago", "Los Angeles", "Houston", "Phoenix")
         start_date &lt;- as.Date("2024-01-01")
         end_date &lt;- as.Date("2024-03-31")
         date_range &lt;- seq(start_date, end_date, by = "day")
         
         # 2. Simulate data collection function (in reality, this would call an API)
         simulate_weather_data &lt;- function(city, date) {
           # Base temperature differs by city
           base_temps &lt;- list(
             "New York" = list(winter = 0, spring = 15, summer = 25, fall = 10),
             "Chicago" = list(winter = -5, spring = 10, summer = 23, fall = 8),
             "Los Angeles" = list(winter = 15, spring = 20, summer = 25, fall = 22),
             "Houston" = list(winter = 10, spring = 20, summer = 30, fall = 18),
             "Phoenix" = list(winter = 15, spring = 25, summer = 35, fall = 20)
           )
           
           # Determine season
           month &lt;- as.numeric(format(date, "%m"))
           season &lt;- case_when(
             month %in% c(12, 1, 2) ~ "winter",
             month %in% c(3, 4, 5) ~ "spring",
             month %in% c(6, 7, 8) ~ "summer",
             month %in% c(9, 10, 11) ~ "fall"
           )
           
           # Generate weather data
           base_temp &lt;- base_temps[[city]][[season]]
           daily_variation &lt;- rnorm(1, mean = 0, sd = 5)
           
           # Add some randomness based on the day of year to simulate seasonal patterns
           day_of_year &lt;- as.numeric(format(date, "%j"))
           seasonal_effect &lt;- sin(2 * pi * day_of_year / 365) * 10
           
           # Temperature calculation
           temperature &lt;- base_temp + daily_variation + seasonal_effect
           
           # Precipitation calculation - more likely in certain seasons
           precip_prob &lt;- case_when(
             season == "winter" ~ 0.3,
             season == "spring" ~ 0.4,
             season == "summer" ~ 0.2,
             season == "fall" ~ 0.3
           )
           
           precipitation &lt;- if (runif(1) &lt; precip_prob) {
             rexp(1, 1) * 10  # Generate precipitation amount
           } else {
             0  # No precipitation
           }
           
           # Return simulated weather data
           data.frame(
             city = city,
             date = date,
             temperature = round(temperature, 1),
             precipitation = round(precipitation, 1),
             humidity = sample(30:90, 1),
             stringsAsFactors = FALSE
           )
         }
         
         # Simulate energy consumption based on weather
         simulate_energy_consumption &lt;- function(weather_data) {
           # Base energy usage varies by city (population, infrastructure, etc.)
           base_energy &lt;- list(
             "New York" = 1000,
             "Chicago" = 800,
             "Los Angeles" = 900,
             "Houston" = 850,
             "Phoenix" = 700
           )
           
           # Calculate energy consumption
           # Formula includes:
           # - Base usage for the city
           # - Temperature effect (higher in extreme temperatures due to heating/cooling)
           # - Day of week effect (weekdays have higher consumption)
           # - Random variation
           
           energy_data &lt;- weather_data %&gt;%
             mutate(
               weekday = as.numeric(format(date, "%u")),
               is_weekend = weekday &gt;= 6,
               # Temperature effect: U-shaped curve centered at 20°C (comfortable temperature)
               temp_effect = 0.5 * (temperature - 20)^2,
               # Base energy for the city
               base_usage = base_energy[[city]],
               # Weekend reduction factor
               weekend_factor = if_else(is_weekend, 0.8, 1.0),
               # Calculate energy usage
               energy_consumption = base_usage * weekend_factor * (1 + temp_effect/100) * 
                 (1 + rnorm(1, mean = 0, sd = 0.05))
             )
           
           # Return the calculated energy consumption
           energy_data$energy_consumption &lt;- round(energy_data$energy_consumption, 0)
           return(energy_data)
         }
         
         # 3. Collect data for all cities and dates
         cat("Simulating data collection...\n")
         all_weather_data &lt;- data.frame()
         
         # This would be API calls in a real scenario
         for (city in cities) {
           cat("Collecting data for", city, "\n")
           for (date in date_range) {
             # Get weather data
             weather &lt;- simulate_weather_data(city, date)
             
             # Get energy consumption data
             energy &lt;- simulate_energy_consumption(weather)
             
             # Combine
             all_weather_data &lt;- rbind(all_weather_data, energy)
           }
         }
         
         # 4. Data Analysis
         
         # Summary by city
         city_summary &lt;- aggregate(
           cbind(temperature, precipitation, energy_consumption) ~ city, 
           data = all_weather_data, 
           FUN = function(x) c(mean = mean(x), min = min(x), max = max(x))
         )
         
         cat("\nSummary by City:\n")
         print(city_summary)
         
         # Correlation between temperature and energy consumption
         cat("\nCorrelation between temperature and energy consumption:\n")
         correlation_by_city &lt;- by(all_weather_data, all_weather_data$city, function(city_data) {
           cor(city_data$temperature, city_data$energy_consumption)
         })
         print(correlation_by_city)
         
         # Overall trends over time
         monthly_avg &lt;- aggregate(
           cbind(temperature, energy_consumption) ~ 
             city + format(date, "%Y-%m"), 
           data = all_weather_data, 
           FUN = mean
         )
         names(monthly_avg)[2] &lt;- "month"
         
         cat("\nMonthly averages:\n")
         print(head(monthly_avg))
         
         # 5. Visualization
         
         # Plot temperature vs. energy consumption for all cities
         plot(all_weather_data$temperature, all_weather_data$energy_consumption,
              main = "Temperature vs. Energy Consumption",
              xlab = "Temperature (°C)",
              ylab = "Energy Consumption (units)",
              pch = 19,
              col = factor(all_weather_data$city))
         
         # Add regression line
         abline(lm(energy_consumption ~ temperature, data = all_weather_data), 
                col = "red", lwd = 2)
         
         # Add legend
         legend("topleft", legend = unique(all_weather_data$city), 
                col = 1:length(cities), pch = 19)
         
         # Create a boxplot of energy consumption by city
         boxplot(energy_consumption ~ city, data = all_weather_data,
                main = "Energy Consumption by City",
                xlab = "City",
                ylab = "Energy Consumption (units)",
                col = rainbow(length(cities)))
         
         # Create time series plot for a specific city
         ny_data &lt;- all_weather_data[all_weather_data$city == "New York", ]
         ny_data &lt;- ny_data[order(ny_data$date), ]
         
         # Smooth the data for better visualization
         smooth_temp &lt;- zoo::rollmean(ny_data$temperature, 7, fill = NA)
         smooth_energy &lt;- zoo::rollmean(ny_data$energy_consumption, 7, fill = NA)
         
         # Create a time series plot with two y-axes
         par(mar = c(5, 4, 4, 4) + 0.1)
         plot(ny_data$date, ny_data$temperature,
              type = "l", col = "blue", lwd = 2,
              xlab = "Date", ylab = "Temperature (°C)",
              main = "Temperature and Energy Consumption in New York")
         
         # Add smoothed temperature line
         lines(ny_data$date, smooth_temp, col = "darkblue", lwd = 3)
         
         # Add right y-axis for energy consumption
         par(new = TRUE)
         plot(ny_data$date, ny_data$energy_consumption,
              type = "l", col = "red", lwd = 2,
              xlab = "", ylab = "",
              yaxt = "n")
         
         # Add smoothed energy consumption line
         lines(ny_data$date, smooth_energy, col = "darkred", lwd = 3)
         
         # Add right y-axis
         axis(4)
         mtext("Energy Consumption (units)", side = 4, line = 3)
         
         # Add legend
         legend("topleft", 
                legend = c("Temperature", "Temperature (7-day avg)", 
                          "Energy", "Energy (7-day avg)"),
                col = c("blue", "darkblue", "red", "darkred"),
                lwd = c(2, 3, 2, 3))
       </code>
     </program>
     
     <exercise xml:id="ex-project-planning">
       <title>Project Planning</title>
       <statement>
         <p>For the "Tracking stock prices and financial news sentiment" project scenario, answer the following questions:</p>
       </statement>
       <exercise>
         <statement>
           <p>What specific APIs or web data sources would you need to access, and what type of data would each provide?</p>
         </statement>
         <answer>
           <p>For tracking stock prices and financial news sentiment, we would need the following APIs and data sources:</p>
           <ol>
             <li>
               <p><strong>Financial Market Data APIs:</strong></p>
               <ul>
                 <li><p><em>Alpha Vantage</em> or <em>Yahoo Finance API</em> - Provides historical and real-time stock price data, including opening/closing prices, high/low prices, trading volume, and technical indicators</p></li>
                 <li><p><em>FRED API</em> (Federal Reserve Economic Data) - Offers macroeconomic indicators that might influence stock performance, such as interest rates, GDP growth, and inflation metrics</p></li>
                 <li><p><em>IEX Cloud</em> - Provides company fundamentals, financial metrics, and real-time market data</p></li>
               </ul>
             </li>
             <li>
               <p><strong>News and Sentiment APIs:</strong></p>
               <ul>
                 <li><p><em>News API</em> - Delivers financial news articles from multiple sources, including publication dates, headlines, and content</p></li>
                 <li><p><em>AYLIEN News API</em> or <em>Google News API</em> - Provides targeted news coverage about specific companies or sectors</p></li>
                 <li><p><em>Sentiment Analysis APIs</em> (TextBlob, IBM Watson, Microsoft Azure) - Offers pre-built sentiment scoring of text content</p></li>
               </ul>
             </li>
             <li>
               <p><strong>Social Media APIs:</strong></p>
               <ul>
                 <li><p><em>Twitter API</em> - Provides access to tweets mentioning target companies or financial keywords</p></li>
                 <li><p><em>Reddit API</em> - Allows access to financial discussion forums like r/wallstreetbets, r/investing, etc.</p></li>
               </ul>
             </li>
             <li>
               <p><strong>SEC Filing APIs:</strong></p>
               <ul>
                 <li><p><em>SEC EDGAR API</em> - Provides access to company filings and financial disclosures</p></li>
               </ul>
             </li>
           </ol>
           <p>These data sources would collectively provide a comprehensive view of both market performance metrics and the sentiment surrounding target companies, enabling analysis of correlations between news sentiment and stock price movements.</p>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>What data transformations would you need to perform to prepare the collected data for analysis, and what challenges might you encounter?</p>
         </statement>
         <answer>
           <p>Several data transformations would be necessary for this project, along with potential challenges:</p>
           
           <p><strong>Data Transformations:</strong></p>
           <ol>
             <li>
               <p><strong>Time Series Alignment</strong></p>
               <ul>
                 <li><p>Synchronize news timestamps with stock market data (accounting for market hours vs. 24/7 news)</p></li>
                 <li><p>Create time-lagged variables to assess delayed impact of news on stock prices</p></li>
                 <li><p>Resampling data to consistent intervals (hourly, daily, etc.)</p></li>
               </ul>
             </li>
             <li>
               <p><strong>Text Processing and Sentiment Analysis</strong></p>
               <ul>
                 <li><p>Convert raw news text to numerical sentiment scores</p></li>
                 <li><p>Perform named entity recognition to identify mentioned companies</p></li>
                 <li><p>Extract relevant keywords and categorize news by topic</p></li>
                 <li><p>Aggregate multiple news items into composite sentiment indicators</p></li>
               </ul>
             </li>
             <li>
               <p><strong>Financial Data Transformations</strong></p>
               <ul>
                 <li><p>Calculate derived metrics (returns, volatility, moving averages)</p></li>
                 <li><p>Normalize price data across different stocks</p></li>
                 <li><p>Create categorical variables for market conditions (bull/bear)</p></li>
                 <li><p>Adjust for stock splits, dividends, and other corporate actions</p></li>
               </ul>
             </li>
             <li>
               <p><strong>Feature Engineering</strong></p>
               <ul>
                 <li><p>Create composite indicators combining sentiment and price data</p></li>
                 <li><p>Develop sector-specific variables to account for industry trends</p></li>
                 <li><p>Engineer seasonality features (day-of-week effects, month effects)</p></li>
               </ul>
             </li>
           </ol>
           
           <p><strong>Potential Challenges:</strong></p>
           <ol>
             <li>
               <p><strong>Temporal Relationship Complexity</strong></p>
               <ul>
                 <li><p>Determining the optimal time window for news impact on stock prices</p></li>
                 <li><p>Distinguishing causation from correlation in news-price relationships</p></li>
                 <li><p>Handling news that breaks outside market hours</p></li>
               </ul>
             </li>
             <li>
               <p><strong>Data Quality Issues</strong></p>
               <ul>
                 <li><p>Dealing with inaccurate or misleading news reports</p></li>
                 <li><p>Managing duplicate news items across different sources</p></li>
                 <li><p>Handling missing data during market closures or API outages</p></li>
               </ul>
             </li>
             <li>
               <p><strong>Sentiment Analysis Limitations</strong></p>
               <ul>
                 <li><p>Financial text often contains nuanced language that basic sentiment models struggle with</p></li>
                 <li><p>Sarcasm, figures of speech, and industry jargon may confuse automated sentiment analysis</p></li>
                 <li><p>News headlines may have different sentiment than full articles</p></li>
               </ul>
             </li>
             <li>
               <p><strong>Scale and Performance Issues</strong></p>
               <ul>
                 <li><p>Processing large volumes of news text efficiently</p></li>
                 <li><p>Managing high-frequency financial data</p></li>
                 <li><p>Optimizing storage and retrieval for time-series analysis</p></li>
               </ul>
             </li>
           </ol>
           
           <p>Addressing these challenges would require an iterative approach to data transformation, careful validation of sentiment analysis results, and potentially the development of finance-specific text analysis models.</p>
         </answer>
       </exercise>
     </exercise>
   </subsection>
 </section>

 <!-- TOPIC 9: MID-TERM PROJECT TOPIC -->
 <section xml:id="sec-mid-term-project">
   <title>Mid-Term Project Topic</title>
   
   <!-- Session 17: Mid-term project work session -->
   <subsection xml:id="subsec-project-work-session">
     <title>Mid-term project work session</title>
     
     <p>This session is dedicated to providing in-class time for you to work on your mid-term projects. You'll have the opportunity to apply the data science concepts and techniques you've learned so far to a practical project.</p>
     
     <activity xml:id="activity-mid-term-project">
       <title>Mid-Term Project: Data Analysis and Visualization</title>
       <statement>
         <p>For your mid-term project, you will work in pairs to analyze a dataset of your choice. The project will involve data import, exploration, transformation, and visualization to extract meaningful insights.</p>
         
         <p>Project requirements:</p>
         <ol>
           <li>
             <p>Select and import a dataset of interest</p>
             <ul>
               <li>
                 <p>You may use a built-in R dataset, a publicly available dataset, or data from an API</p>
               </li>
               <li>
                 <p>The dataset should be of reasonable size and complexity to allow for meaningful analysis</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Perform data preparation and transformation</p>
             <ul>
               <li>
                 <p>Clean and handle any missing or problematic data</p>
               </li>
               <li>
                 <p>Create derived variables that enhance your analysis</p>
               </li>
               <li>
                 <p>Reshape or aggregate data as needed</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Conduct exploratory data analysis</p>
             <ul>
               <li>
                 <p>Generate appropriate summary statistics</p>
               </li>
               <li>
                 <p>Create visualizations to understand data distributions and relationships</p>
               </li>
               <li>
                 <p>Identify interesting patterns or trends</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Develop at least four different types of visualizations</p>
             <ul>
               <li>
                 <p>The visualizations should effectively communicate key insights</p>
               </li>
               <li>
                 <p>Each visualization should serve a specific analytical purpose</p>
               </li>
               <li>
                 <p>Include proper titles, labels, and other elements for clarity</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Document your process and findings</p>
             <ul>
               <li>
                 <p>Include comments in your code explaining key steps</p>
               </li>
               <li>
                 <p>Provide written explanations of your approach and discoveries</p>
               </li>
               <li>
                 <p>Address any challenges you encountered and how you resolved them</p>
               </li>
             </ul>
           </li>
         </ol>
         
         <p>Deliverables:</p>
         <ul>
           <li>
             <p>R script or R Markdown document containing your code and documentation</p>
           </li>
           <li>
             <p>Brief presentation (5-7 minutes) showcasing your key findings and visualizations</p>
           </li>
           <li>
             <p>Short written report summarizing your project (2-3 pages)</p>
           </li>
         </ul>
       </statement>
     </activity>
     
     <p>During this work session, you should:</p>
     <ul>
       <li>
         <p>Finalize your dataset selection</p>
       </li>
       <li>
         <p>Begin data import and initial exploration</p>
       </li>
       <li>
         <p>Discuss with your partner the key questions you want to answer</p>
       </li>
       <li>
         <p>Create a project plan outlining the tasks and timeline</p>
       </li>
       <li>
         <p>Consult with the instructor about any questions or challenges</p>
       </li>
     </ul>
     
     <program interactive="webr" xml:id="prog-project-template">
       <code>
         # Mid-Term Project Template
         # Names: [Your Names Here]
         # Date: [Date]
         
         # 1. Data Import and Initial Exploration
         # ======================================
         
         # Load necessary packages
         # install.packages("package_name")  # If needed
         # library(package_name)
         
         # Import the dataset
         # data &lt;- read.csv("your_file.csv")  # For CSV files
         # OR use a built-in dataset
         data(mtcars)  # Example using built-in dataset
         
         # Explore the structure of the data
         str(mtcars)
         
         # View the first few rows
         head(mtcars)
         
         # Summary statistics
         summary(mtcars)
         
         # Check for missing values
         any(is.na(mtcars))
         
         # 2. Data Cleaning and Transformation
         # ==================================
         
         # Handle missing values (if any)
         # Example: Replace missing values with the mean
         # data$column[is.na(data$column)] &lt;- mean(data$column, na.rm = TRUE)
         
         # Create derived variables
         mtcars$efficiency &lt;- mtcars$mpg / mtcars$wt
         
         # Create categorical variables
         mtcars$weight_category &lt;- cut(mtcars$wt, 
                                    breaks = c(0, 2.5, 3.5, Inf), 
                                    labels = c("Light", "Medium", "Heavy"))
         
         # 3. Exploratory Data Analysis
         # ===========================
         
         # Distribution of a numerical variable
         hist(mtcars$mpg, 
              main = "Distribution of MPG",
              xlab = "Miles Per Gallon",
              col = "lightblue")
         
         # Relationship between two numerical variables
         plot(mtcars$wt, mtcars$mpg,
              main = "Weight vs. MPG",
              xlab = "Weight (1000 lbs)",
              ylab = "Miles Per Gallon",
              pch = 19,
              col = "darkblue")
         
         # Add a regression line
         abline(lm(mpg ~ wt, data = mtcars), col = "red", lwd = 2)
         
         # Boxplot of a numerical variable by a categorical variable
         boxplot(mpg ~ cyl, data = mtcars,
               main = "MPG by Number of Cylinders",
               xlab = "Number of Cylinders",
               ylab = "Miles Per Gallon",
               col = c("lightgreen", "lightblue", "lightpink"))
         
         # 4. Advanced Visualizations
         # =========================
         
         # Scatter plot matrix
         pairs(mtcars[, c("mpg", "disp", "hp", "wt")], 
               main = "Scatter Plot Matrix",
               pch = 19,
               col = "darkblue")
         
         # Multiple plots in one figure
         par(mfrow = c(2, 2))
         
         # Plot 1: Histogram
         hist(mtcars$mpg, 
              main = "MPG Distribution",
              xlab = "Miles Per Gallon",
              col = "lightblue")
         
         # Plot 2: Boxplot
         boxplot(mpg ~ cyl, data = mtcars,
               main = "MPG by Cylinders",
               xlab = "Cylinders",
               ylab = "MPG",
               col = c("lightgreen", "lightblue", "lightpink"))
         
         # Plot 3: Scatter plot
         plot(mtcars$wt, mtcars$mpg,
              main = "Weight vs. MPG",
              xlab = "Weight",
              ylab = "MPG",
              pch = 19,
              col = "darkblue")
         
         # Plot 4: Bar plot
         barplot(table(mtcars$cyl),
               main = "Cylinder Count",
               xlab = "Number of Cylinders",
               ylab = "Count",
               col = c("red", "green", "blue"))
         
         # Reset plotting window
         par(mfrow = c(1, 1))
         
         # 5. Key Findings and Insights
         # ===========================
         
         # Calculate correlation between variables
         cor_matrix &lt;- cor(mtcars[, c("mpg", "disp", "hp", "wt", "qsec")])
         round(cor_matrix, 2)
         
         # Linear regression model
         model &lt;- lm(mpg ~ wt + hp, data = mtcars)
         summary(model)
         
         # Group comparisons
         aggregate(mpg ~ cyl, data = mtcars, FUN = mean)
         
         # Efficiency analysis
         aggregate(efficiency ~ weight_category, data = mtcars, FUN = mean)
         
         # 6. Documentation of Findings
         # ===========================
         
         # Key Finding 1: [Description of finding]
         
         # Key Finding 2: [Description of finding]
         
         # Key Finding 3: [Description of finding]
         
         # Challenges and Solutions:
         # [Describe any challenges you encountered and how you resolved them]
         
         # Conclusions:
         # [Summarize your main conclusions from the analysis]
       </code>
     </program>
     
     <exercise xml:id="ex-project-planning-template">
       <title>Project Planning</title>
       <statement>
         <p>Complete the following project planning template for your mid-term project:</p>
         
         <p><strong>Title:</strong> [Your Project Title]</p>
         
         <p><strong>Team Members:</strong> [Names]</p>
         
         <p><strong>Dataset:</strong> [Name and brief description of your chosen dataset]</p>
         
         <p><strong>Research Questions:</strong> [List 2-3 specific questions you aim to answer with your analysis]</p>
         
         <p><strong>Project Tasks and Timeline:</strong></p>
         <ul>
           <li><p>Data acquisition and exploration: [Date]</p></li>
           <li><p>Data cleaning and transformation: [Date]</p></li>
           <li><p>Exploratory analysis and visualization: [Date]</p></li>
           <li><p>Advanced analysis and key findings: [Date]</p></li>
           <li><p>Documentation and presentation preparation: [Date]</p></li>
         </ul>
         
         <p><strong>Planned Visualizations:</strong> [Describe at least 4 types of visualizations you plan to create]</p>
         
         <p><strong>Expected Challenges:</strong> [Identify potential challenges and how you plan to address them]</p>
         
         <p><strong>Division of Responsibilities:</strong> [How will you divide the work between team members?]</p>
       </statement>
       <response/>
     </exercise>
   </subsection>
   
   <!-- Instructor and peer consultations -->
   <subsection xml:id="subsec-consultations">
     <title>Instructor and peer consultations</title>
     
     <p>During this session, you'll have the opportunity to consult with the instructor and your peers about your mid-term project. This collaborative environment will help you refine your approach and address any questions or challenges.</p>
     
     <p>Consultation guidelines:</p>
     <ul>
       <li>
         <p>Be prepared to briefly describe your project and the specific aspects you need help with</p>
       </li>
       <li>
         <p>Listen actively to suggestions and be open to constructive feedback</p>
       </li>
       <li>
         <p>Take notes on the advice you receive</p>
       </li>
       <li>
         <p>When providing feedback to peers, be specific, constructive, and respectful</p>
       </li>
     </ul>
     
     <activity xml:id="activity-peer-consultation">
       <title>Peer Consultation Exercise</title>
       <statement>
         <p>Form groups of four (two project pairs) and conduct a structured peer consultation session following these steps:</p>
         
         <ol>
           <li>
             <p>Project Overview (2 minutes per pair)</p>
             <ul>
               <li>
                 <p>Briefly present your project topic, dataset, and research questions</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Progress Update (2 minutes per pair)</p>
             <ul>
               <li>
                 <p>Share what you've accomplished so far and your next steps</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Challenges Discussion (3 minutes per pair)</p>
             <ul>
               <li>
                 <p>Describe specific challenges or questions you're facing</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Feedback Exchange (5 minutes per pair)</p>
             <ul>
               <li>
                 <p>The other pair provides suggestions, ideas, and constructive feedback</p>
               </li>
             </ul>
           </li>
           <li>
             <p>Action Planning (3 minutes per pair)</p>
             <ul>
               <li>
                 <p>Based on the feedback received, update your project plan</p>
               </li>
             </ul>
           </li>
         </ol>
       </statement>
     </activity>
     
     <exercise xml:id="ex-consultation-reflection">
       <title>Consultation Reflection</title>
       <statement>
         <p>After completing the peer consultation exercise, reflect on the following questions:</p>
         <ol>
           <li>
             <p>What were the most valuable insights or suggestions you received during the consultation?</p>
           </li>
           <li>
             <p>How will you incorporate this feedback into your project?</p>
           </li>
           <li>
             <p>What specific changes will you make to your project plan based on the consultation?</p>
           </li>
           <li>
             <p>What did you learn from reviewing your peers' projects that could be helpful for your own work?</p>
           </li>
         </ol>
       </statement>
       <response/>
     </exercise>
   </subsection>
   
   <!-- Progress check-ins -->
   <subsection xml:id="subsec-progress-checkins">
     <title>Progress check-ins</title>
     
     <p>Regular progress check-ins are essential for keeping your project on track. This section provides a structured format for documenting your progress and planning your next steps.</p>
     
     <activity xml:id="activity-progress-checkin">
       <title>Mid-Term Project Progress Check-In</title>
       <statement>
         <p>Complete the following progress check-in form to document your project status and plan your next steps:</p>
         
         <p><strong>Project Title:</strong> [Your Project Title]</p>
         
         <p><strong>Team Members:</strong> [Names]</p>
         
         <p><strong>Date:</strong> [Current Date]</p>
         
         <p><strong>1. Accomplished Since Last Check-In:</strong></p>
         <ul>
           <li>
             <p>[Task 1 completed]</p>
           </li>
           <li>
             <p>[Task 2 completed]</p>
           </li>
           <li>
             <p>[Task 3 completed]</p>
           </li>
         </ul>
         
         <p><strong>2. Current Status:</strong></p>
         <ul>
           <li>
             <p>Data collection: [Complete/In progress/Not started]</p>
           </li>
           <li>
             <p>Data cleaning: [Complete/In progress/Not started]</p>
           </li>
           <li>
             <p>Exploratory analysis: [Complete/In progress/Not started]</p>
           </li>
           <li>
             <p>Visualization development: [Complete/In progress/Not started]</p>
           </li>
           <li>
             <p>Documentation: [Complete/In progress/Not started]</p>
           </li>
         </ul>
         
         <p><strong>3. Challenges and Solutions:</strong></p>
         <ul>
           <li>
             <p>Challenge 1: [Description]</p>
             <p>Solution: [How you addressed it or plan to address it]</p>
           </li>
           <li>
             <p>Challenge 2: [Description]</p>
             <p>Solution: [How you addressed it or plan to address it]</p>
           </li>
         </ul>
         
         <p><strong>4. Next Steps:</strong></p>
         <ul>
           <li>
             <p>[Task 1 to complete] - Responsible: [Name] - Deadline: [Date]</p>
           </li>
           <li>
             <p>[Task 2 to complete] - Responsible: [Name] - Deadline: [Date]</p>
           </li>
           <li>
             <p>[Task 3 to complete] - Responsible: [Name] - Deadline: [Date]</p>
           </li>
         </ul>
         
         <p><strong>5. Questions for Instructor or Peers:</strong></p>
         <ul>
           <li>
             <p>[Question 1]</p>
           </li>
           <li>
             <p>[Question 2]</p>
           </li>
         </ul>
       </statement>
     </activity>
     
     <exercise xml:id="ex-project-milestone-checklist">
       <title>Project Milestone Checklist</title>
       <statement>
         <p>Use the following checklist to track your progress on key project milestones:</p>
       </statement>
       <choices>
         <choice>
           <statement>
             <p>We have selected an appropriate dataset for our analysis</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have formulated clear research questions</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have successfully imported and examined the raw data</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have cleaned the data and handled any missing or problematic values</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have created appropriate derived variables to enhance our analysis</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have conducted initial exploratory data analysis</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have created at least two different types of visualizations</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have identified preliminary insights or patterns in the data</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have documented our code with appropriate comments</p>
           </statement>
         </choice>
         <choice>
           <statement>
             <p>We have started drafting our project report</p>
           </statement>
         </choice>
       </choices>
     </exercise>
   </subsection>
 </section>

 <!-- TOPIC 9: MID-TERM PROJECT TOPIC (CONTINUED) -->
 <section xml:id="sec-mid-term-presentations">
   <title>Mid-term project presentations</title>
   
   <!-- Session 18: Mid-term project presentations -->
   <subsection xml:id="subsec-presentation-guidelines">
     <title>Presentation guidelines</title>
     
     <p>Your mid-term project presentation should effectively communicate your analysis and findings to your classmates and instructor. Follow these guidelines to create a clear and engaging presentation.</p>
     
     <p>Presentation structure (5-7 minutes total):</p>
     <ol>
       <li>
         <p><term>Introduction (30-45 seconds)</term></p>
         <ul>
           <li>
             <p>Project title and team members</p>
           </li>
           <li>
             <p>Brief dataset description</p>
           </li>
           <li>
             <p>Research questions or objectives</p>
           </li>
         </ul>
       </li>
       <li>
         <p><term>Methods (1-1.5 minutes)</term></p>
         <ul>
           <li>
             <p>Data acquisition and preparation approach</p>
           </li>
           <li>
             <p>Key transformations or derived variables</p>
           </li>
           <li>
             <p>Analysis techniques used</p>
           </li>
         </ul>
       </li>
       <li>
         <p><term>Results (2-3 minutes)</term></p>
         <ul>
           <li>
             <p>Present your most important visualizations</p>
           </li>
           <li>
             <p>Highlight key patterns or relationships</p>
           </li>
           <li>
             <p>Share surprising or interesting findings</p>
           </li>
         </ul>
       </li>
       <li>
         <p><term>Conclusions (1 minute)</term></p>
         <ul>
           <li>
             <p>Main takeaways from your analysis</p>
           </li>
           <li>
             <p>Answers to your research questions</p>
           </li>
           <li>
             <p>Limitations of your analysis</p>
           </li>
         </ul>
       </li>
       <li>
         <p><term>Future work (30 seconds)</term></p>
         <ul>
           <li>
             <p>How your analysis could be extended or improved</p>
           </li>
           <li>
             <p>Additional questions that arose during your analysis</p>
           </li>
         </ul>
       </li>
     </ol>
     
     <p>Presentation tips:</p>
     <ul>
       <li>
         <p>Focus on the most important aspects of your project - you can't cover everything in 5-7 minutes</p>
       </li>
       <li>
         <p>Use clear, well-labeled visualizations that are easy to understand</p>
       </li>
       <li>
         <p>Minimize text on slides - use visuals to tell your story</p>
       </li>
       <li>
         <p>Practice your timing to ensure you don't run over the allotted time</p>
       </li>
       <li>
         <p>Be prepared to answer questions about your methods and findings</p>
       </li>
       <li>
         <p>Both team members should participate in the presentation</p>
       </li>
     </ul>
     
     <exercise xml:id="ex-presentation-outline">
       <title>Presentation Outline</title>
       <statement>
         <p>Create an outline for your mid-term project presentation using the following template:</p>
         
         <p><strong>Title Slide:</strong> [Project Title], [Team Members]</p>
         
         <p><strong>Introduction:</strong></p>
         <ul>
           <li>
             <p>Dataset: [Brief description of your dataset]</p>
           </li>
           <li>
             <p>Research questions: [List your main research questions]</p>
           </li>
         </ul>
         
         <p><strong>Methods:</strong></p>
         <ul>
           <li>
             <p>Data acquisition: [How you obtained the data]</p>
           </li>
           <li>
             <p>Data cleaning: [Key cleaning steps]</p>
           </li>
           <li>
             <p>Analysis approach: [Main analytical techniques]</p>
           </li>
         </ul>
         
         <p><strong>Results (for each visualization):</strong></p>
         <ul>
           <li>
             <p>Visualization 1: [Description and key finding]</p>
           </li>
           <li>
             <p>Visualization 2: [Description and key finding]</p>
           </li>
           <li>
             <p>Visualization 3: [Description and key finding]</p>
           </li>
           <li>
             <p>Visualization 4: [Description and key finding]</p>
           </li>
         </ul>
         
         <p><strong>Conclusions:</strong></p>
         <ul>
           <li>
             <p>Key finding 1: [Brief description]</p>
           </li>
           <li>
             <p>Key finding 2: [Brief description]</p>
           </li>
           <li>
             <p>Limitations: [Brief description]</p>
           </li>
         </ul>
         
         <p><strong>Future Work:</strong></p>
         <ul>
           <li>
             <p>Potential extension: [Brief description]</p>
           </li>
           <li>
             <p>New questions: [Brief description]</p>
           </li>
         </ul>
       </statement>
       <response/>
     </exercise>
   </subsection>
   
   <!-- Groups present their data transformation and analysis -->
   <subsection xml:id="subsec-presentation-session">
     <title>Groups present their data transformation and analysis</title>
     
     <p>During this session, each project group will present their work to the class. After each presentation, there will be a brief period for questions and feedback.</p>
     
     <p>Presentation schedule:</p>
     <ul>
       <li>
         <p>Each group will have 5-7 minutes to present</p>
       </li>
       <li>
         <p>Followed by 2-3 minutes for questions and feedback</p>
       </li>
       <li>
         <p>Short transition time between presentations</p>
       </li>
     </ul>
     
     <activity xml:id="activity-peer-feedback">
       <title>Peer Presentation Feedback</title>
       <statement>
         <p>For each presentation, complete the following peer feedback form:</p>
         
         <p><strong>Presenters:</strong> [Names]</p>
         
         <p><strong>Project Title:</strong> [Title]</p>
         
         <p><strong>What aspects of the project were most effective or interesting?</strong></p>
         <p>[Your response]</p>
         
         <p><strong>What visualizations were particularly insightful?</strong></p>
         <p>[Your response]</p>
         
         <p><strong>What questions do you have about their analysis or findings?</strong></p>
         <p>[Your response]</p>
         
         <p><strong>What suggestions do you have for improving or extending their analysis?</strong></p>
         <p>[Your response]</p>
         
         <p><strong>One thing you learned or found valuable from this presentation:</strong></p>
         <p>[Your response]</p>
       </statement>
     </activity>
     
     <exercise xml:id="ex-presentation-reflection">
       <title>Presentation Reflection</title>
       <statement>
         <p>After completing your presentation, reflect on the following questions:</p>
         <ol>
           <li>
             <p>What aspects of your presentation do you think were most effective?</p>
           </li>
           <li>
             <p>What would you do differently if you were to give this presentation again?</p>
           </li>
           <li>
             <p>What was the most valuable feedback you received from your peers or instructor?</p>
           </li>
           <li>
             <p>How will you incorporate this feedback into your future data science projects or presentations?</p>
           </li>
         </ol>
       </statement>
       <response/>
     </exercise>
   </subsection>
   
   <!-- Peer feedback and discussion -->
   <subsection xml:id="subsec-feedback-discussion">
     <title>Peer feedback and discussion</title>
     
     <p>Peer feedback and discussion are valuable components of the learning process. In this section, we'll discuss how to provide constructive feedback to your peers and reflect on the common themes and insights from all the presentations.</p>
     
     <p>Guidelines for providing effective feedback:</p>
     <ul>
       <li>
         <p>Be specific and descriptive rather than general and evaluative</p>
       </li>
       <li>
         <p>Focus on the work, not the person</p>
       </li>
       <li>
         <p>Balance positive observations with suggestions for improvement</p>
       </li>
       <li>
         <p>Offer actionable suggestions rather than vague critiques</p>
       </li>
       <li>
         <p>Ask questions to clarify your understanding</p>
       </li>
       <li>
         <p>Be respectful and constructive in your tone</p>
       </li>
     </ul>
     
     <activity xml:id="activity-class-discussion">
       <title>Class Discussion: Project Insights</title>
       <statement>
         <p>After all presentations have been completed, participate in a class discussion addressing the following questions:</p>
         
         <ol>
           <li>
             <p>What common challenges did most groups face in their data analysis?</p>
           </li>
           <li>
             <p>What visualization techniques were particularly effective across projects?</p>
           </li>
           <li>
             <p>What surprising or unexpected findings emerged from the various analyses?</p>
           </li>
           <li>
             <p>What data transformation techniques proved most useful across different projects?</p>
           </li>
           <li>
             <p>How might the insights from these projects be applied in real-world scenarios?</p>
           </li>
           <li>
             <p>What additional skills or techniques would be beneficial to learn for future data science projects?</p>
           </li>
         </ol>
       </statement>
     </activity>
     
     <exercise xml:id="ex-learning-reflection">
       <title>Learning Reflection</title>
       <statement>
         <p>Reflect on what you've learned from both your own project and your peers' presentations:</p>
         <ol>
           <li>
             <p>What are the three most important data science skills or concepts you've developed through this project?</p>
           </li>
           <li>
             <p>What techniques or approaches did you observe in other presentations that you'd like to incorporate into your future work?</p>
           </li>
           <li>
             <p>How has your understanding of the data analysis process evolved through this project?</p>
           </li>
           <li>
             <p>What aspects of data science do you find most challenging, and how do you plan to improve in these areas?</p>
           </li>
           <li>
             <p>How might you apply what you've learned in this project to future academic or professional endeavors?</p>
           </li>
         </ol>
       </statement>
       <response/>
     </exercise>
   </subsection>
   
   <!-- Reflection on computational techniques learned -->
   <subsection xml:id="subsec-computational-reflection">
     <title>Reflection on computational techniques learned</title>
     
     <p>As we conclude the mid-term project and the first half of the course, it's valuable to reflect on the computational techniques and skills you've developed. This reflection will help consolidate your learning and identify areas for further growth.</p>
     
     <p>Key computational techniques covered in Units 1 and 2:</p>
     <ul>
       <li>
         <p><term>Basic R syntax and operations</term>: Variables, data types, functions, and control structures</p>
       </li>
       <li>
         <p><term>Data structures</term>: Vectors, lists, data frames, and matrices</p>
       </li>
       <li>
         <p><term>Data import</term>: Reading data from files and web sources</p>
       </li>
       <li>
         <p><term>Data exploration</term>: Summary statistics, initial visualizations, and data inspection</p>
       </li>
       <li>
         <p><term>Data transformation</term>: Filtering, selecting, creating new variables, and reshaping data</p>
       </li>
       <li>
         <p><term>Data visualization</term>: Creating and customizing various plot types</p>
       </li>
       <li>
         <p><term>Working with web data</term>: Understanding APIs and accessing web resources</p>
       </li>
     </ul>
     
     <activity xml:id="activity-technique-inventory">
       <title>Computational Technique Inventory</title>
       <statement>
         <p>Create a personal inventory of the computational techniques you've learned and applied in this course so far. For each technique, rate your confidence level on a scale of 1-5 (1 = Novice, 5 = Proficient) and note how you've applied it in your project work.</p>
         
         <table>
           <tabular halign="center">
             <row header="yes">
               <cell>Technique</cell>
               <cell>Confidence (1-5)</cell>
               <cell>How I've Applied It</cell>
               <cell>Areas for Improvement</cell>
             </row>
             <row>
               <cell>Basic R syntax</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
             <row>
               <cell>Data import</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
             <row>
               <cell>Data cleaning</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
             <row>
               <cell>Data exploration</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
             <row>
               <cell>Data visualization</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
             <row>
               <cell>Data transformation</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
             <row>
               <cell>Working with web data</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
             <row>
               <cell>Documentation and commenting</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
             <row>
               <cell>Problem-solving approaches</cell>
               <cell>[Rating]</cell>
               <cell>[Application]</cell>
               <cell>[Improvement]</cell>
             </row>
           </tabular>
         </table>
       </statement>
     </activity>
     
     <exercise xml:id="ex-course-midpoint-reflection">
       <title>Course Midpoint Reflection</title>
       <statement>
         <p>As we reach the midpoint of the course, reflect on your overall learning experience:</p>
         <ol>
           <li>
             <p>How has your perception of data science changed since the beginning of the course?</p>
           </li>
           <li>
             <p>What has been the most valuable skill or concept you've learned so far?</p>
           </li>
           <li>
             <p>What aspects of data science are you most excited to explore in the second half of the course?</p>
           </li>
           <li>
             <p>What specific goals do you have for your data science learning in the remaining units?</p>
           </li>
           <li>
             <p>How do you plan to continue developing your computational skills beyond this course?</p>
           </li>
         </ol>
       </statement>
       <response/>
     </exercise>
   </subsection>
 </section>
</chapter>
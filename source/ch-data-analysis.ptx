<?xml version="1.0" encoding="UTF-8"?>
<chapter xml:id="ch-data-analysis">
  <title>Data Analysis and Visualization with Web-R</title>

  <introduction>
    <p>This unit introduces techniques for analyzing and visualizing data using Web-R. You will learn how to create effective visualizations, apply statistical methods, and communicate insights through visual representation of data.</p>
  </introduction>

  <!-- TOPIC 10: DATA VISUALIZATION PRINCIPLES -->
  <section xml:id="sec-visualization-principles">
    <title>Data Visualization Principles</title>
    
    <!-- Session 19: Principles of effective visualization -->
    <subsection xml:id="subsec-effective-visualization">
      <title>Principles of effective visualization</title>
      
      <p>Data visualization is the graphical representation of information and data. Effective visualizations help to communicate complex data relationships, identify patterns, and support decision-making.</p>
      
      <p>Key principles of effective data visualization include:</p>
      <ul>
        <li>
          <p><term>Show the data</term>: Ensure the visualization accurately represents the underlying data without distortion</p>
        </li>
        <li>
          <p><term>Minimize chart junk</term>: Remove excessive decorative elements that don't convey information</p>
        </li>
        <li>
          <p><term>Use appropriate visualization types</term>: Choose chart types that best represent the relationships in your data</p>
        </li>
        <li>
          <p><term>Emphasize clarity</term>: Make your visualizations easy to understand and interpret</p>
        </li>
        <li>
          <p><term>Label appropriately</term>: Include clear titles, axis labels, and legends</p>
        </li>
        <li>
          <p><term>Consider color effectively</term>: Use color purposefully and consider accessibility</p>
        </li>
      </ul>
      
      <p>As Edward Tufte, a pioneer in data visualization, states: "Above all else, show the data."</p>
      
      <example>
        <statement>
          <p>Consider a dataset showing monthly sales figures for a company over two years. An effective visualization might be a line chart with:</p>
          <ul>
            <li><p>X-axis showing months</p></li>
            <li><p>Y-axis showing sales values</p></li>
            <li><p>Clear title indicating what is being shown</p></li>
            <li><p>Different colors for each year to allow comparison</p></li>
            <li><p>Minimal gridlines to aid reading values</p></li>
            <li><p>Direct labeling of notable peaks or trends</p></li>
          </ul>
        </statement>
      </example>
    </subsection>
    
    <!-- Visual perception principles -->
    <subsection xml:id="subsec-visual-perception">
      <title>Visual perception principles</title>
      
      <p>Understanding how humans perceive visual information is fundamental to creating effective visualizations. Several key principles guide visual perception:</p>
      
      <p><term>Pre-attentive processing</term>: Certain visual properties are processed automatically and quickly by our visual system, including:</p>
      <ul>
        <li><p>Position</p></li>
        <li><p>Length</p></li>
        <li><p>Color (hue)</p></li>
        <li><p>Orientation</p></li>
        <li><p>Size</p></li>
        <li><p>Shape</p></li>
      </ul>
      
      <p><term>Visual hierarchy</term>: How we organize and prioritize visual elements:</p>
      <ul>
        <li><p>Position: Elements at the top or left are typically seen first</p></li>
        <li><p>Size: Larger elements attract more attention</p></li>
        <li><p>Color: Brighter or more saturated colors stand out</p></li>
        <li><p>Contrast: Elements that differ from their surroundings are noticed more</p></li>
      </ul>
      
      <p><term>Gestalt principles</term>: How we perceive visual elements as organized patterns:</p>
      <ul>
        <li><p>Proximity: Objects near each other appear grouped</p></li>
        <li><p>Similarity: Similar objects appear grouped</p></li>
        <li><p>Continuity: We perceive continuous forms rather than disjointed segments</p></li>
        <li><p>Closure: We perceive complete shapes even when parts are missing</p></li>
        <li><p>Figure-ground: We distinguish objects (figures) from backgrounds</p></li>
      </ul>
      
      <p>These principles help us design visualizations that are quickly and accurately perceived by viewers.</p>
      
      <exercise xml:id="ex-visual-processing">
        <title>Visual Perception Exercise</title>
        <statement>
          <p>For each of the following data relationships, identify which visual encoding (position, length, angle, area, color, etc.) would be most effective and explain why:</p>
        </statement>
        <exercise>
          <statement>
            <p>Comparing the exact values of sales across five product categories</p>
          </statement>
          <answer>
            <p>Position or length (as in a bar chart) would be most effective because these encodings allow for the most accurate comparisons of quantitative values. Humans can easily and accurately compare positions along a common scale or the lengths of bars, making it ideal for comparing exact values across categories.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Showing the geographic distribution of customers across different regions</p>
          </statement>
          <answer>
            <p>Position on a map (spatial encoding) combined with color intensity would be most effective. Position naturally corresponds to geographic location, while color intensity can represent the density or number of customers in each region, creating an intuitive visualization of geographic distribution.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Displaying the proportion of a budget allocated to different departments</p>
          </statement>
          <answer>
            <p>Area (as in a pie chart or treemap) can be effective for showing proportions of a whole. However, for precise comparison of proportions, length encoding (as in a stacked or side-by-side bar chart) would be more accurate, as humans perceive differences in areas less accurately than differences in length or position.</p>
          </answer>
        </exercise>
      </exercise>
    </subsection>
    
    <!-- Choosing appropriate chart types -->
    <subsection xml:id="subsec-choosing-charts">
      <title>Choosing appropriate chart types</title>
      
      <p>Selecting the right type of visualization for your data is crucial for effective communication. Different chart types are suited to different data relationships and analysis goals.</p>
      
      <table xml:id="table-chart-types">
        <title>Common Chart Types and Their Uses</title>
        <tabular halign="center">
          <row header="yes">
            <cell>Chart Type</cell>
            <cell>Best Used For</cell>
            <cell>Example Scenario</cell>
          </row>
          <row>
            <cell>Bar Chart</cell>
            <cell>Comparing values across categories</cell>
            <cell>Sales by product category</cell>
          </row>
          <row>
            <cell>Line Chart</cell>
            <cell>Showing trends over time</cell>
            <cell>Stock price changes over months</cell>
          </row>
          <row>
            <cell>Scatter Plot</cell>
            <cell>Showing relationship between two variables</cell>
            <cell>Height vs. weight correlation</cell>
          </row>
          <row>
            <cell>Pie Chart</cell>
            <cell>Showing composition of a whole (use sparingly)</cell>
            <cell>Budget allocation percentages</cell>
          </row>
          <row>
            <cell>Histogram</cell>
            <cell>Showing distribution of a continuous variable</cell>
            <cell>Age distribution in a population</cell>
          </row>
          <row>
            <cell>Box Plot</cell>
            <cell>Showing statistical distribution and outliers</cell>
            <cell>Test scores across different schools</cell>
          </row>
          <row>
            <cell>Heatmap</cell>
            <cell>Showing patterns in a matrix of values</cell>
            <cell>Correlation matrix of variables</cell>
          </row>
          <row>
            <cell>Map</cell>
            <cell>Showing geographic data</cell>
            <cell>Sales by region or country</cell>
          </row>
        </tabular>
      </table>
      
      <p>Guidelines for selecting chart types:</p>
      <ul>
        <li>
          <p>Consider your audience and their familiarity with different visualization types</p>
        </li>
        <li>
          <p>Think about the key message you want to convey</p>
        </li>
        <li>
          <p>Choose visualizations that highlight the patterns or relationships most relevant to your analysis</p>
        </li>
        <li>
          <p>Consider combining multiple chart types for complex data stories</p>
        </li>
        <li>
          <p>When in doubt, simpler chart types are often better</p>
        </li>
      </ul>
      
      <exercise xml:id="ex-chart-selection">
        <title>Chart Selection Exercise</title>
        <statement>
          <p>Match each data scenario with the most appropriate chart type:</p>
        </statement>
        <matches>
          <match>
            <premise>Monthly revenue over the past five years</premise>
            <response>Line chart</response>
          </match>
          <match>
            <premise>Distribution of employee salaries</premise>
            <response>Histogram</response>
          </match>
          <match>
            <premise>Relationship between advertising spend and sales</premise>
            <response>Scatter plot</response>
          </match>
          <match>
            <premise>Comparison of quarterly sales across product categories</premise>
            <response>Grouped bar chart</response>
          </match>
          <match>
            <premise>Customer satisfaction scores by department</premise>
            <response>Bar chart</response>
          </match>
          <match>
            <premise>Market share of top competitors</premise>
            <response>Pie chart</response>
          </match>
        </matches>
      </exercise>
    </subsection>
    
    <!-- Activity: Critique and improve sample visualizations -->
    <subsection xml:id="subsec-critique-visualizations">
      <title>Activity: Critique and improve sample visualizations</title>
      
      <activity xml:id="activity-visualization-critique">
        <title>Visualization Critique and Improvement</title>
        <statement>
          <p>In this activity, you will analyze and critique sample visualizations, then suggest improvements based on the principles we've discussed.</p>
          
          <p>For each of the following visualization descriptions, identify potential issues and suggest specific improvements:</p>
          
          <p><term>Visualization 1</term>: A 3D pie chart showing the percentage breakdown of a company's expenses across 8 categories.</p>
          
          <p><term>Visualization 2</term>: A line chart showing monthly sales over two years, with no clear title, unlabeled axes, and rainbow-colored lines for different product categories.</p>
          
          <p><term>Visualization 3</term>: A bar chart comparing the GDP of 20 countries, with the y-axis starting at 10 trillion dollars instead of zero.</p>
          
          <p>For each visualization:</p>
          <ol>
            <li>
              <p>Identify at least three specific issues that make the visualization less effective</p>
            </li>
            <li>
              <p>Suggest specific improvements to address each issue</p>
            </li>
            <li>
              <p>Consider whether a different chart type would be more appropriate</p>
            </li>
          </ol>
        </statement>
      </activity>
      
      <exercise xml:id="ex-visualization-principles">
        <title>Visualization Principles Application</title>
        <statement>
          <p>You've been asked to create a visualization showing monthly website traffic over the past year, broken down by traffic source (organic search, paid advertising, social media, direct, and referral).</p>
          
          <p>Describe how you would design this visualization, addressing each of the following aspects:</p>
          <ol>
            <li>
              <p>What chart type would you choose and why?</p>
            </li>
            <li>
              <p>How would you use color effectively?</p>
            </li>
            <li>
              <p>What labels and annotations would you include?</p>
            </li>
            <li>
              <p>How would you handle potential issues like overlapping lines or seasonal patterns?</p>
            </li>
            <li>
              <p>What alternative visualization might you consider if the goal was to emphasize the proportion of traffic from each source rather than the total volume?</p>
            </li>
          </ol>
        </statement>
        <response/>
      </exercise>
    </subsection>
  </section>

  <!-- TOPIC 10: DATA VISUALIZATION PRINCIPLES (CONTINUED) -->
  <section xml:id="sec-intro-ggplot2">
    <title>Introduction to ggplot2</title>
    
    <!-- Session 20: Introduction to ggplot2 -->
    <subsection xml:id="subsec-ggplot2-intro">
      <title>Grammar of graphics concept</title>
      
      <p>The <term>grammar of graphics</term> is a systematic approach to creating visualizations by breaking them down into semantic components. Developed by Leland Wilkinson, this framework provides a structured way to describe and construct visualizations.</p>
      
      <p>In R, the ggplot2 package (created by Hadley Wickham) implements this grammar, allowing for a consistent and flexible approach to creating a wide range of visualizations.</p>
      
      <p>The key components of the grammar of graphics include:</p>
      <ul>
        <li>
          <p><term>Data</term>: The dataset being visualized</p>
        </li>
        <li>
          <p><term>Aesthetics</term>: Mappings from data variables to visual properties (position, color, size, shape, etc.)</p>
        </li>
        <li>
          <p><term>Geometries</term>: The visual elements used to represent data points (points, lines, bars, etc.)</p>
        </li>
        <li>
          <p><term>Facets</term>: Division of the plot into subplots based on categorical variables</p>
        </li>
        <li>
          <p><term>Statistics</term>: Statistical transformations applied to the data (counts, means, regressions, etc.)</p>
        </li>
        <li>
          <p><term>Coordinates</term>: The space in which data is mapped (Cartesian, polar, etc.)</p>
        </li>
        <li>
          <p><term>Themes</term>: Visual styling of non-data elements (background, grid lines, labels, etc.)</p>
        </li>
      </ul>
      
      <p>This layered approach allows for building complex visualizations by combining simple elements in a systematic way.</p>
      
      <program interactive="webr" xml:id="prog-ggplot2-intro">
        <input>
<![CDATA[
# Load required packages (in a real environment)
# install.packages("ggplot2")
# library(ggplot2)

# For our Web-R environment, we'll use a built-in dataset
data(mtcars)

# Let's examine the data structure
head(mtcars)

# Basic ggplot2 syntax demonstration
# Step 1: Initialize a plot with data
# Step 2: Map variables to aesthetics
# Step 3: Add geometric elements

# In a real R environment, you would use:
# ggplot(mtcars, aes(x = wt, y = mpg)) +
#   geom_point()

# In our limited environment, we'll create a similar plot using base R
plot(mtcars$wt, mtcars$mpg,
     main = "Car Weight vs. Fuel Efficiency",
     xlab = "Weight (1000 lbs)",
     ylab = "Miles per Gallon",
     pch = 19,
     col = "blue")

# Add a regression line
abline(lm(mpg ~ wt, data = mtcars), col = "red", lwd = 2)

# Another example: Distribution of MPG
hist(mtcars$mpg,
     main = "Distribution of Fuel Efficiency",
     xlab = "Miles per Gallon",
     col = "lightblue",
     breaks = 10)

# In ggplot2, this would be:
# ggplot(mtcars, aes(x = mpg)) +
#   geom_histogram(bins = 10, fill = "lightblue", color = "black")

# Boxplot by cylinder count
boxplot(mpg ~ factor(cyl), data = mtcars,
        main = "Fuel Efficiency by Number of Cylinders",
        xlab = "Number of Cylinders",
        ylab = "Miles per Gallon",
        col = c("lightgreen", "lightblue", "lightpink"))

# In ggplot2, this would be:
# ggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +
#   geom_boxplot() +
#   scale_fill_manual(values = c("lightgreen", "lightblue", "lightpink")) +
#   labs(title = "Fuel Efficiency by Number of Cylinders",
#        x = "Number of Cylinders",
#        y = "Miles per Gallon")
]]>
        </input>
      </program>
      
      <exercise xml:id="ex-ggplot-components">
        <title>Grammar of Graphics Components</title>
        <statement>
          <p>Match each ggplot2 component with its correct description:</p>
        </statement>
        <matches>
          <match>
            <premise><c>ggplot(data, aes(x = var1, y = var2))</c></premise>
            <response>Initializes a plot and maps variables to aesthetic properties</response>
          </match>
          <match>
            <premise><c>geom_point()</c></premise>
            <response>Adds a scatter plot layer using points to represent observations</response>
          </match>
          <match>
            <premise><c>geom_smooth(method = "lm")</c></premise>
            <response>Adds a statistical model layer showing a linear trend line</response>
          </match>
          <match>
            <premise><c>facet_wrap(~ category)</c></premise>
            <response>Creates separate subplots for each unique value of a categorical variable</response>
          </match>
          <match>
            <premise><c>scale_color_manual(values = c("red", "blue"))</c></premise>
            <response>Customizes the mapping between categorical values and colors</response>
          </match>
          <match>
            <premise><c>theme_minimal()</c></premise>
            <response>Changes the visual style of non-data plot elements to a simplified design</response>
          </match>
        </matches>
      </exercise>
    </subsection>
    
    <!-- Creating basic plots in ggplot2 -->
    <subsection xml:id="subsec-basic-ggplot">
      <title>Creating basic plots in ggplot2</title>
      
      <p>While we can't use ggplot2 directly in our Web-R environment, we'll learn the principles and syntax for creating basic plot types with ggplot2. These principles can be applied when you work in a full R environment.</p>
      
      <p>The basic structure of a ggplot2 visualization follows this pattern:</p>
      <ol>
        <li>
          <p>Start with <c>ggplot(data, aes(...))</c> to specify the data and aesthetic mappings</p>
        </li>
        <li>
          <p>Add layers with <c>geom_*()</c> functions for different plot types</p>
        </li>
        <li>
          <p>Customize with additional elements like labels, scales, and themes</p>
        </li>
      </ol>
      
      <p>Let's examine the code structure for common plot types:</p>
      
      <paragraphs>
        <title>Scatter Plot</title>
        <p>Used to show relationship between two continuous variables</p>
        <pre>
ggplot(data, aes(x = variable1, y = variable2)) +
  geom_point() +
  labs(title = "Title", x = "X Label", y = "Y Label")
        </pre>
      </paragraphs>
      
      <paragraphs>
        <title>Line Chart</title>
        <p>Used to show trends over time or ordered categories</p>
        <pre>
ggplot(data, aes(x = time_variable, y = value_variable)) +
  geom_line() +
  labs(title = "Title", x = "Time", y = "Value")
        </pre>
      </paragraphs>
      
      <paragraphs>
        <title>Bar Chart</title>
        <p>Used to compare values across categories</p>
        <pre>
ggplot(data, aes(x = category_variable, y = value_variable)) +
  geom_bar(stat = "identity") +
  labs(title = "Title", x = "Category", y = "Value")
        </pre>
      </paragraphs>
      
      <paragraphs>
        <title>Histogram</title>
        <p>Used to show distribution of a continuous variable</p>
        <pre>
ggplot(data, aes(x = continuous_variable)) +
  geom_histogram(bins = 30) +
  labs(title = "Title", x = "Variable", y = "Count")
        </pre>
      </paragraphs>
      
      <paragraphs>
        <title>Box Plot</title>
        <p>Used to show distribution and summary statistics by group</p>
        <pre>
ggplot(data, aes(x = category_variable, y = continuous_variable)) +
  geom_boxplot() +
  labs(title = "Title", x = "Category", y = "Value")
        </pre>
      </paragraphs>
      
      <program interactive="webr" xml:id="prog-base-r-plots">
        <input>
<![CDATA[
# Since we can't use ggplot2 directly in Web-R,
# we'll create equivalents in base R

# Load the built-in iris dataset
data(iris)

# Examine the data
head(iris)
summary(iris)

# 1. Scatter Plot
plot(iris$Sepal.Length, iris$Sepal.Width,
     main = "Sepal Length vs. Width",
     xlab = "Sepal Length (cm)",
     ylab = "Sepal Width (cm)",
     pch = 19,
     col = as.numeric(iris$Species))

# Add a legend
legend("topright", 
       legend = levels(iris$Species),
       col = 1:3, 
       pch = 19,
       title = "Species")

# 2. Grouped Box Plot
boxplot(Petal.Length ~ Species, data = iris,
        main = "Petal Length by Species",
        xlab = "Species",
        ylab = "Petal Length (cm)",
        col = c("lightpink", "lightblue", "lightgreen"))

# 3. Histogram
hist(iris$Petal.Width,
     main = "Distribution of Petal Width",
     xlab = "Petal Width (cm)",
     col = "lightblue",
     breaks = 15)

# 4. Multiple histograms (by group)
# Create a layout with 3 rows and 1 column
par(mfrow = c(3, 1))

# Create histograms for each species
hist(iris$Petal.Length[iris$Species == "setosa"],
     main = "Petal Length: Setosa",
     xlab = "Length (cm)",
     col = "lightpink",
     xlim = c(1, 7))

hist(iris$Petal.Length[iris$Species == "versicolor"],
     main = "Petal Length: Versicolor",
     xlab = "Length (cm)",
     col = "lightblue",
     xlim = c(1, 7))

hist(iris$Petal.Length[iris$Species == "virginica"],
     main = "Petal Length: Virginica",
     xlab = "Length (cm)",
     col = "lightgreen",
     xlim = c(1, 7))

# Reset the layout
par(mfrow = c(1, 1))

# 5. Scatterplot Matrix
pairs(iris[, 1:4],
      main = "Scatterplot Matrix of Iris Dataset",
      pch = 19,
      col = as.numeric(iris$Species))
]]>
        </input>
      </program>
      
      <exercise xml:id="ex-plot-selection">
        <title>Plot Selection Exercise</title>
        <statement>
          <p>For each of the following analytical questions, identify the most appropriate plot type and explain why it would be effective:</p>
        </statement>
        <exercise>
          <statement>
            <p>How does fuel efficiency (mpg) relate to car weight (wt) in the mtcars dataset?</p>
          </statement>
          <answer>
            <p>A scatter plot would be most appropriate for this analysis because:</p>
            <ul>
              <li><p>Both fuel efficiency (mpg) and car weight (wt) are continuous variables</p></li>
              <li><p>We're interested in understanding their relationship or correlation</p></li>
              <li><p>A scatter plot allows us to visualize the pattern of association (likely negative)</p></li>
              <li><p>We can add a trend line to quantify the relationship</p></li>
              <li><p>Individual data points represent different car models, which maintains the granularity of the data</p></li>
            </ul>
            <p>This plot would effectively show whether heavier cars tend to have lower fuel efficiency, and how strong that relationship is.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>How does the distribution of petal lengths differ among the three iris species?</p>
          </statement>
          <answer>
            <p>A box plot would be most appropriate for this analysis because:</p>
            <ul>
              <li><p>We're comparing the distribution of a continuous variable (petal length) across categories (species)</p></li>
              <li><p>Box plots display key statistical properties (median, quartiles, range, outliers) in a compact form</p></li>
              <li><p>They allow for easy visual comparison of distributions between groups</p></li>
              <li><p>We can immediately see differences in central tendency, spread, and potential outliers</p></li>
              <li><p>The visual design facilitates direct comparison between the three species</p></li>
            </ul>
            <p>Alternative options could include violin plots (to show the full distribution shape) or grouped histograms, but box plots provide the most efficient summary for this comparison.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>What is the monthly trend in website traffic over the past year?</p>
          </statement>
          <answer>
            <p>A line chart would be most appropriate for this analysis because:</p>
            <ul>
              <li><p>Time series data is best visualized with connected points to show continuity</p></li>
              <li><p>Line charts emphasize trends, patterns, and changes over time</p></li>
              <li><p>The x-axis represents time (months) in a natural sequential order</p></li>
              <li><p>We can easily identify seasonal patterns, overall trends (increasing/decreasing), and anomalies</p></li>
              <li><p>Y-axis would show traffic volume, while x-axis shows the progression of months</p></li>
            </ul>
            <p>This visualization would clearly show whether website traffic has been growing, declining, or following seasonal patterns throughout the year.</p>
          </answer>
        </exercise>
      </exercise>
    </subsection>
    
    <!-- Exercise: Transform basic R plots into ggplot2 -->
    <subsection xml:id="subsec-transform-to-ggplot">
      <title>Exercise: Transform basic R plots into ggplot2</title>
      
      <activity xml:id="activity-transform-plots">
        <title>Transforming Plots to ggplot2</title>
        <statement>
          <p>In this exercise, you will practice transforming basic R plots into their ggplot2 equivalents. While we can't directly run ggplot2 code in our Web-R environment, this will help you understand the syntax and structure for when you work in a full R environment.</p>
          
          <p>For each of the following base R plots, write the equivalent ggplot2 code:</p>
          
          <paragraphs>
            <title>Plot 1: Scatter Plot</title>
            <pre>
# Base R code
plot(mtcars$wt, mtcars$mpg,
     main = "Car Weight vs. Fuel Efficiency",
     xlab = "Weight (1000 lbs)",
     ylab = "Miles per Gallon",
     pch = 19,
     col = "blue")
            </pre>
          </paragraphs>
          
          <paragraphs>
            <title>Plot 2: Histogram</title>
            <pre>
# Base R code
hist(mtcars$mpg,
     main = "Distribution of Fuel Efficiency",
     xlab = "Miles per Gallon",
     col = "lightblue",
     breaks = 10)
            </pre>
          </paragraphs>
          
          <paragraphs>
            <title>Plot 3: Box Plot</title>
            <pre>
# Base R code
boxplot(mpg ~ factor(cyl), data = mtcars,
        main = "Fuel Efficiency by Number of Cylinders",
        xlab = "Number of Cylinders",
        ylab = "Miles per Gallon",
        col = c("lightgreen", "lightblue", "lightpink"))
            </pre>
          </paragraphs>
          
          <paragraphs>
            <title>Plot 4: Line Plot</title>
            <pre>
# Base R code
# Assume we have time series data in a data frame called 'ts_data'
# with columns 'date' and 'value'
plot(ts_data$date, ts_data$value,
     type = "l",
     main = "Time Series Data",
     xlab = "Date",
     ylab = "Value",
     col = "red")
            </pre>
          </paragraphs>
        </statement>
      </activity>
      
      <exercise xml:id="ex-ggplot2-equivalents">
        <title>ggplot2 Equivalents</title>
        <statement>
          <p>Write ggplot2 code for each of the following base R plots:</p>
        </statement>
        <exercise>
          <statement>
            <p>Plot 1: Scatter Plot</p>
          </statement>
          <answer>
            <pre>
ggplot(mtcars, aes(x = wt, y = mpg)) +
  geom_point(color = "blue", size = 3) +
  labs(title = "Car Weight vs. Fuel Efficiency",
       x = "Weight (1000 lbs)",
       y = "Miles per Gallon") +
  theme_minimal()
            </pre>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Plot 2: Histogram</p>
          </statement>
          <answer>
            <pre>
ggplot(mtcars, aes(x = mpg)) +
  geom_histogram(bins = 10, fill = "lightblue", color = "black") +
  labs(title = "Distribution of Fuel Efficiency",
       x = "Miles per Gallon",
       y = "Count") +
  theme_minimal()
            </pre>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Plot 3: Box Plot</p>
          </statement>
          <answer>
            <pre>
ggplot(mtcars, aes(x = factor(cyl), y = mpg, fill = factor(cyl))) +
  geom_boxplot() +
  scale_fill_manual(values = c("lightgreen", "lightblue", "lightpink")) +
  labs(title = "Fuel Efficiency by Number of Cylinders",
       x = "Number of Cylinders",
       y = "Miles per Gallon") +
  theme_minimal() +
  guides(fill = "none")  # Remove the legend
            </pre>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Plot 4: Line Plot</p>
          </statement>
          <answer>
            <pre>
ggplot(ts_data, aes(x = date, y = value)) +
  geom_line(color = "red", size = 1) +
  labs(title = "Time Series Data",
       x = "Date",
       y = "Value") +
  theme_minimal()
            </pre>
          </answer>
        </exercise>
      </exercise>
    </subsection>
  </section>

  <!-- TOPIC 11: ADVANCED VISUALIZATIONS -->
  <section xml:id="sec-advanced-visualizations">
    <title>Advanced Visualizations</title>
    
    <!-- Session 21: Single variable visualizations -->
    <subsection xml:id="subsec-single-var-viz">
      <title>Histograms, density plots, and bar charts</title>
      
      <p>Visualizing the distribution of a single variable is fundamental in exploratory data analysis. There are several techniques for effectively visualizing both continuous and categorical variables.</p>
      
      <p><term>For continuous variables</term>, the main visualization types include:</p>
      
      <paragraphs>
        <title>Histograms</title>
        <p>Histograms divide the data into bins and show the frequency or count of observations in each bin. They are useful for understanding the shape, center, and spread of a distribution.</p>
        <p>Key considerations:</p>
        <ul>
          <li><p>Bin width affects the appearance and interpretation</p></li>
          <li><p>Can reveal modality, skewness, and potential outliers</p></li>
          <li><p>Y-axis can show counts or density</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Density Plots</title>
        <p>Density plots show a smoothed version of the distribution, making it easier to identify the shape. They're particularly useful for comparing multiple distributions.</p>
        <p>Key considerations:</p>
        <ul>
          <li><p>Smoothing parameter affects the appearance</p></li>
          <li><p>Can be overlaid with other density plots for comparison</p></li>
          <li><p>Area under the curve equals 1</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Box Plots</title>
        <p>Box plots display the five-number summary (minimum, first quartile, median, third quartile, maximum) and highlight potential outliers. They're compact and show key distribution features.</p>
        <p>Key considerations:</p>
        <ul>
          <li><p>Shows central tendency, spread, and outliers</p></li>
          <li><p>Facilitates comparison between groups</p></li>
          <li><p>Less detailed than histograms about distribution shape</p></li>
        </ul>
      </paragraphs>
      
      <p><term>For categorical variables</term>, the main visualization type is:</p>
      
      <paragraphs>
        <title>Bar Charts</title>
        <p>Bar charts show the frequency or count of observations in each category. They're ideal for comparing values across discrete categories.</p>
        <p>Key considerations:</p>
        <ul>
          <li><p>Can be sorted by frequency for better interpretation</p></li>
          <li><p>Categories can be reordered based on another variable</p></li>
          <li><p>Horizontal orientation works well for long category labels</p></li>
        </ul>
      </paragraphs>
      
      <program interactive="webr" xml:id="prog-single-var-viz">
        <input>
<![CDATA[
# Load datasets
data(mtcars)
data(iris)

# Histogram example
hist(mtcars$mpg, 
     main = "Distribution of Fuel Efficiency",
     xlab = "Miles Per Gallon",
     col = "lightblue",
     breaks = 10)

# Box plot example
boxplot(mtcars$mpg, 
       main = "Box Plot of Fuel Efficiency",
       ylab = "Miles Per Gallon",
       col = "lightgreen")

# Density plot example (using base R)
plot(density(mtcars$mpg), 
     main = "Density Plot of Fuel Efficiency",
     xlab = "Miles Per Gallon",
     col = "blue",
     lwd = 2)

# Filling the density curve
polygon(density(mtcars$mpg), col = "lightblue", border = "blue")

# Bar chart for a categorical variable
# First, create a categorical variable from cyl
cyl_factor <- factor(mtcars$cyl, levels = c(4, 6, 8), 
                    labels = c("4 cylinders", "6 cylinders", "8 cylinders"))

# Create a table of counts
cyl_counts <- table(cyl_factor)

# Bar chart
barplot(cyl_counts,
        main = "Number of Cars by Cylinder Count",
        xlab = "Cylinder Count",
        ylab = "Number of Cars",
        col = c("lightgreen", "lightblue", "lightpink"))

# Comparing multiple distributions
# Create separate density plots for each species
plot(density(iris$Sepal.Length[iris$Species == "setosa"]), 
     main = "Sepal Length Distribution by Species",
     xlab = "Sepal Length (cm)",
     ylim = c(0, 1.5),
     col = "red",
     lwd = 2)

# Add the other species
lines(density(iris$Sepal.Length[iris$Species == "versicolor"]), 
      col = "blue", 
      lwd = 2)
lines(density(iris$Sepal.Length[iris$Species == "virginica"]), 
      col = "green", 
      lwd = 2)

# Add a legend
legend("topright", 
       legend = c("Setosa", "Versicolor", "Virginica"),
       col = c("red", "blue", "green"),
       lwd = 2)

# Compare distributions with box plots
boxplot(Sepal.Length ~ Species, data = iris,
        main = "Sepal Length by Species",
        xlab = "Species",
        ylab = "Sepal Length (cm)",
        col = c("lightpink", "lightblue", "lightgreen"))
]]>
        </input>
      </program>
      
      <exercise xml:id="ex-distribution-viz">
        <title>Distribution Visualization</title>
        <statement>
          <p>For each of the following scenarios, identify the most appropriate visualization type and explain why:</p>
        </statement>
        <exercise>
          <statement>
            <p>You want to show the distribution of ages in a population, which ranges from 18 to 85 years old.</p>
          </statement>
          <answer>
            <p>A histogram or density plot would be most appropriate for visualizing age distribution because:</p>
            <ul>
              <li><p>Age is a continuous variable spanning a wide range (18-85 years)</p></li>
              <li><p>A histogram would show the frequency of people in different age brackets, revealing the overall shape of the distribution (e.g., whether it's normal, skewed, bimodal)</p></li>
              <li><p>A density plot would provide a smoother representation of the same information, which might be more interpretable if the sample size is large</p></li>
              <li><p>These visualizations would reveal important features such as central tendency, spread, and any unusual patterns like clusters around certain ages</p></li>
            </ul>
            <p>While a box plot could also be used, it would provide less detail about the actual distribution shape, which is often important when examining demographic variables like age.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>You want to compare the income levels of five different occupational categories.</p>
          </statement>
          <answer>
            <p>A box plot would be most appropriate for comparing income levels across occupational categories because:</p>
            <ul>
              <li><p>It allows for comparing distributions of a continuous variable (income) across categories (occupations)</p></li>
              <li><p>Box plots visually summarize key statistics (median, quartiles, range) that are relevant for income comparisons</p></li>
              <li><p>Income data often contains outliers, which box plots clearly identify</p></li>
              <li><p>The visualization makes it easy to compare central tendency (median) and spread (IQR) across the five occupational categories</p></li>
              <li><p>The compact design allows easy side-by-side comparison even with limited space</p></li>
            </ul>
            <p>Alternative approaches could include violin plots (for more detailed distribution comparison) or grouped/overlaid density plots, but box plots provide the clearest summary for comparing income distributions across multiple categories.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>You want to show the frequency of different blood types (A, B, AB, O) in a patient population.</p>
          </statement>
          <answer>
            <p>A bar chart would be most appropriate for showing the frequency of blood types because:</p>
            <ul>
              <li><p>Blood type is a categorical variable with distinct, non-ordinal categories (A, B, AB, O)</p></li>
              <li><p>Bar charts effectively show counts or proportions for categorical data</p></li>
              <li><p>The height of each bar provides a clear visual comparison of the relative frequency of each blood type</p></li>
              <li><p>Labels can be clearly attached to each category</p></li>
              <li><p>The visualization is intuitive and widely understood, even by audiences without statistical background</p></li>
            </ul>
            <p>A pie chart could also be used since there are only four categories, but bar charts generally allow for more accurate visual comparison of values. If you wanted to further break down the data (such as by Rh factor: positive/negative), a grouped bar chart would allow for that additional level of detail.</p>
          </answer>
        </exercise>
      </exercise>
    </subsection>
    
    <!-- Customizing aesthetics and themes -->
    <subsection xml:id="subsec-aesthetics-themes">
      <title>Customizing aesthetics and themes</title>
      
      <p>Visual customization is essential for creating effective and appealing visualizations. Thoughtful customization can enhance readability, emphasize key patterns, and create a professional appearance.</p>
      
      <p>Key aspects of visualization customization include:</p>
      
      <paragraphs>
        <title>Color Selection</title>
        <p>Colors should be chosen deliberately to enhance understanding and accessibility:</p>
        <ul>
          <li><p>Use color to highlight important data or distinguish categories</p></li>
          <li><p>Consider color blindness (avoid red-green combinations)</p></li>
          <li><p>Use sequential color schemes for ordered data (e.g., light to dark blue for low to high values)</p></li>
          <li><p>Use diverging color schemes for data with a meaningful midpoint (e.g., blue-white-red for negative-zero-positive)</p></li>
          <li><p>Use qualitative color schemes for categorical data (e.g., distinct colors for different categories)</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Titles and Labels</title>
        <p>Clear and informative text elements improve understanding:</p>
        <ul>
          <li><p>Use descriptive titles that explain what the visualization shows</p></li>
          <li><p>Label axes clearly with both the variable name and units</p></li>
          <li><p>Include a subtitle or caption for additional context if needed</p></li>
          <li><p>Consider direct labeling instead of or in addition to legends</p></li>
          <li><p>Use consistent terminology throughout the visualization</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Visual Elements</title>
        <p>Adjust visual properties to enhance clarity and focus:</p>
        <ul>
          <li><p>Control point size, line width, and transparency</p></li>
          <li><p>Use appropriate fonts and font sizes</p></li>
          <li><p>Consider the aspect ratio of the plot</p></li>
          <li><p>Use grid lines and backgrounds judiciously</p></li>
          <li><p>Add reference lines or annotations to highlight key values</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Themes</title>
        <p>Overall visual style can be customized through themes:</p>
        <ul>
          <li><p>Control background colors, grid lines, fonts, and other non-data elements</p></li>
          <li><p>Choose themes appropriate for the context (presentation, publication, dashboard)</p></li>
          <li><p>Create consistent visual identity across multiple visualizations</p></li>
          <li><p>Consider minimalist themes that emphasize the data</p></li>
        </ul>
      </paragraphs>
      
      <program interactive="webr" xml:id="prog-customization">
        <input>
<![CDATA[
# Load dataset
data(iris)

# Basic customization in base R plots
# Create a custom color palette
my_colors <- c("#E69F00", "#56B4E9", "#009E73")

# Create a scatterplot with customization
plot(iris$Sepal.Length, iris$Sepal.Width,
     main = "Sepal Dimensions by Species",
     xlab = "Sepal Length (cm)",
     ylab = "Sepal Width (cm)",
     pch = 19,                     # Solid circle points
     col = my_colors[iris$Species], # Custom colors by species
     cex = 1.2,                    # Slightly larger points
     font.main = 2,                # Bold title
     family = "serif",             # Font family
     bty = "l",                    # Box type - "L" shaped axes
     las = 1)                      # Horizontal axis labels

# Add a customized legend
legend("topright", 
       legend = levels(iris$Species),
       col = my_colors, 
       pch = 19,
       pt.cex = 1.2,
       title = "Species",
       bg = "white",
       box.lty = 1,
       box.lwd = 1,
       box.col = "gray80",
       cex = 0.9)

# Custom axes and grid
# Create a plot with custom axes
par(mar = c(5, 5, 4, 2))  # Increase margin for larger labels

plot(iris$Petal.Length, iris$Petal.Width,
     main = "Petal Dimensions by Species",
     xlab = "",  # We'll add custom labels
     ylab = "",
     pch = c(21, 22, 24)[iris$Species],  # Different point shapes
     bg = my_colors[iris$Species],       # Background colors
     col = "black",                      # Point borders
     cex = 1.3,                          # Point size
     axes = FALSE)                       # No default axes

# Add custom axes
axis(1, at = seq(1, 7, by = 1), 
     labels = seq(1, 7, by = 1), 
     lwd = 1.5, cex.axis = 0.9)
axis(2, at = seq(0, 2.5, by = 0.5), 
     labels = seq(0, 2.5, by = 0.5), 
     lwd = 1.5, cex.axis = 0.9, las = 1)

# Add grid lines
grid(nx = 6, ny = 5, 
     lty = 2, col = "gray80")

# Add axis labels with more customization
mtext("Petal Length (cm)", side = 1, line = 3, cex = 1.2)
mtext("Petal Width (cm)", side = 2, line = 3, cex = 1.2, las = 0)

# Add a legend
legend("topleft", 
       legend = levels(iris$Species),
       pt.bg = my_colors, 
       pch = c(21, 22, 24),
       pt.cex = 1.3,
       title = "Species",
       bg = "white",
       box.lty = 1,
       box.lwd = 1.5,
       cex = 0.9)

# Add a box around the plot
box(lwd = 1.5)

# Add a subtitle
mtext("Comparing Petal Dimensions Across Three Iris Species", 
      side = 3, line = 0.5, cex = 0.8)

# Reset plotting parameters
par(mar = c(5, 4, 4, 2) + 0.1)  # Reset to default margins

# Creating multiple plots with consistent styling
par(mfrow = c(2, 2), 
    mar = c(4, 4, 3, 1),  # Reduced margins
    bg = "white",         # White background
    col.main = "black",   # Title color
    col.axis = "darkgray", # Axis color
    col.lab = "black",    # Label color
    family = "serif")     # Font family

# Plot 1: Sepal Length Histogram
hist(iris$Sepal.Length, 
     main = "Sepal Length",
     xlab = "Length (cm)",
     col = "#E69F00",
     border = "white",
     breaks = 10)

# Plot 2: Sepal Width Histogram
hist(iris$Sepal.Width, 
     main = "Sepal Width",
     xlab = "Width (cm)",
     col = "#56B4E9",
     border = "white",
     breaks = 10)

# Plot 3: Petal Length Histogram
hist(iris$Petal.Length, 
     main = "Petal Length",
     xlab = "Length (cm)",
     col = "#009E73",
     border = "white",
     breaks = 10)

# Plot 4: Petal Width Histogram
hist(iris$Petal.Width, 
     main = "Petal Width",
     xlab = "Width (cm)",
     col = "#CC79A7",
     border = "white",
     breaks = 10)

# Reset to default settings
par(mfrow = c(1, 1), mar = c(5, 4, 4, 2) + 0.1)
]]>
        </input>
      </program>
      
      <exercise xml:id="ex-visualization-makeover">
        <title>Visualization Makeover</title>
        <statement>
          <p>Consider the following base plot code which creates a simple scatter plot:</p>
          <pre>
plot(mtcars$wt, mtcars$mpg)
          </pre>
          
          <p>Describe how you would enhance this visualization to make it more informative and visually appealing. Include specific improvements for each of the following aspects:</p>
          <ol>
            <li>
              <p>Titles and labels</p>
            </li>
            <li>
              <p>Point styling (color, shape, size)</p>
            </li>
            <li>
              <p>Additional elements to enhance understanding (e.g., trend line, annotations)</p>
            </li>
            <li>
              <p>Overall theme and aesthetics</p>
            </li>
            <li>
              <p>How you would use color to add another dimension of information</p>
            </li>
          </ol>
        </statement>
        <response/>
      </exercise>
    </subsection>
    
    <!-- Hands-on practice: Create polished single-variable plots -->
    <subsection xml:id="subsec-practice-polished-plots">
      <title>Hands-on practice: Create polished single-variable plots</title>
      
      <activity xml:id="activity-polished-plots">
        <title>Creating Polished Plots</title>
        <statement>
          <p>In this activity, you will create polished, publication-quality visualizations for single variables. While we'll use base R for this exercise, the principles apply to any visualization tool.</p>
          
          <p>Choose one continuous variable and one categorical variable from the built-in datasets (mtcars, iris, etc.) and create polished visualizations for each:</p>
          
          <p>For the continuous variable:</p>
          <ol>
            <li>
              <p>Create a histogram with the following enhancements:</p>
              <ul>
                <li><p>Informative title and axis labels</p></li>
                <li><p>Appropriate bin width</p></li>
                <li><p>Pleasing color scheme</p></li>
                <li><p>Added reference lines for mean and median</p></li>
                <li><p>Annotations explaining key features</p></li>
              </ul>
            </li>
            <li>
              <p>Create a boxplot for the same variable, grouped by another categorical variable, with similar enhancements</p>
            </li>
          </ol>
          
          <p>For the categorical variable:</p>
          <ol>
            <li>
              <p>Create a bar chart with the following enhancements:</p>
              <ul>
                <li><p>Informative title and axis labels</p></li>
                <li><p>Ordered bars (by frequency or another meaningful order)</p></li>
                <li><p>Color scheme that highlights important categories</p></li>
                <li><p>Value labels on the bars</p></li>
                <li><p>Explanation of significance in a caption or annotation</p></li>
              </ul>
            </li>
          </ol>
          
          <p>Document your code and explain the customization choices you made.</p>
        </statement>
      </activity>
      
      <program interactive="webr" xml:id="prog-polished-example">
        <input>
<![CDATA[
# Example of a polished visualization for a continuous variable
# We'll use the mpg variable from mtcars

# Load data
data(mtcars)

# Calculate statistics for reference lines
mean_mpg <- mean(mtcars$mpg)
median_mpg <- median(mtcars$mpg)

# Set up a pleasing color scheme
hist_color <- "#4292c6"
mean_color <- "#de2d26"
median_color <- "#31a354"

# Create a polished histogram
hist(mtcars$mpg,
     main = "Distribution of Fuel Efficiency in 1974 Cars",
     xlab = "Miles Per Gallon (MPG)",
     ylab = "Number of Cars",
     col = hist_color,
     border = "white",
     breaks = 10,
     xlim = c(10, 35),
     cex.main = 1.2,
     cex.lab = 1.1)

# Add reference lines for mean and median
abline(v = mean_mpg, col = mean_color, lwd = 2, lty = 2)
abline(v = median_mpg, col = median_color, lwd = 2, lty = 3)

# Add a legend
legend("topright",
       legend = c(paste("Mean:", round(mean_mpg, 1)),
                  paste("Median:", round(median_mpg, 1))),
       col = c(mean_color, median_color),
       lwd = 2,
       lty = c(2, 3),
       bg = "white",
       cex = 0.9)

# Add annotations
text(14, 7, "Low efficiency\nrange", cex = 0.8)
text(30, 7, "High efficiency\nrange", cex = 0.8)

# Add a meaningful caption
mtext("Most cars in the dataset fall between 15-20 MPG, with a few high-efficiency outliers.",
      side = 1, line = 4, cex = 0.8)

# Example of a boxplot for the same variable, grouped by cylinder count
# First, convert cylinders to a factor for better labels
mtcars$cyl_f <- factor(mtcars$cyl, 
                      levels = c(4, 6, 8),
                      labels = c("4-cylinder", "6-cylinder", "8-cylinder"))

# Create a custom color palette
cyl_colors <- c("4-cylinder" = "#4daf4a", 
               "6-cylinder" = "#377eb8", 
               "8-cylinder" = "#e41a1c")

# Create a polished boxplot
boxplot(mpg ~ cyl_f, data = mtcars,
        main = "Fuel Efficiency by Engine Size",
        xlab = "Engine Type",
        ylab = "Miles Per Gallon (MPG)",
        col = cyl_colors,
        border = "gray30",
        notch = TRUE,  # Add notches for confidence intervals
        cex.main = 1.2,
        cex.lab = 1.1,
        cex.axis = 0.9,
        outpch = 16,   # Solid circle for outliers
        outcol = "gray30")

# Add a horizontal line for overall average
abline(h = mean(mtcars$mpg), col = "gray30", lwd = 1.5, lty = 2)
text(3.2, mean(mtcars$mpg) + 1, "Overall Mean", cex = 0.8)

# Add annotations
mtext("Notches indicate 95% confidence interval around median", 
      side = 3, line = 0.3, cex = 0.8)
mtext("Data source: 1974 Motor Trend US magazine", 
      side = 1, line = 3, cex = 0.8)

# Example of a polished bar chart for a categorical variable
# We'll use the number of cars by cylinder count

# Calculate the count of cars by cylinder
cyl_counts <- table(mtcars$cyl)

# Create a data frame for easier manipulation
cyl_data <- data.frame(
  Cylinders = c("4-cylinder", "6-cylinder", "8-cylinder"),
  Count = as.numeric(cyl_counts),
  Percentage = round(100 * as.numeric(cyl_counts) / sum(cyl_counts), 1)
)

# Sort by count (descending)
cyl_data <- cyl_data[order(-cyl_data$Count), ]

# Create a polished bar chart
barplot(cyl_data$Count,
        names.arg = cyl_data$Cylinders,
        main = "Distribution of Cars by Engine Type",
        xlab = "Engine Type",
        ylab = "Number of Cars",
        col = cyl_colors[cyl_data$Cylinders],
        border = "white",
        ylim = c(0, max(cyl_data$Count) * 1.2),  # Make room for labels
        cex.main = 1.2,
        cex.lab = 1.1,
        cex.names = 0.9)

# Add count labels on top of bars
text(seq(0.7, by = 1.2, length.out = 3), 
     cyl_data$Count + 0.8, 
     paste0(cyl_data$Count, " (", cyl_data$Percentage, "%)"), 
     cex = 0.9)

# Add a caption
mtext("8-cylinder engines were the most common in the 1974 dataset, reflecting the prevalence of large engines before fuel economy became a priority.",
      side = 1, line = 4, cex = 0.8)
]]>
        </input>
      </program>
      
      <exercise xml:id="ex-visualization-critique">
        <title>Visualization Critique</title>
        <statement>
          <p>Examine the following R code and the visualization it creates. Identify three specific ways to improve the visualization and explain why these changes would make the chart more effective.</p>
          <pre>
# Problematic visualization code
sales &lt;- c(120, 90, 115, 45, 170)
months &lt;- c("Jan", "Feb", "Mar", "Apr", "May")
barplot(sales, names.arg = months, 
        col = rainbow(5), 
        main = "DATA",
        border = "blue")
          </pre>
        </statement>
        <answer>
          <p>Three key improvements for this visualization:</p>
          
          <p><strong>1. Improve the title and add axis labels</strong></p>
          <ul>
            <li><p><em>Issue</em>: The title "DATA" is vague and uninformative. There are no axis labels to indicate what the values represent.</p></li>
            <li><p><em>Improvement</em>: Replace with a descriptive title like "Monthly Sales: January-May 2025" and add a y-axis label such as "Sales ($ thousands)" to clearly communicate what data is being shown.</p></li>
            <li><p><em>Why it matters</em>: Proper titles and labels are essential for viewers to understand what they're looking at without requiring additional explanation.</p></li>
          </ul>
          
          <p><strong>2. Use a more appropriate color scheme</strong></p>
          <ul>
            <li><p><em>Issue</em>: The rainbow color scheme creates visual confusion by assigning unrelated colors to sequential months, suggesting categorical differences when the data is actually a time series.</p></li>
            <li><p><em>Improvement</em>: Use a single color for all bars (since months are part of a sequence) or a sequential color palette if highlighting a trend. Remove the blue borders which add visual noise.</p></li>
            <li><p><em>Why it matters</em>: Color should enhance understanding, not distract from it. Rainbow palettes are generally discouraged in data visualization as they can be hard to interpret and problematic for color-blind viewers.</p></li>
          </ul>
          
          <p><strong>3. Add value labels and context</strong></p>
          <ul>
            <li><p><em>Issue</em>: The chart lacks context about the dramatic drop in April and subsequent rise in May. There are no exact values shown.</p></li>
            <li><p><em>Improvement</em>: Add value labels on top of each bar and include annotations explaining significant changes (e.g., "April sales dropped due to warehouse closure"). Consider adding a reference line showing the average sales.</p></li>
            <li><p><em>Why it matters</em>: Context and precise values help readers understand not just the pattern but the significance of the data, especially for unexpected values that might otherwise be misinterpreted.</p></li>
          </ul>
          
          <p>Implementing these changes would transform this basic chart into a more professional, informative visualization that effectively communicates the sales trend.</p>
        </answer>
      </exercise>
    </subsection>
  </section>
<!-- TOPIC 11: ADVANCED VISUALIZATIONS (CONTINUED) -->
  <section xml:id="sec-relationships-viz">
    <title>Relationships between variables</title>
    
    <!-- Session 22: Relationships between variables -->
    <subsection xml:id="subsec-scatter-plots">
      <title>Scatter plots, line charts, and heatmaps</title>
      
      <p>Visualizing relationships between variables is essential for identifying patterns, correlations, and potential causal connections in your data. Several plot types are particularly useful for exploring these relationships.</p>
      
      <paragraphs>
        <title>Scatter Plots</title>
        <p>Scatter plots display the relationship between two continuous variables by placing each observation as a point on a two-dimensional plane.</p>
        <p>Key features and enhancements:</p>
        <ul>
          <li><p>Basic form: points positioned according to their x and y values</p></li>
          <li><p>Add a trend line to visualize the overall relationship direction</p></li>
          <li><p>Use color to represent a third categorical variable</p></li>
          <li><p>Use point size to represent a third continuous variable</p></li>
          <li><p>Add transparency when plotting many points to show density</p></li>
          <li><p>Consider adding confidence intervals around trend lines</p></li>
          <li><p>Add annotations to highlight interesting outliers or patterns</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Line Charts</title>
        <p>Line charts connect points with lines, emphasizing the continuity or progression between observations. They're especially useful for time series data.</p>
        <p>Key features and enhancements:</p>
        <ul>
          <li><p>Basic form: points connected by lines in sequence</p></li>
          <li><p>Multiple lines can show different categories or series</p></li>
          <li><p>Use line style (solid, dashed, dotted) to distinguish categories</p></li>
          <li><p>Add markers at data points for more precision</p></li>
          <li><p>Consider dual y-axes for variables with different scales (use cautiously)</p></li>
          <li><p>Highlight important segments or anomalies</p></li>
          <li><p>Add annotations for key events or turning points</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Heatmaps</title>
        <p>Heatmaps use color intensity to represent the value of a third variable across a grid defined by two categorical or binned continuous variables.</p>
        <p>Key features and enhancements:</p>
        <ul>
          <li><p>Basic form: a grid with cells colored according to their value</p></li>
          <li><p>Commonly used for correlation matrices</p></li>
          <li><p>Choose appropriate color scales (sequential, diverging)</p></li>
          <li><p>Add dendrograms for hierarchical clustering</p></li>
          <li><p>Include value labels in cells when appropriate</p></li>
          <li><p>Order rows and columns meaningfully</p></li>
          <li><p>Consider normalization for better comparison</p></li>
        </ul>
      </paragraphs>
      
      <program interactive="webr" xml:id="prog-relationship-viz">
        <input>
<![CDATA[
# Load datasets
data(mtcars)
data(iris)

# Basic scatter plot
plot(mtcars$wt, mtcars$mpg,
     main = "Car Weight vs. Fuel Efficiency",
     xlab = "Weight (1000 lbs)",
     ylab = "Miles Per Gallon",
     pch = 19,
     col = "darkblue")

# Add a trend line
model <- lm(mpg ~ wt, data = mtcars)
abline(model, col = "red", lwd = 2)

# Add model equation and R-squared
rsq <- round(summary(model)$r.squared, 2)
eq <- paste0("mpg = ", round(coef(model)[1], 1), 
            " + (", round(coef(model)[2], 1), " × weight)")
legend("topright", legend = c(eq, paste0("R² = ", rsq)),
       bty = "n")  # No box around legend

# Enhanced scatter plot with color for a third variable
# Convert cylinders to a factor
cyl_f <- as.factor(mtcars$cyl)

# Create a color palette
cyl_colors <- c("4" = "#4DAF4A", "6" = "#377EB8", "8" = "#E41A1C")

# Plot with colors by cylinder count
plot(mtcars$wt, mtcars$mpg,
     main = "Car Weight vs. Fuel Efficiency by Engine Size",
     xlab = "Weight (1000 lbs)",
     ylab = "Miles Per Gallon",
     pch = 19,
     col = cyl_colors[cyl_f],
     cex = 1.2)

# Add a legend
legend("topright", 
       legend = c("4 cylinders", "6 cylinders", "8 cylinders"),
       col = cyl_colors,
       pch = 19,
       bty = "n",
       title = "Engine Type")

# Add separate trend lines for each cylinder group
for(cyl in c(4, 6, 8)) {
  subset_data <- mtcars[mtcars$cyl == cyl, ]
  if(nrow(subset_data) > 2) {  # Need at least 3 points for a meaningful trend
    model <- lm(mpg ~ wt, data = subset_data)
    abline(model, col = cyl_colors[as.character(cyl)], lwd = 2, lty = 2)
  }
}

# Create a line chart for a time series
# Create a simple time series dataset
months <- 1:12
temps <- c(35, 37, 45, 55, 65, 75, 82, 80, 72, 60, 48, 38)
rainfall <- c(3.2, 2.8, 3.5, 3.9, 4.1, 3.7, 3.0, 2.9, 3.3, 3.2, 3.6, 3.1)

# Plot temperature over months
plot(months, temps,
     type = "b",  # Both points and lines
     main = "Monthly Temperature and Rainfall",
     xlab = "Month",
     ylab = "Temperature (°F)",
     pch = 19,
     col = "red",
     xaxt = "n",  # No x-axis labels yet
     ylim = c(30, 90))  # Set y limits

# Add month names as x-axis labels
month_abbr <- c("Jan", "Feb", "Mar", "Apr", "May", "Jun", 
                "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
axis(1, at = months, labels = month_abbr, las = 2)

# Add a second y-axis for rainfall
par(new = TRUE)  # Add a new plot on top
plot(months, rainfall,
     type = "b",
     pch = 17,
     col = "blue",
     xlab = "",
     ylab = "",
     xaxt = "n",  # No second x-axis
     yaxt = "n")  # No second y-axis yet

# Add the second y-axis
axis(4, col = "blue", col.axis = "blue")
mtext("Rainfall (inches)", side = 4, line = 3, col = "blue")

# Add a legend
legend("topleft", 
       legend = c("Temperature", "Rainfall"),
       col = c("red", "blue"),
       pch = c(19, 17),
       lty = 1,
       bty = "n")

# Create a simple correlation heatmap
# Calculate the correlation matrix for iris measurements
iris_cor <- cor(iris[, 1:4])

# Create a simple heatmap-like visualization
# First, create a layout for the visualization
layout_matrix <- matrix(c(1, 0, 2), nrow = 1)
layout(layout_matrix, widths = c(10, 1, 2))

# Set margins
par(mar = c(5, 5, 4, 1))

# Create a color palette for the correlation values
col_palette <- colorRampPalette(c("#67001F", "#B2182B", "#D6604D", 
                                 "#F4A582", "#FDDBC7", "#FFFFFF", 
                                 "#D1E5F0", "#92C5DE", "#4393C3", 
                                 "#2166AC", "#053061"))(100)

# Create a matrix of colors based on correlation values
color_indices <- round((iris_cor + 1) * 49) + 1
colors_matrix <- matrix(col_palette[color_indices], nrow = 4)

# Create an empty plot
plot(0, 0, type = "n", xlim = c(0, 4), ylim = c(0, 4),
     xlab = "", ylab = "", xaxt = "n", yaxt = "n",
     main = "Correlation Heatmap of Iris Measurements")

# Add variable names
variable_names <- c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")
axis(1, at = 0.5:3.5, labels = variable_names, las = 2)
axis(2, at = 3.5:0.5, labels = variable_names, las = 2)

# Add colored cells
for(i in 1:4) {
  for(j in 1:4) {
    rect(j-1, 4-i, j, 5-i, col = colors_matrix[i, j], border = "white")
    # Add correlation values
    text(j-0.5, 4.5-i, round(iris_cor[i, j], 2), 
         col = ifelse(abs(iris_cor[i, j]) > 0.7, "white", "black"))
  }
}

# Create a color scale legend
par(mar = c(5, 1, 4, 3))
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(-1, 1),
     xlab = "", ylab = "", xaxt = "n", yaxt = "n", bty = "n")
for(i in 1:100) {
  y_bottom <- -1 + (i-1) * 0.02
  y_top <- -1 + i * 0.02
  rect(0, y_bottom, 1, y_top, col = col_palette[i], border = NA)
}
axis(4, at = seq(-1, 1, by = 0.5), las = 2)
mtext("Correlation", side = 4, line = 2)

# Reset the layout
layout(1)
par(mar = c(5, 4, 4, 2) + 0.1)  # Reset to default margins
]]>
        </input>
      </program>
      
      <exercise xml:id="ex-relationship-plots">
        <title>Relationship Visualization Selection</title>
        <statement>
          <p>For each of the following scenarios, identify the most appropriate plot type for visualizing the relationship and explain why:</p>
        </statement>
        <exercise>
          <statement>
            <p>You want to visualize how a company's stock price has changed over the past five years, including major company events and market trends.</p>
          </statement>
          <answer>
            <p>A line chart would be most appropriate for visualizing stock price changes over time because:</p>
            <ul>
              <li><p>It effectively shows the continuous nature of time series data</p></li>
              <li><p>The connected points emphasize trends, patterns, and volatility over the five-year period</p></li>
              <li><p>The x-axis can represent dates in chronological order</p></li>
              <li><p>The y-axis can show the stock price values</p></li>
              <li><p>Major company events and market trends can be annotated directly on the chart with vertical lines or text labels</p></li>
              <li><p>You can easily add visual elements like a moving average line to show underlying trends</p></li>
              <li><p>It allows viewers to quickly identify peaks, troughs, and periods of stability</p></li>
            </ul>
            <p>Enhancements could include adding reference lines for important financial thresholds, color-coding different time periods (e.g., different fiscal years), and using dual axes to compare with market indices.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>You want to examine the correlation between advertising spend and sales across different product categories.</p>
          </statement>
          <answer>
            <p>A scatter plot would be most appropriate for examining the correlation between advertising spend and sales because:</p>
            <ul>
              <li><p>It directly visualizes the relationship between two continuous variables (advertising spend and sales)</p></li>
              <li><p>Each point can represent an observation (e.g., a specific campaign or time period)</p></li>
              <li><p>The overall pattern reveals the strength and direction of the relationship</p></li>
              <li><p>You can add trend lines to quantify the relationship through regression</p></li>
              <li><p>Different product categories can be represented through color or shape, allowing for comparison of relationships across categories</p></li>
              <li><p>Outliers and unusual cases become immediately visible</p></li>
              <li><p>The density of points shows where most observations fall</p></li>
            </ul>
            <p>Adding regression lines for each product category would help quantify the different advertising-to-sales relationships, and including confidence intervals around these lines would indicate the reliability of the trends. You could also add a third dimension using point size to represent another relevant variable like market size or profit margin.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>You want to analyze the correlation matrix of multiple financial indicators (GDP growth, unemployment rate, inflation, interest rates, stock market returns) across several years.</p>
          </statement>
          <answer>
            <p>A heatmap would be most appropriate for analyzing a correlation matrix of financial indicators because:</p>
            <ul>
              <li><p>It efficiently visualizes the entire correlation matrix, showing relationships between all pairs of variables simultaneously</p></li>
              <li><p>Color intensity provides an intuitive way to understand correlation strength and direction</p></li>
              <li><p>The grid layout makes it easy to identify patterns, clusters, and outlying relationships</p></li>
              <li><p>It works well for comparing multiple variables (the various financial indicators)</p></li>
              <li><p>A diverging color palette (e.g., blue to red) can clearly show positive vs. negative correlations</p></li>
              <li><p>The correlation values can be displayed directly in the cells</p></li>
              <li><p>Hierarchical clustering can be added to group similar indicators</p></li>
            </ul>
            <p>Enhancements could include ordering the indicators to highlight meaningful patterns, adding dendrograms to show relationship hierarchies, and using color scales that highlight specific threshold values of correlation (e.g., highlighting correlations above 0.7 or below -0.7).</p>
          </answer>
        </exercise>
      </exercise>
    </subsection>
    
    <!-- Visualizing correlations and patterns -->
    <subsection xml:id="subsec-visualizing-correlations">
      <title>Visualizing correlations and patterns</title>
      
      <p>Identifying and visualizing correlations and patterns in your data is a critical step in data analysis. Several specialized techniques can help reveal these relationships more clearly.</p>
      
      <paragraphs>
        <title>Correlation Matrices</title>
        <p>Correlation matrices show the pairwise correlations between multiple variables. They help identify which variables are strongly related to each other.</p>
        <p>Visualization approaches:</p>
        <ul>
          <li><p>Numeric tables with color highlighting</p></li>
          <li><p>Heatmaps with color intensity representing correlation strength</p></li>
          <li><p>Network graphs where strongly correlated variables are connected</p></li>
          <li><p>Hierarchically clustered correlation matrices to show related groups</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Scatterplot Matrices</title>
        <p>Scatterplot matrices (sometimes called "sploms" or "pairs plots") show all pairwise relationships between multiple variables simultaneously.</p>
        <p>Key features:</p>
        <ul>
          <li><p>Grid of scatterplots showing each variable plotted against every other variable</p></li>
          <li><p>Diagonal often shows distributions of individual variables</p></li>
          <li><p>Can be enhanced with trend lines, correlation coefficients, or density plots</p></li>
          <li><p>Color or shape can represent a categorical grouping variable</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Bubble Charts</title>
        <p>Bubble charts extend scatter plots by using point size to represent a third continuous variable.</p>
        <p>Key features:</p>
        <ul>
          <li><p>X and Y positions show two continuous variables</p></li>
          <li><p>Point size represents a third continuous variable</p></li>
          <li><p>Color can represent a fourth categorical variable</p></li>
          <li><p>Useful for multivariate analysis</p></li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Contour Plots and Density Maps</title>
        <p>Contour plots and density maps show the density or "height" of data points across a two-dimensional space.</p>
        <p>Key features:</p>
        <ul>
          <li><p>Useful for visualizing the concentration of data points</p></li>
          <li><p>Reveals patterns and clusters in dense datasets</p></li>
          <li><p>Similar to topographic maps with "elevation" showing data density</p></li>
          <li><p>Can be overlaid on scatter plots to enhance them</p></li>
        </ul>
      </paragraphs>
      
      <program interactive="webr" xml:id="prog-correlation-viz">
        <input>
<![CDATA[
# Load dataset
data(mtcars)

# Calculate correlation matrix
cor_matrix <- cor(mtcars[, c("mpg", "disp", "hp", "drat", "wt", "qsec")])

# Round for display
round(cor_matrix, 2)

# Create a visual correlation matrix with colors
# Define a function to create a colored correlation matrix
cor_matrix_plot <- function(cormat, title="Correlation Matrix") {
  # Create a blank plot
  plot(c(0, 1), c(0, 1), type = "n", frame.plot = FALSE, axes = FALSE,
       xlab = "", ylab = "", main = title)
  
  # Get the number of variables
  n <- nrow(cormat)
  
  # Define a color palette
  col_palette <- colorRampPalette(c("#67001F", "#B2182B", "#D6604D", 
                                   "#F4A582", "#FDDBC7", "#FFFFFF", 
                                   "#D1E5F0", "#92C5DE", "#4393C3", 
                                   "#2166AC", "#053061"))(100)
  
  # Get variable names
  var_names <- colnames(cormat)
  
  # Calculate cell positions
  cell_size <- 1/n
  
  # Draw the cells
  for(i in 1:n) {
    for(j in 1:n) {
      # Calculate coordinates
      x1 <- (j-1) * cell_size
      x2 <- j * cell_size
      y1 <- 1 - i * cell_size
      y2 <- 1 - (i-1) * cell_size
      
      # Map correlation to color
      col_idx <- round((cormat[i, j] + 1) * 49) + 1
      cell_col <- col_palette[col_idx]
      
      # Draw the cell
      rect(x1, y1, x2, y2, col = cell_col, border = "white")
      
      # Add the correlation value
      text_col <- ifelse(abs(cormat[i, j]) > 0.7, "white", "black")
      text(x1 + cell_size/2, y1 + cell_size/2, 
           round(cormat[i, j], 2), col = text_col)
    }
  }
  
  # Add variable names
  for(i in 1:n) {
    # Variable names on the top
    text(i * cell_size - cell_size/2, 1 + cell_size/4, var_names[i], 
         srt = 45, adj = c(1, 1), cex = 0.8)
    
    # Variable names on the left
    text(-cell_size/4, 1 - i * cell_size + cell_size/2, var_names[i], 
         adj = c(1, 0.5), cex = 0.8)
  }
}

# Create a colored correlation matrix
cor_matrix_plot(cor_matrix, "Car Attributes Correlation Matrix")

# Scatterplot matrix (pairs plot)
pairs(mtcars[, c("mpg", "disp", "hp", "wt")],
      main = "Scatterplot Matrix of Car Attributes",
      pch = 21,
      bg = ifelse(mtcars$cyl == 4, "green3",
                 ifelse(mtcars$cyl == 6, "blue3", "red3")),
      cex = 1.2,
      upper.panel = NULL)  # Only show lower panel for simplicity

# Add correlation coefficients in the upper panel
panel.cor <- function(x, y, digits = 2, cex.cor = 0.8, ...) {
  usr <- par("usr")
  on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- round(cor(x, y), digits)
  txt <- paste0("r = ", r)
  text(0.5, 0.5, txt, cex = cex.cor)
}

# Improved scatterplot matrix with correlation coefficients
pairs(mtcars[, c("mpg", "disp", "hp", "wt")],
      main = "Scatterplot Matrix with Correlation Coefficients",
      pch = 21,
      bg = ifelse(mtcars$cyl == 4, "green3",
                 ifelse(mtcars$cyl == 6, "blue3", "red3")),
      cex = 1.2,
      lower.panel = panel.smooth,  # Smoothed scatter with lowess line
      upper.panel = panel.cor)     # Correlation coefficient

# Bubble chart (scatter plot with size)
# Create a bubble chart showing mpg vs. hp with weight as bubble size
plot(mtcars$hp, mtcars$mpg,
     main = "Car Performance Bubble Chart",
     xlab = "Horsepower",
     ylab = "Miles Per Gallon",
     type = "n")  # Empty plot to set up

# Add bubbles
symbols(mtcars$hp, mtcars$mpg, 
        circles = sqrt(mtcars$wt)/2,  # Size proportional to weight
        inches = FALSE,
        fg = "black",
        bg = ifelse(mtcars$cyl == 4, "#4DAF4A80",
                   ifelse(mtcars$cyl == 6, "#377EB880", "#E41A1C80")),
        add = TRUE)

# Add a legend for cylinder count
legend("topright", 
       legend = c("4 cylinders", "6 cylinders", "8 cylinders"),
       pt.bg = c("#4DAF4A80", "#377EB880", "#E41A1C80"),
       pt.cex = 2,
       pch = 21,
       title = "Engine Type")

# Add a legend for bubble size
legend("topleft",
       legend = c("Light (2000 lbs)", "Medium (3000 lbs)", "Heavy (4000 lbs)"),
       pt.cex = c(1, 1.5, 2),
       pch = 21,
       pt.bg = "gray",
       title = "Car Weight")

# Contour plot / Density map
# Create some sample data
x <- rnorm(500, mean = 0, sd = 1)
y <- rnorm(500, mean = 0, sd = 1)
z <- x^2 + y^2 + rnorm(500, mean = 0, sd = 0.5)

# Create a scatter plot first
plot(x, y, 
     main = "Contour Plot Example",
     xlab = "X Variable",
     ylab = "Y Variable",
     pch = 20,
     col = "#00000020")  # Transparent points

# Add contour lines
contour(kde2d(x, y, n = 50), add = TRUE, col = "blue")

# Create a filled contour plot (density map)
# First, create a new plot for this example
filled.contour(kde2d(x, y, n = 50),
              xlab = "X Variable",
              ylab = "Y Variable",
              main = "Density Map Example",
              color.palette = colorRampPalette(c("white", "blue", "red", "darkred")))
]]>
        </input>
      </program>
      
      <exercise xml:id="ex-correlation-patterns">
        <title>Correlation Pattern Identification</title>
        <statement>
          <p>Looking at the correlation matrix and scatterplot matrix for the mtcars dataset, answer the following questions:</p>
        </statement>
        <exercise>
          <statement>
            <p>Which variables show the strongest negative correlation with miles per gallon (mpg)?</p>
          </statement>
          <answer>
            <p>Based on the correlation matrix for the mtcars dataset, the variables with the strongest negative correlations with miles per gallon (mpg) are:</p>
            <ol>
              <li><p><strong>Weight (wt)</strong>: Shows a very strong negative correlation (approximately -0.87). This indicates that heavier cars tend to have significantly lower fuel efficiency.</p></li>
              <li><p><strong>Displacement (disp)</strong>: Also shows a strong negative correlation (approximately -0.85). Larger engine displacement is strongly associated with lower miles per gallon.</p></li>
              <li><p><strong>Horsepower (hp)</strong>: Exhibits a strong negative correlation (approximately -0.78). Cars with more horsepower tend to have lower fuel efficiency.</p></li>
            </ol>
            <p>These negative correlations make physical sense as heavier cars require more energy to move, and larger, more powerful engines generally consume more fuel. The scatterplot matrix visually confirms these relationships, showing clear downward trends in the plots of mpg versus these variables.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>What patterns do you notice about the relationships between displacement (disp), horsepower (hp), and weight (wt)?</p>
          </statement>
          <answer>
            <p>The relationships between displacement (disp), horsepower (hp), and weight (wt) show several clear patterns:</p>
            <ol>
              <li><p><strong>Strong positive correlations</strong>: All three variables are strongly positively correlated with each other:
                <ul>
                  <li><p>disp and hp: correlation ≈ 0.79-0.90</p></li>
                  <li><p>disp and wt: correlation ≈ 0.89</p></li>
                  <li><p>hp and wt: correlation ≈ 0.66-0.70</p></li>
                </ul>
              </p></li>
              <li><p><strong>Linear relationships</strong>: The scatterplots show generally linear relationships among these variables, particularly between displacement and weight.</p></li>
              <li><p><strong>Clustering by engine size</strong>: When colored by cylinder count, the points form distinct clusters in these relationships, with 4-cylinder cars having lower values for all three variables, and 8-cylinder cars having higher values.</p></li>
              <li><p><strong>Engineering constraints</strong>: These correlations reflect physical engineering relationships - larger engines (higher displacement) tend to produce more power (horsepower) and also weigh more. Additionally, cars designed to carry more weight generally need more powerful engines.</p></li>
            </ol>
            <p>These relationships create a reinforcing effect on fuel efficiency: heavier cars need more powerful engines with larger displacement, and all three factors contribute to lower mpg.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>How would you describe the relationship between quarter-mile time (qsec) and the other variables?</p>
          </statement>
          <answer>
            <p>The quarter-mile time (qsec) shows an interesting pattern of relationships with other variables:</p>
            <ol>
              <li><p><strong>Positive correlation with mpg</strong>: qsec has a moderate positive correlation with miles per gallon (approximately 0.42-0.44). This indicates that cars with better fuel efficiency tend to have longer quarter-mile times (slower acceleration).</p></li>
              <li><p><strong>Negative correlation with performance variables</strong>: qsec shows negative correlations with:
                <ul>
                  <li><p>Horsepower (hp): approximately -0.71</p></li>
                  <li><p>Displacement (disp): approximately -0.43</p></li>
                </ul>
                This makes intuitive sense as more powerful engines typically provide faster acceleration (lower quarter-mile times).
              </p></li>
              <li><p><strong>Weak relationship with weight</strong>: The correlation between qsec and weight is negative but relatively weak (around -0.17 to -0.25). This suggests that while heavier cars might be slightly faster in the quarter-mile, weight is not as strong a predictor of acceleration as horsepower.</p></li>
              <li><p><strong>Complex relationship with rear axle ratio (drat)</strong>: qsec shows a weak positive correlation with drat, suggesting that cars with higher rear axle ratios tend to have slightly slower acceleration, though this relationship isn't strong.</p></li>
            </ol>
            <p>Overall, qsec represents a performance trade-off in these cars: vehicles optimized for fuel efficiency (higher mpg) tend to sacrifice acceleration performance (higher qsec), while high-performance vehicles with powerful engines have faster acceleration but lower fuel efficiency.</p>
          </answer>
        </exercise>
      </exercise>
    </subsection>
    
    <!-- Activity: Create visualizations showing relationships in data -->
    <subsection xml:id="subsec-activity-relationship-viz">
      <title>Activity: Create visualizations showing relationships in data</title>
      
      <activity xml:id="activity-relationship-visualization">
        <title>Relationship Visualization</title>
        <statement>
          <p>In this activity, you will create visualizations that effectively show relationships between variables in a dataset. You will use the built-in iris dataset, which contains measurements of sepal length, sepal width, petal length, and petal width for three species of iris flowers.</p>
          
          <p>Your task is to create the following visualizations:</p>
          
          <ol>
            <li>
              <p><em>Scatter Plot with Enhancements</em>: Create a scatter plot showing the relationship between petal length and petal width, with the following enhancements:</p>
              <ul>
                <li><p>Color points by species</p></li>
                <li><p>Add a different shaped marker for each species</p></li>
               <li><p>Include a trend line for the overall relationship</p></li>
               <li><p>Add separate trend lines for each species</p></li>
               <li><p>Include appropriate titles, labels, and a legend</p></li>
               <li><p>Add text annotations highlighting interesting patterns</p></li>
             </ul>
           </li>
           <li>
             <p><em>Multiple Relationship Visualization</em>: Create a visualization that shows the relationships among all four measurement variables, using one of the following approaches:</p>
             <ul>
               <li><p>A scatterplot matrix with enhanced features</p></li>
               <li><p>A correlation matrix visualization</p></li>
               <li><p>A creative alternative that shows multiple relationships simultaneously</p></li>
             </ul>
           </li>
           <li>
             <p><em>Specialized Visualization</em>: Create one of the following specialized relationship visualizations:</p>
             <ul>
               <li><p>A bubble chart showing three variables</p></li>
               <li><p>A heatmap of the correlation matrix</p></li>
               <li><p>A line chart showing trends across the three species</p></li>
             </ul>
           </li>
         </ol>
         
         <p>For each visualization:</p>
         <ul>
           <li><p>Document your code with comments explaining key decisions</p></li>
           <li><p>Write a brief interpretation of the patterns or relationships you observe</p></li>
           <li><p>Explain how your visualization choices enhance understanding of the data</p></li>
         </ul>
       </statement>
     </activity>
     
     <program interactive="webr" xml:id="prog-relationship-activity">
       <input>
<![CDATA[
# Example solution for the relationship visualization activity
# Using the iris dataset

# Load data
data(iris)

# 1. Enhanced Scatter Plot
# ------------------------

# Create a scatter plot of petal dimensions
plot(iris$Petal.Length, iris$Petal.Width,
    main = "Relationship Between Petal Dimensions\nAcross Iris Species",
    xlab = "Petal Length (cm)",
    ylab = "Petal Width (cm)",
    type = "n")  # Empty plot first to set up the space

# Define colors and shapes for each species
species_colors <- c("setosa" = "#E41A1C", 
                   "versicolor" = "#377EB8", 
                   "virginica" = "#4DAF4A")
species_shapes <- c("setosa" = 16,        # Solid circle
                   "versicolor" = 17,     # Solid triangle
                   "virginica" = 15)      # Solid square

# Add points with different colors and shapes by species
for (species in levels(iris$Species)) {
 subset_data <- iris[iris$Species == species, ]
 points(subset_data$Petal.Length, subset_data$Petal.Width,
        col = species_colors[species],
        pch = species_shapes[species],
        cex = 1.2)
}

# Add an overall trend line
overall_model <- lm(Petal.Width ~ Petal.Length, data = iris)
abline(overall_model, col = "black", lwd = 2, lty = 2)

# Add trend lines for each species
for (species in levels(iris$Species)) {
 subset_data <- iris[iris$Species == species, ]
 if (nrow(subset_data) > 2) {  # Ensure enough data for a trend line
   species_model <- lm(Petal.Width ~ Petal.Length, data = subset_data)
   abline(species_model, col = species_colors[species], lwd = 2)
 }
}

# Add a legend
legend("topleft", 
      legend = levels(iris$Species),
      col = unname(species_colors),
      pch = unname(species_shapes),
      title = "Iris Species",
      bty = "n",
      cex = 0.9)

# Add annotations highlighting interesting patterns
text(1.5, 0.3, "Setosa forms\na distinct cluster", cex = 0.8)
text(4.5, 1.5, "Versicolor and Virginica\nshow overlap but\ndifferent slopes", cex = 0.8)
arrows(2.0, 0.3, 1.5, 0.2, length = 0.1, angle = 20)
arrows(4.0, 1.5, 4.7, 1.7, length = 0.1, angle = 20)

# Add R² value for the overall model
r_squared <- round(summary(overall_model)$r.squared, 2)
text(6.5, 0.5, paste("Overall R² =", r_squared), cex = 0.9)

# 2. Multiple Relationship Visualization
# -------------------------------------

# Create an enhanced scatterplot matrix
par(mar = c(4, 4, 3, 2))  # Adjust margins

# Define custom panels
panel.hist <- function(x, ...) {
 usr <- par("usr")
 on.exit(par(usr))
 par(usr = c(usr[1:2], 0, 1.5))
 h <- hist(x, plot = FALSE)
 breaks <- h$breaks
 nB <- length(breaks)
 y <- h$counts
 y <- y/max(y)
 rect(breaks[-nB], 0, breaks[-1], y, col = "lightblue", ...)
}

panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor = 0.8, ...) {
 usr <- par("usr"); on.exit(par(usr))
 par(usr = c(0, 1, 0, 1))
 r <- round(cor(x, y), digits)
 txt <- paste0(prefix, r)
 text(0.5, 0.5, txt, cex = cex.cor * abs(r) + 0.5)
}

panel.scatter <- function(x, y, ...) {
 points(x, y, 
        col = species_colors[iris$Species],
        pch = species_shapes[iris$Species],
        cex = 0.8)
}

# Create the enhanced scatterplot matrix
pairs(iris[, 1:4],
     main = "Relationships Among Iris Measurements",
     diag.panel = panel.hist,
     upper.panel = panel.cor,
     lower.panel = panel.scatter)

# Add a legend to the plot
par(xpd = TRUE)  # Allow plotting outside the figure region
legend(0.1, 0.05, 
      legend = levels(iris$Species),
      col = unname(species_colors),
      pch = unname(species_shapes),
      horiz = TRUE,
      bty = "n",
      cex = 0.8)
par(xpd = FALSE)  # Reset to default

# 3. Specialized Visualization - Bubble Chart
# -----------------------------------------

# Create a bubble chart with 3 variables:
# X-axis: Sepal.Length, Y-axis: Sepal.Width, Size: Petal.Length

# Set up the plot
plot(iris$Sepal.Length, iris$Sepal.Width,
    main = "Iris Measurements: Sepal Dimensions and Petal Length",
    xlab = "Sepal Length (cm)",
    ylab = "Sepal Width (cm)",
    type = "n",
    xlim = c(4, 8),
    ylim = c(1.5, 4.5))

# Add bubbles for each species
for (species in levels(iris$Species)) {
 subset_data <- iris[iris$Species == species, ]
 # Size proportional to petal length
 symbols(subset_data$Sepal.Length, 
         subset_data$Sepal.Width, 
         circles = sqrt(subset_data$Petal.Length)/2,
         inches = FALSE,
         bg = adjustcolor(species_colors[species], alpha.f = 0.7),
         fg = species_colors[species],
         add = TRUE)
}

# Add a legend for species
legend("topright", 
      legend = levels(iris$Species),
      pt.bg = adjustcolor(unname(species_colors), alpha.f = 0.7),
      pt.cex = 2,
      pch = 21,
      title = "Iris Species")

# Add a legend for bubble size
legend("bottomright",
      legend = c("Small Petal (1cm)", "Medium Petal (3cm)", "Large Petal (6cm)"),
      pt.cex = c(0.5, 1.0, 1.5),
      pch = 21,
      pt.bg = "gray80",
      title = "Petal Length")

# Add text annotations describing patterns
text(5.0, 4.3, "Setosa: Shorter petals, wider sepals", cex = 0.8)
text(7.0, 2.5, "Virginica: Longer petals and sepals", cex = 0.8)
text(6.5, 3.4, "Versicolor: Intermediate characteristics", cex = 0.8)

# Draw ellipses to highlight clusters
library(car)  # Note: This wouldn't work in Web-R but would work in a full R environment
# Commented out as it requires the 'car' package
# for (species in levels(iris$Species)) {
#   subset_data <- iris[iris$Species == species, ]
#   dataEllipse(subset_data$Sepal.Length, subset_data$Sepal.Width,
#              levels = 0.68, add = TRUE, plot.points = FALSE,
#              center.pch = NULL, col = species_colors[species], lwd = 2)
# }

# Add interpretation notes at the bottom
mtext("Note: This visualization shows that Iris species can be distinguished using sepal dimensions,", 
     side = 1, line = 4, cex = 0.8)
mtext("with petal length (bubble size) strongly correlated with species classification.", 
     side = 1, line = 5, cex = 0.8)

# Reset plotting parameters
par(mar = c(5, 4, 4, 2) + 0.1)  # Reset to default margins
]]>
       </input>
     </program>
     
     <exercise xml:id="ex-viz-interpretation">
       <title>Visualization Interpretation</title>
       <statement>
         <p>Based on the relationship visualizations you've created or observed, answer the following questions about the iris dataset:</p>
       </statement>
       <exercise>
         <statement>
           <p>What are the most significant patterns or relationships you can identify among the four measurement variables? Describe at least three specific observations.</p>
         </statement>
         <response/>
       </exercise>
       <exercise>
         <statement>
           <p>How effective are the different visualization types in revealing relationships in this dataset? Which visualization provided the most insight and why?</p>
         </statement>
         <response/>
       </exercise>
       <exercise>
         <statement>
           <p>If you were trying to develop a method to classify iris flowers by species based only on measurements, which variables would be most useful? Explain your reasoning based on the visualizations.</p>
         </statement>
         <response/>
       </exercise>
     </exercise>
   </subsection>
 </section>

 <!-- TOPIC 12: STATISTICAL ANALYSIS -->
 <section xml:id="sec-statistical-analysis">
   <title>Statistical Analysis</title>
   
   <!-- Session 23: Descriptive statistics in R -->
   <subsection xml:id="subsec-descriptive-stats">
     <title>Measures of center and spread</title>
     
     <p>Descriptive statistics provide a concise summary of the main characteristics of a dataset. They help us understand the central tendency, variability, and distribution of our data.</p>
     
     <paragraphs>
       <title>Measures of Central Tendency</title>
       <p>These statistics describe the "typical" or "central" values in a dataset:</p>
       <ul>
         <li>
           <p><term>Mean</term>: The arithmetic average of all values, calculated as the sum of values divided by the number of values. Useful for normally distributed data but sensitive to outliers.</p>
           <p>In R: <c>mean(x)</c></p>
         </li>
         <li>
           <p><term>Median</term>: The middle value when data is arranged in order. Half the values are above the median and half are below. Less sensitive to outliers than the mean.</p>
           <p>In R: <c>median(x)</c></p>
         </li>
         <li>
           <p><term>Mode</term>: The most frequently occurring value. Useful for categorical data but can also be applied to continuous data.</p>
           <p>In R (no built-in function): Calculate using frequency tables or density estimation</p>
         </li>
       </ul>
     </paragraphs>
     
     <paragraphs>
       <title>Measures of Spread (Dispersion)</title>
       <p>These statistics describe how values are distributed or how much they vary from the central tendency:</p>
       <ul>
         <li>
           <p><term>Range</term>: The difference between the maximum and minimum values. Simple but only uses two values, so sensitive to outliers.</p>
           <p>In R: <c>max(x) - min(x)</c> or <c>range(x)</c> (returns min and max)</p>
         </li>
         <li>
           <p><term>Variance</term>: The average of squared differences from the mean. Measures the spread but is in squared units of the original data.</p>
           <p>In R: <c>var(x)</c></p>
         </li>
         <li>
           <p><term>Standard Deviation</term>: The square root of the variance. Represents the typical distance from the mean in the original units.</p>
           <p>In R: <c>sd(x)</c></p>
         </li>
         <li>
           <p><term>Interquartile Range (IQR)</term>: The difference between the third quartile (75th percentile) and first quartile (25th percentile). Robust to outliers.</p>
           <p>In R: <c>IQR(x)</c></p>
         </li>
         <li>
           <p><term>Quantiles and Percentiles</term>: Values that divide the data into equal portions. Common quantiles include quartiles (dividing into four parts) and percentiles (dividing into 100 parts).</p>
           <p>In R: <c>quantile(x)</c> (default: 0%, 25%, 50%, 75%, 100%)</p>
         </li>
       </ul>
     </paragraphs>
     
     <program interactive="webr" xml:id="prog-descriptive-stats">
       <input>
<![CDATA[
# Descriptive statistics examples in R
# Load the built-in datasets
data(mtcars)
data(iris)

# Basic descriptive statistics for a single variable
# Miles per gallon from mtcars dataset
mpg <- mtcars$mpg

# Measures of central tendency
mean_mpg <- mean(mpg)
median_mpg <- median(mpg)
# Mode (R doesn't have a built-in function)
# One approach is to use the value with highest density
density_mpg <- density(mpg)
mode_mpg <- density_mpg$x[which.max(density_mpg$y)]

# Print results
cat("Mean MPG:", mean_mpg, "\n")
cat("Median MPG:", median_mpg, "\n")
cat("Approximate Mode MPG:", round(mode_mpg, 2), "\n")

# Measures of spread
range_mpg <- range(mpg)
var_mpg <- var(mpg)
sd_mpg <- sd(mpg)
iqr_mpg <- IQR(mpg)
quantiles_mpg <- quantile(mpg)
deciles_mpg <- quantile(mpg, probs = seq(0, 1, 0.1))

# Print results
cat("\nRange: [", range_mpg[1], ",", range_mpg[2], "] (width:", range_mpg[2] - range_mpg[1], ")\n")
cat("Variance:", var_mpg, "\n")
cat("Standard Deviation:", sd_mpg, "\n")
cat("Interquartile Range:", iqr_mpg, "\n")
cat("Quartiles (0%, 25%, 50%, 75%, 100%):\n")
print(quantiles_mpg)
cat("Deciles (0%, 10%, 20%, ..., 100%):\n")
print(deciles_mpg)

# The summary() function provides a quick overview
cat("\nSummary statistics:\n")
summary(mpg)

# Descriptive statistics by group
# Compare sepal length across iris species
cat("\nSepal Length by Species:\n")
aggregate(Sepal.Length ~ Species, data = iris, FUN = mean)
aggregate(Sepal.Length ~ Species, data = iris, FUN = median)
aggregate(Sepal.Length ~ Species, data = iris, FUN = sd)

# Multiple statistics at once
cat("\nMultiple statistics at once:\n")
aggregate(Sepal.Length ~ Species, data = iris, 
         FUN = function(x) c(mean = mean(x), 
                            median = median(x), 
                            sd = sd(x), 
                            n = length(x)))

# Comprehensive descriptive statistics for multiple variables
# First, create a function to calculate multiple statistics
descriptive_stats <- function(x) {
 # Check if the input is numeric
 if (!is.numeric(x)) {
   return(c(n = length(x), 
            n_missing = sum(is.na(x)), 
            n_unique = length(unique(x))))
 }
 
 # For numeric variables
 c(n = length(x),
   n_missing = sum(is.na(x)),
   mean = mean(x, na.rm = TRUE),
   median = median(x, na.rm = TRUE),
   min = min(x, na.rm = TRUE),
   max = max(x, na.rm = TRUE),
   range = max(x, na.rm = TRUE) - min(x, na.rm = TRUE),
   sd = sd(x, na.rm = TRUE),
   var = var(x, na.rm = TRUE),
   cv = sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE),
   skew = (mean(x, na.rm = TRUE) - median(x, na.rm = TRUE)) / sd(x, na.rm = TRUE),
   q1 = quantile(x, 0.25, na.rm = TRUE),
   q3 = quantile(x, 0.75, na.rm = TRUE),
   iqr = IQR(x, na.rm = TRUE))
}

# Apply to all numeric columns in mtcars
stats_mtcars <- sapply(mtcars, descriptive_stats)
# Transpose for better display
t(stats_mtcars[c("n", "mean", "median", "min", "max", "sd", "cv"), ])

# Apply to iris variables grouped by species
cat("\nSepal Length Statistics by Species:\n")
by(iris$Sepal.Length, iris$Species, descriptive_stats)

# Creating a custom summary function
custom_summary <- function(x) {
 # Five number summary plus mean and standard deviation
 c(min = min(x),
   q1 = quantile(x, 0.25),
   median = median(x),
   mean = mean(x),
   q3 = quantile(x, 0.75),
   max = max(x),
   sd = sd(x),
   n = length(x))
}

# Apply to all numeric variables in iris
sapply(iris[, 1:4], custom_summary)

# Visualizing descriptive statistics
# Boxplot shows the five-number summary visually
boxplot(Sepal.Length ~ Species, data = iris,
       main = "Sepal Length by Species",
       xlab = "Species",
       ylab = "Sepal Length (cm)",
       col = c("lightpink", "lightblue", "lightgreen"))

# Add the mean to the boxplot (shown as a point)
means <- aggregate(Sepal.Length ~ Species, data = iris, FUN = mean)
points(1:3, means$Sepal.Length, pch = 18, col = "red", cex = 1.5)

# A more comprehensive comparison of central tendency and spread
par(mfrow = c(1, 2))

# Boxplot for distribution and five-number summary
boxplot(mpg ~ factor(cyl), data = mtcars,
       main = "MPG by Cylinder Count",
       xlab = "Number of Cylinders",
       ylab = "Miles Per Gallon",
       col = c("lightgreen", "lightblue", "lightpink"))

# Bar plot for means with error bars for standard deviation
cyl_means <- aggregate(mpg ~ cyl, data = mtcars, FUN = mean)
cyl_sds <- aggregate(mpg ~ cyl, data = mtcars, FUN = sd)
cyl_counts <- aggregate(mpg ~ cyl, data = mtcars, FUN = length)

# Create bar plot
bar_positions <- barplot(cyl_means$mpg,
                        names.arg = cyl_means$cyl,
                        main = "Mean MPG by Cylinder Count",
                        xlab = "Number of Cylinders",
                        ylab = "Miles Per Gallon (Mean)",
                        col = c("lightgreen", "lightblue", "lightpink"),
                        ylim = c(0, max(cyl_means$mpg + cyl_sds$mpg) * 1.1))

# Add error bars (±1 standard deviation)
arrows(bar_positions, cyl_means$mpg - cyl_sds$mpg,
      bar_positions, cyl_means$mpg + cyl_sds$mpg,
      angle = 90, code = 3, length = 0.1)

# Add sample size
text(bar_positions, cyl_means$mpg + cyl_sds$mpg + 1,
    paste("n =", cyl_counts$mpg),
    cex = 0.8)

# Reset to default plotting parameters
par(mfrow = c(1, 1))
]]>
       </input>
     </program>
     
     <exercise xml:id="ex-descriptive-stats">
       <title>Descriptive Statistics Practice</title>
       <statement>
         <p>Calculate and interpret the following descriptive statistics for the Petal.Length variable in the iris dataset:</p>
       </statement>
       <exercise>
         <statement>
           <p>Calculate the mean, median, and standard deviation. What do these values tell you about the distribution of petal lengths?</p>
         </statement>
         <answer>
           <pre>
mean(iris$Petal.Length)   # Result: 3.758
median(iris$Petal.Length) # Result: 4.35
sd(iris$Petal.Length)     # Result: 1.7653
           </pre>
           <p>These statistics reveal key insights about the petal length distribution:</p>
           <ul>
             <li><p>The mean (3.758 cm) is notably lower than the median (4.35 cm), creating a negative skew (skewness ≈ -0.335)</p></li>
             <li><p>This indicates a distribution with more flowers having longer petals, but with some shorter-petaled flowers pulling the mean down</p></li>
             <li><p>The standard deviation (1.7653 cm) is relatively large compared to the mean, showing considerable variability in petal length</p></li>
             <li><p>The coefficient of variation (sd/mean ≈ 0.47) indicates high dispersion, suggesting the presence of distinct groups within the data</p></li>
           </ul>
           <p>These findings align with what we know about the iris dataset - it contains three species with Setosa having notably shorter petals than Versicolor and Virginica, creating a multimodal distribution rather than a normal one.</p>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>Calculate the petal length statistics (mean, median, and standard deviation) for each species separately. What patterns do you observe?</p>
         </statement>
         <answer>
           <pre>
aggregate(Petal.Length ~ Species, data = iris, 
         FUN = function(x) c(mean = mean(x), median = median(x), sd = sd(x)))

# Results:
# Species     mean median    sd
# setosa      1.462  1.50 0.1737
# versicolor  4.260  4.35 0.4699
# virginica   5.552  5.55 0.5519
           </pre>
           <p>These species-specific statistics reveal striking patterns:</p>
           <ul>
             <li><p><strong>Clear separation between species:</strong> Each species has distinctly different petal lengths with minimal overlap</p></li>
             <li><p><strong>Setosa:</strong> Very short petals (mean 1.462 cm) with minimal variation (sd 0.1737 cm), making it highly distinguishable</p></li>
             <li><p><strong>Versicolor:</strong> Intermediate petal length (mean 4.260 cm) with moderate variation (sd 0.4699 cm)</p></li>
             <li><p><strong>Virginica:</strong> Longest petals (mean 5.552 cm) with slightly greater variation (sd 0.5519 cm)</p></li>
             <li><p><strong>Within-species distributions appear normal:</strong> For each species, the mean and median are very close, suggesting symmetrical distributions within each group</p></li>
             <li><p><strong>Increasing variability with length:</strong> The standard deviation increases with increasing petal length across species</p></li>
           </ul>
           <p>This explains the negative skew in the overall distribution - it's actually a mixture of three separate, relatively normal distributions with the Setosa group far removed from the others.</p>
         </answer>
       </exercise>
     </exercise>
   </subsection>
   
   <!-- Distributions and outliers -->
   <subsection xml:id="subsec-distributions-outliers">
     <title>Distributions and outliers</title>
     
     <p>Understanding the distribution of your data and identifying outliers are crucial steps in statistical analysis. They help you choose appropriate analytical methods and identify potential data quality issues or interesting anomalies.</p>
     
     <paragraphs>
       <title>Common Distribution Types</title>
       <p>Several distribution patterns are frequently encountered in data analysis:</p>
       <ul>
         <li>
           <p><term>Normal Distribution</term>: Symmetric bell-shaped curve; mean, median, and mode are equal. Common in many natural phenomena and forms the basis for many statistical methods.</p>
         </li>
         <li>
           <p><term>Skewed Distributions</term>: Asymmetric with a longer tail on one side:
             <ul>
               <li><p>Right-skewed (positively skewed): Longer tail on the right, mean &gt; median</p></li>
               <li><p>Left-skewed (negatively skewed): Longer tail on the left, mean &lt; median</p></li>
             </ul>
           </p>
         </li>
         <li>
           <p><term>Bimodal/Multimodal Distributions</term>: Two or more peaks, suggesting multiple subgroups or processes in the data.</p>
         </li>
         <li>
           <p><term>Uniform Distribution</term>: Equal probability across all values within a range.</p>
         </li>
         <li>
           <p><term>Exponential Distribution</term>: Values decrease exponentially from a starting point; common in time-to-event data.</p>
         </li>
       </ul>
     </paragraphs>
     
     <paragraphs>
       <title>Assessing Distributions</title>
       <p>Several methods help analyze the distribution of your data:</p>
       <ul>
         <li>
           <p><term>Visual Methods</term>:
             <ul>
               <li><p>Histograms and density plots show the overall shape</p></li>
               <li><p>Q-Q plots compare the distribution to a theoretical normal distribution</p></li>
               <li><p>Box plots show the five-number summary and highlight potential outliers</p></li>
             </ul>
           </p>
         </li>
         <li>
           <p><term>Numerical Methods</term>:
             <ul>
               <li><p>Comparison of mean and median (for skewness)</p></li>
               <li><p>Skewness and kurtosis statistics</p></li>
               <li><p>Normality tests (e.g., Shapiro-Wilk test)</p></li>
             </ul>
           </p>
         </li>
       </ul>
     </paragraphs>
     
     <paragraphs>
       <title>Identifying and Handling Outliers</title>
       <p>Outliers are values that differ significantly from other observations and may influence statistical analyses.</p>
       <p>Methods for identifying outliers:</p>
       <ul>
         <li>
           <p><term>Visual inspection</term> using box plots, scatter plots, or histograms</p>
         </li>
         <li>
           <p><term>Z-scores</term>: Standardized values, with |z| > 3 often considered outliers for normally distributed data</p>
         </li>
         <li>
           <p><term>IQR method</term>: Values below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are potential outliers</p>
         </li>
         <li>
           <p><term>Modified Z-scores</term>: Based on median absolute deviation, more robust for skewed data</p>
         </li>
         <li>
           <p><term>Statistical tests</term> such as Grubbs' test or Dixon's Q test</p>
         </li>
       </ul>
       <p>Options for handling outliers:</p>
       <ul>
         <li>
           <p><term>Investigate</term> the cause of outliers (measurement error, data entry error, or a genuine extreme value)</p>
         </li>
         <li>
           <p><term>Retain</term> outliers if they represent valid observations</p>
         </li>
         <li>
           <p><term>Remove</term> outliers if they are errors or will severely impact analysis (with clear documentation)</p>
         </li>
         <li>
           <p><term>Transform</term> the data (e.g., log transformation) to reduce the impact of outliers</p>
         </li>
         <li>
           <p><term>Use robust statistical methods</term> that are less sensitive to outliers</p>
         </li>
         <li>
           <p><term>Winsorize</term> extreme values by replacing them with less extreme values (e.g., the 5th or 95th percentile)</p>
         </li>
       </ul>
     </paragraphs>
     
     <program interactive="webr" xml:id="prog-distributions-outliers">
       <input>
<![CDATA[
# Analyzing distributions and outliers in R
# Load necessary data
data(mtcars)
data(iris)

# Create some example distributions
# Normal distribution
set.seed(123)
normal_data <- rnorm(1000, mean = 50, sd = 10)

# Right-skewed distribution
right_skewed <- exp(rnorm(1000, mean = 1, sd = 0.5))

# Left-skewed distribution
left_skewed <- 100 - exp(rnorm(1000, mean = 1, sd = 0.5))

# Bimodal distribution
bimodal <- c(rnorm(500, mean = 40, sd = 5), rnorm(500, mean = 70, sd = 5))

# Uniform distribution
uniform <- runif(1000, min = 0, max = 100)

# Create a function to plot distributions
plot_distribution <- function(data, title) {
 # Set up a layout with 2 rows and 2 columns
 par(mfrow = c(2, 2))
 
 # Histogram
 hist(data, 
      main = paste("Histogram of", title),
      xlab = "Value",
      col = "lightblue",
      border = "white",
      breaks = 30)
 
 # Density plot
 plot(density(data), 
      main = paste("Density Plot of", title),
      xlab = "Value",
      ylab = "Density",
      col = "blue",
      lwd = 2)
 
 # Add vertical lines for mean and median
 abline(v = mean(data), col = "red", lwd = 2, lty = 1)
 abline(v = median(data), col = "green", lwd = 2, lty = 2)
 legend("topright", 
        legend = c("Density", "Mean", "Median"),
        col = c("blue", "red", "green"),
        lwd = 2,
        lty = c(1, 1, 2),
        bty = "n")
 
 # Boxplot
 boxplot(data, 
         main = paste("Boxplot of", title),
         ylab = "Value",
         col = "lightblue")
 
 # Q-Q plot (comparing to normal distribution)
 qqnorm(data, 
        main = paste("Q-Q Plot of", title))
 qqline(data, col = "red", lwd = 2)
 
 # Reset plotting parameters
 par(mfrow = c(1, 1))
 
 # Calculate and print summary statistics
 cat("\nSummary Statistics for", title, "Distribution:\n")
 cat("Mean:", mean(data), "\n")
 cat("Median:", median(data), "\n")
 cat("Standard Deviation:", sd(data), "\n")
 cat("Skewness:", (mean(data) - median(data)) / sd(data), "\n")
 cat("Range:", range(data), "\n")
 cat("IQR:", IQR(data), "\n")
 
 # Perform Shapiro-Wilk test for normality
 sw_test <- shapiro.test(data)
 cat("Shapiro-Wilk normality test p-value:", sw_test$p.value, "\n")
 cat("Interpretation: ", ifelse(sw_test$p.value < 0.05, 
                              "Reject normality", 
                              "Cannot reject normality"), "\n\n")
}

# Plot and analyze each distribution
plot_distribution(normal_data, "Normal")
plot_distribution(right_skewed, "Right-Skewed")
plot_distribution(left_skewed, "Left-Skewed")
plot_distribution(bimodal, "Bimodal")
plot_distribution(uniform, "Uniform")

# Compare distributions visually
par(mfrow = c(1, 1))
plot(density(normal_data), 
    main = "Comparison of Different Distributions",
    xlab = "Value", 
    ylab = "Density",
    col = "black",
    lwd = 2,
    xlim = c(0, 100),
    ylim = c(0, 0.08))
lines(density(right_skewed), col = "red", lwd = 2)
lines(density(left_skewed), col = "blue", lwd = 2)
lines(density(bimodal), col = "green", lwd = 2)
lines(density(uniform), col = "purple", lwd = 2)
legend("topright", 
      legend = c("Normal", "Right-Skewed", "Left-Skewed", "Bimodal", "Uniform"),
      col = c("black", "red", "blue", "green", "purple"),
      lwd = 2,
      bty = "n")

# Outlier Detection Techniques
# Create a dataset with outliers
set.seed(456)
data_with_outliers <- c(rnorm(95, mean = 50, sd = 10), 
                      c(5, 10, 95, 100, 120))  # Adding 5 outliers

# Method 1: Visualize with boxplot
boxplot(data_with_outliers, 
       main = "Boxplot for Outlier Detection",
       ylab = "Value",
       col = "lightblue")

# Method 2: Z-score method
z_scores <- (data_with_outliers - mean(data_with_outliers)) / sd(data_with_outliers)
outliers_z <- data_with_outliers[abs(z_scores) > 3]
cat("Outliers detected using Z-score method (|z| > 3):\n")
print(outliers_z)

# Method 3: IQR method
Q1 <- quantile(data_with_outliers, 0.25)
Q3 <- quantile(data_with_outliers, 0.75)
IQR_value <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value
outliers_iqr <- data_with_outliers[data_with_outliers < lower_bound | 
                                 data_with_outliers > upper_bound]
cat("\nOutliers detected using IQR method (outside Q1-1.5*IQR to Q3+1.5*IQR):\n")
print(outliers_iqr)

# Visualize the outlier detection methods
hist(data_with_outliers, 
    main = "Histogram with Outlier Boundaries",
    xlab = "Value",
    col = "lightblue",
    breaks = 30)

# Add vertical lines for Z-score boundaries
z_lower <- mean(data_with_outliers) - 3 * sd(data_with_outliers)
z_upper <- mean(data_with_outliers) + 3 * sd(data_with_outliers)
abline(v = z_lower, col = "red", lwd = 2, lty = 2)
abline(v = z_upper, col = "red", lwd = 2, lty = 2)

# Add vertical lines for IQR boundaries
abline(v = lower_bound, col = "blue", lwd = 2, lty = 3)
abline(v = upper_bound, col = "blue", lwd = 2, lty = 3)

# Add a legend
legend("topright", 
      legend = c("Z-score boundaries (±3 SD)", "IQR boundaries (1.5 IQR)"),
      col = c("red", "blue"),
      lwd = 2,
      lty = c(2, 3),
      bty = "n")

# Compare the effect of outliers on mean and median
cat("\nEffect of outliers on central tendency measures:\n")
cat("Original data with outliers:\n")
cat("  Mean:", mean(data_with_outliers), "\n")
cat("  Median:", median(data_with_outliers), "\n")

# Remove outliers and recalculate
data_no_outliers <- data_with_outliers[!(data_with_outliers %in% outliers_iqr)]
cat("After removing outliers (IQR method):\n")
cat("  Mean:", mean(data_no_outliers), "\n")
cat("  Median:", median(data_no_outliers), "\n")
cat("  Difference in mean:", mean(data_with_outliers) - mean(data_no_outliers), "\n")
cat("  Difference in median:", median(data_with_outliers) - median(data_no_outliers), "\n")

# Demonstrate Winsorization (replacing outliers with percentile values)
# 5th percentile and 95th percentile
p05 <- quantile(data_with_outliers, 0.05)
p95 <- quantile(data_with_outliers, 0.95)

# Winsorize the data
data_winsorized <- data_with_outliers
data_winsorized[data_winsorized < p05] <- p05
data_winsorized[data_winsorized > p95] <- p95

cat("\nAfter Winsorizing (5th to 95th percentile):\n")
cat("  Mean:", mean(data_winsorized), "\n")
cat("  Median:", median(data_winsorized), "\n")

# Compare the different handling methods visually
par(mfrow = c(3, 1))

# Original data with outliers
boxplot(data_with_outliers, 
       main = "Original Data with Outliers",
       ylab = "Value",
       col = "lightcoral")

# Data with outliers removed
boxplot(data_no_outliers, 
       main = "Data with Outliers Removed",
       ylab = "Value",
       col = "lightblue")

# Winsorized data
boxplot(data_winsorized, 
       main = "Winsorized Data (5th to 95th percentile)",
       ylab = "Value",
       col = "lightgreen")

# Reset plotting parameters
par(mfrow = c(1, 1))

# Real-world example: Detecting outliers in mtcars data
# Let's analyze MPG
mpg <- mtcars$mpg

# Create a boxplot
boxplot(mpg ~ factor(mtcars$cyl), 
       main = "MPG by Cylinder Count",
       xlab = "Number of Cylinders",
       ylab = "Miles Per Gallon",
       col = c("lightgreen", "lightblue", "lightpink"))

# Detect outliers by group using IQR method
detect_outliers_by_group <- function(data, group_var, var_name) {
 # Split the data by group
 groups <- split(data[[var_name]], data[[group_var]])
 
 # Detect outliers in each group
 outliers_by_group <- list()
 
 for (group_name in names(groups)) {
   values <- groups[[group_name]]
   Q1 <- quantile(values, 0.25)
   Q3 <- quantile(values, 0.75)
   IQR_value <- Q3 - Q1
   lower_bound <- Q1 - 1.5 * IQR_value
   upper_bound <- Q3 + 1.5 * IQR_value
   
   # Find outliers
   outlier_values <- values[values < lower_bound | values > upper_bound]
   
   if (length(outlier_values) > 0) {
     # Find the row indices of outliers
     outlier_indices <- which(data[[group_var]] == group_name & 
                              (data[[var_name]] < lower_bound | 
                               data[[var_name]] > upper_bound))
     
     outliers_by_group[[group_name]] <- list(
       indices = outlier_indices,
       values = outlier_values,
       bounds = c(lower = lower_bound, upper = upper_bound)
     )
   }
 }
 
 return(outliers_by_group)
}

# Detect outliers in MPG by cylinder group
mpg_outliers <- detect_outliers_by_group(mtcars, "cyl", "mpg")

# Print the results
cat("\nOutliers in MPG by Cylinder Group:\n")
for (cyl in names(mpg_outliers)) {
 if (length(mpg_outliers[[cyl]]$values) > 0) {
   cat("Cylinder group", cyl, ":\n")
   cat("  Number of outliers:", length(mpg_outliers[[cyl]]$values), "\n")
   cat("  Outlier values:", mpg_outliers[[cyl]]$values, "\n")
   cat("  Bounds: [", mpg_outliers[[cyl]]$bounds["lower"], ",", 
       mpg_outliers[[cyl]]$bounds["upper"], "]\n")
   cat("  Car models:", rownames(mtcars)[mpg_outliers[[cyl]]$indices], "\n\n")
 } else {
   cat("Cylinder group", cyl, ": No outliers detected\n")
 }
}
]]>
       </input>
     </program>
     
     <exercise xml:id="ex-distribution-analysis">
       <title>Distribution Analysis</title>
       <statement>
         <p>Analyze the distribution of the petal width variable in the iris dataset, addressing the following questions:</p>
       </statement>
       <exercise>
         <statement>
           <p>Based on visual inspection and descriptive statistics, what type of distribution does petal width follow? Explain your reasoning.</p>
         </statement>
         <answer>
           <p>The petal width distribution in the iris dataset shows clear characteristics of a bimodal (or potentially trimodal) distribution:</p>
           <ul>
             <li><p><strong>Visual inspection</strong> of a histogram or density plot reveals distinct peaks rather than a single bell curve</p></li>
             <li><p><strong>Descriptive statistics</strong> show that while the overall mean is approximately 1.20 cm and the median is 1.30 cm, these central tendency measures are not representative of the actual data distribution</p></li>
             <li><p>The distribution fails normality tests (e.g., Shapiro-Wilk) with p-values well below 0.05</p></li>
             <li><p>The relatively large standard deviation (approximately 0.76 cm) compared to the mean indicates high variability</p></li>
             <li><p>There's a notable gap in the frequency distribution around 0.6-1.0 cm</p></li>
           </ul>
           <p>This multimodal pattern reflects the dataset's composition of three distinct iris species with different petal width characteristics:
             <ul>
               <li><p>Setosa: narrow petals, clustering around 0.2-0.3 cm</p></li>
               <li><p>Versicolor: medium petals, clustering around 1.2-1.3 cm</p></li>
               <li><p>Virginica: wider petals, clustering around 1.8-2.0 cm</p></li>
             </ul>
           </p>
           <p>This is a classic example of how combining distinct subpopulations creates a multimodal distribution, making overall measures of central tendency potentially misleading without considering the subgroup structure.</p>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>Are there any outliers in the petal width measurements? If so, identify them and discuss whether they should be removed or retained for analysis.</p>
         </statement>
         <answer>
           <p>When analyzing the iris dataset's petal width for outliers:</p>
           <p><strong>Initial analysis across all species:</strong></p>
           <ul>
             <li><p>Using the IQR method (Q1 - 1.5*IQR to Q3 + 1.5*IQR) on the full dataset might identify some values as statistical outliers</p></li>
             <li><p>However, this approach is problematic because we're dealing with a multimodal distribution representing three distinct species</p></li>
           </ul>
           
           <p><strong>Species-specific analysis:</strong></p>
           <ul>
             <li><p>When examining each species separately, very few or no outliers are detected</p></li>
             <li><p>For Setosa: The petal widths are tightly clustered around 0.2-0.3 cm with minimal variation</p></li>
             <li><p>For Versicolor: Measurements are generally between 1.0-1.5 cm with no extreme values</p></li>
             <li><p>For Virginica: Measurements typically range from 1.4-2.3 cm, possibly with 1-2 mild outliers</p></li>
           </ul>
           
           <p><strong>Recommendation:</strong></p>
           <ul>
             <li><p>Any outliers identified should be retained for analysis because:</p>
               <ul>
                 <li><p>They appear to be valid biological measurements, not errors</p></li>
                 <li><p>They represent natural variation within species</p></li>
                 <li><p>The sample size per species is relatively small (50 each), making each observation valuable</p></li>
                 <li><p>These measurements are important for understanding the full range of morphological variation</p></li>
               </ul>
             </li>
           </ul>
           
           <p>The key insight is that proper outlier detection requires considering the underlying structure of the data. In this case, analyzing by species provides a more appropriate context for identifying true outliers versus natural group differences.</p>
         </answer>
       </exercise>
     </exercise>
   </subsection>
   
   <!-- Exercise: Statistical summary of datasets -->
   <subsection xml:id="subsec-exercise-stats-summary">
     <title>Exercise: Statistical summary of datasets</title>
     
     <activity xml:id="activity-stat-summary">
       <title>Comprehensive Statistical Analysis</title>
       <statement>
         <p>In this activity, you will perform a comprehensive statistical analysis of the built-in mtcars dataset, focusing on fuel efficiency (mpg) and its relationship with other car attributes.</p>
         
         <p>Your task is to:</p>
         
         <ol>
           <li>
             <p>Perform a descriptive statistical analysis of the mpg variable:</p>
             <ul>
               <li><p>Calculate measures of central tendency (mean, median, mode)</p></li>
               <li><p>Calculate measures of dispersion (range, variance, standard deviation, IQR)</p></li>
               <li><p>Determine the distribution shape (visually and using skewness/kurtosis)</p></li>
               <li><p>Identify any potential outliers</p></li>
             </ul>
           </li>
           <li>
             <p>Analyze how mpg varies across different categorical variables:</p>
             <ul>
               <li><p>Number of cylinders (cyl)</p></li>
               <li><p>Transmission type (am: 0 = automatic, 1 = manual)</p></li>
               <li><p>V/S (vs: 0 = V-shaped, 1 = straight engine)</p></li>
             </ul>
           </li>
           <li>
             <p>Examine the relationship between mpg and other continuous variables:</p>
             <ul>
               <li><p>Weight (wt)</p></li>
               <li><p>Horsepower (hp)</p></li>
               <li><p>Displacement (disp)</p></li>
             </ul>
           </li>
           <li>
             <p>Create appropriate visualizations to support your analysis</p>
           </li>
           <li>
             <p>Write a brief summary of your findings, including the most significant factors influencing fuel efficiency</p>
           </li>
         </ol>
         
         <p>Document your code with comments and explain your analytical choices.</p>
       </statement>
     </activity>
     
     <program interactive="webr" xml:id="prog-stat-summary">
       <input>
<![CDATA[
# Comprehensive Statistical Analysis of the mtcars dataset
# Focus on fuel efficiency (mpg)

# Load the data
data(mtcars)

# Add row names as a column for easier filtering
mtcars$car_name <- rownames(mtcars)

# 1. Descriptive statistical analysis of mpg
# -----------------------------------------

# Basic summary statistics
mpg_summary <- summary(mtcars$mpg)
print(mpg_summary)

# Calculate additional statistics
mpg_stats <- c(
 n = length(mtcars$mpg),
 mean = mean(mtcars$mpg),
 median = median(mtcars$mpg),
 sd = sd(mtcars$mpg),
 var = var(mtcars$mpg),
 min = min(mtcars$mpg),
 max = max(mtcars$mpg),
 range = max(mtcars$mpg) - min(mtcars$mpg),
 IQR = IQR(mtcars$mpg),
 q1 = quantile(mtcars$mpg, 0.25),
 q3 = quantile(mtcars$mpg, 0.75)
)

# Calculate approximate skewness
skewness_approx <- (mean(mtcars$mpg) - median(mtcars$mpg)) / sd(mtcars$mpg)
mpg_stats["skewness_approx"] <- skewness_approx

# Print the statistics
print(mpg_stats)

# Visualize the distribution of mpg
par(mfrow = c(2, 2))

# Histogram
hist(mtcars$mpg, 
    main = "Distribution of Fuel Efficiency",
    xlab = "Miles Per Gallon",
    col = "lightblue",
    breaks = 10)

# Density plot
plot(density(mtcars$mpg), 
    main = "Density Plot of MPG",
    xlab = "Miles Per Gallon",
    col = "blue",
    lwd = 2)
abline(v = mean(mtcars$mpg), col = "red", lwd = 2, lty = 2)
abline(v = median(mtcars$mpg), col = "green", lwd = 2, lty = 3)
legend("topright", 
      legend = c("Density", "Mean", "Median"),
      col = c("blue", "red", "green"),
      lwd = 2,
      lty = c(1, 2, 3),
      bty = "n")

# Boxplot
boxplot(mtcars$mpg, 
       main = "Boxplot of MPG",
       ylab = "Miles Per Gallon",
       col = "lightblue")

# Q-Q plot
qqnorm(mtcars$mpg, main = "Q-Q Plot of MPG")
qqline(mtcars$mpg, col = "red")

# Reset plot layout
par(mfrow = c(1, 1))

# Check for outliers using the IQR method
q1 <- quantile(mtcars$mpg, 0.25)
q3 <- quantile(mtcars$mpg, 0.75)
iqr <- q3 - q1
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr

# Identify outliers
mpg_outliers <- mtcars[mtcars$mpg < lower_bound | mtcars$mpg > upper_bound, ]
if(nrow(mpg_outliers) > 0) {
 cat("\nPotential MPG outliers:\n")
 print(mpg_outliers[, c("car_name", "mpg")])
} else {
 cat("\nNo MPG outliers detected using the IQR method.\n")
}

# 2. Analyze how mpg varies across different categorical variables
# --------------------------------------------------------------

# Convert factors for better analysis and plotting
mtcars$cyl_f <- as.factor(mtcars$cyl)
mtcars$am_f <- factor(mtcars$am, labels = c("Automatic", "Manual"))
mtcars$vs_f <- factor(mtcars$vs, labels = c("V-shaped", "Straight"))

# Function to calculate statistics by group
group_stats <- function(data, group_var, value_var) {
 stats <- aggregate(data[[value_var]] ~ data[[group_var]], 
                   FUN = function(x) c(n = length(x),
                                     mean = mean(x),
                                     median = median(x),
                                     sd = sd(x),
                                     min = min(x),
                                     max = max(x)))
 return(stats)
}

# MPG by number of cylinders
mpg_by_cyl <- group_stats(mtcars, "cyl_f", "mpg")
colnames(mpg_by_cyl) <- c("Cylinders", "n", "Mean", "Median", "SD", "Min", "Max")
cat("\nMPG statistics by number of cylinders:\n")
print(mpg_by_cyl)

# MPG by transmission type
mpg_by_am <- group_stats(mtcars, "am_f", "mpg")
colnames(mpg_by_am) <- c("Transmission", "n", "Mean", "Median", "SD", "Min", "Max")
cat("\nMPG statistics by transmission type:\n")
print(mpg_by_am)

# MPG by engine shape
mpg_by_vs <- group_stats(mtcars, "vs_f", "mpg")
colnames(mpg_by_vs) <- c("Engine Shape", "n", "Mean", "Median", "SD", "Min", "Max")
cat("\nMPG statistics by engine shape:\n")
print(mpg_by_vs)

# Visualize MPG by categorical variables
par(mfrow = c(1, 3))

# MPG by cylinder count
boxplot(mpg ~ cyl_f, data = mtcars,
       main = "MPG by Number of Cylinders",
       xlab = "Number of Cylinders",
       ylab = "Miles Per Gallon",
       col = c("lightgreen", "lightblue", "lightpink"))

# MPG by transmission type
boxplot(mpg ~ am_f, data = mtcars,
       main = "MPG by Transmission Type",
       xlab = "Transmission",
       ylab = "Miles Per Gallon",
       col = c("lightblue", "lightgreen"))

# MPG by engine shape
boxplot(mpg ~ vs_f, data = mtcars,
       main = "MPG by Engine Shape",
       xlab = "Engine Shape",
       ylab = "Miles Per Gallon",
       col = c("lightpink", "lightblue"))

# Reset plot layout
par(mfrow = c(1, 1))

# 3. Examine the relationship between mpg and continuous variables
# --------------------------------------------------------------

# Calculate correlations
cor_vars <- c("mpg", "wt", "hp", "disp")
cor_matrix <- cor(mtcars[, cor_vars])
cat("\nCorrelation matrix:\n")
print(round(cor_matrix, 3))

# Visualize relationships with scatter plots
par(mfrow = c(2, 2))

# MPG vs. Weight
plot(mtcars$wt, mtcars$mpg,
    main = "MPG vs. Weight",
    xlab = "Weight (1000 lbs)",
    ylab = "Miles Per Gallon",
    pch = 19,
    col = "darkblue")
# Add regression line
abline(lm(mpg ~ wt, data = mtcars), col = "red", lwd = 2)
# Add correlation coefficient
text(4, 30, paste("r =", round(cor(mtcars$mpg, mtcars$wt), 2)), pos = 4)

# MPG vs. Horsepower
plot(mtcars$hp, mtcars$mpg,
    main = "MPG vs. Horsepower",
    xlab = "Horsepower",
    ylab = "Miles Per Gallon",
    pch = 19,
    col = "darkgreen")
# Add regression line
abline(lm(mpg ~ hp, data = mtcars), col = "red", lwd = 2)
# Add correlation coefficient
text(250, 30, paste("r =", round(cor(mtcars$mpg, mtcars$hp), 2)), pos = 4)

# MPG vs. Displacement
plot(mtcars$disp, mtcars$mpg,
    main = "MPG vs. Displacement",
    xlab = "Displacement (cu. in.)",
    ylab = "Miles Per Gallon",
    pch = 19,
    col = "darkred")
# Add regression line
abline(lm(mpg ~ disp, data = mtcars), col = "blue", lwd = 2)
# Add correlation coefficient
text(350, 30, paste("r =", round(cor(mtcars$mpg, mtcars$disp), 2)), pos = 4)

# MPG vs. all three variables (bubble chart)
# Weight on x-axis, MPG on y-axis, bubble size proportional to horsepower, color by cylinder
plot(mtcars$wt, mtcars$mpg, 
    type = "n",  # Empty plot
    main = "MPG vs. Weight, Horsepower, and Cylinders",
    xlab = "Weight (1000 lbs)",
    ylab = "Miles Per Gallon")

# Define colors for cylinders
cyl_colors <- c("4" = "green3", "6" = "blue3", "8" = "red3")

# Add bubbles
symbols(mtcars$wt, mtcars$mpg, 
       circles = sqrt(mtcars$hp)/10,  # Size proportional to horsepower
       inches = FALSE,
       bg = cyl_colors[as.character(mtcars$cyl)],
       fg = "black",
       add = TRUE)

# Add a legend
legend("topright", 
      legend = c("4 cylinders", "6 cylinders", "8 cylinders"),
      pt.bg = cyl_colors,
      pt.cex = 2,
      pch = 21,
      title = "Cylinder Count")

# Reset plot layout
par(mfrow = c(1, 1))

# 5. Summary of findings
# --------------------
cat("\n========================================================\n")
cat("SUMMARY OF FINDINGS: FACTORS INFLUENCING FUEL EFFICIENCY\n")
cat("========================================================\n\n")

cat("1. Overall MPG Distribution:\n")
cat("   - Average fuel efficiency: ", round(mean(mtcars$mpg), 1), " MPG\n")
cat("   - Range: ", min(mtcars$mpg), " to ", max(mtcars$mpg), " MPG\n")
cat("   - Distribution appears slightly right-skewed (more cars with lower MPG)\n\n")

cat("2. Key Categorical Factors:\n")
cat("   - Number of cylinders has a strong impact on MPG:\n")
cat("     * 4-cylinder cars average ", round(mpg_by_cyl[1, "Mean"], 1), " MPG\n")
cat("     * 6-cylinder cars average ", round(mpg_by_cyl[2, "Mean"], 1), " MPG\n")
cat("     * 8-cylinder cars average ", round(mpg_by_cyl[3, "Mean"], 1), " MPG\n\n")

cat("   - Transmission type shows a significant difference:\n")
cat("     * Manual transmission cars average ", round(mpg_by_am[2, "Mean"], 1), " MPG\n")
cat("     * Automatic transmission cars average ", round(mpg_by_am[1, "Mean"], 1), " MPG\n\n")

cat("   - Engine shape also correlates with efficiency:\n")
cat("     * Straight engines average ", round(mpg_by_vs[2, "Mean"], 1), " MPG\n")
cat("     * V-shaped engines average ", round(mpg_by_vs[1, "Mean"], 1), " MPG\n\n")

cat("3. Key Continuous Factors:\n")
cat("   - Weight has the strongest negative correlation with MPG (r = ", 
   round(cor(mtcars$mpg, mtcars$wt), 2), ")\n")
cat("   - Displacement also strongly negatively correlates with MPG (r = ", 
   round(cor(mtcars$mpg, mtcars$disp), 2), ")\n")
cat("   - Horsepower shows a significant negative correlation with MPG (r = ", 
   round(cor(mtcars$mpg, mtcars$hp), 2), ")\n\n")

cat("4. Multivariate Relationships:\n")
cat("   - The bubble chart reveals that cars with:\n")
cat("     * Lower weight, lower horsepower, and fewer cylinders tend to have the highest MPG\n")
cat("     * Higher weight, higher horsepower, and more cylinders tend to have the lowest MPG\n\n")

cat("5. Conclusions:\n")
cat("   - Weight appears to be the most influential factor affecting fuel efficiency\n")
cat("   - Engine characteristics (cylinders, displacement, shape) also play significant roles\n")
cat("   - Manual transmission cars tend to be more fuel-efficient than automatics\n")
cat("   - The data suggests trade-offs between performance (horsepower) and efficiency (MPG)\n")
]]>
       </input>
     </program>
     
     <exercise xml:id="ex-statistical-interpretation">
       <title>Statistical Interpretation</title>
       <statement>
         <p>Based on the comprehensive statistical analysis of the mtcars dataset, answer the following questions:</p>
       </statement>
       <exercise>
         <statement>
           <p>What are the three most influential factors affecting fuel efficiency (mpg) in this dataset? Provide specific statistical evidence to support your answer.</p>
         </statement>
         <response/>
       </exercise>
       <exercise>
         <statement>
           <p>Is manual transmission associated with better fuel efficiency than automatic transmission? Analyze the data to provide a nuanced answer that considers potential confounding factors.</p>
         </statement>
         <response/>
       </exercise>
       <exercise>
         <statement>
           <p>Based on the statistical analysis, what characteristics would you expect a car with high fuel efficiency (>25 mpg) to have? Support your answer with specific data from the analysis.</p>
         </statement>
         <response/>
       </exercise>
     </exercise>
   </subsection>
 </section>

 <!-- Session 24: Introduction to statistical testing -->
 <section xml:id="sec-statistical-testing">
   <title>Introduction to statistical testing</title>
   
   <!-- Confidence intervals -->
   <subsection xml:id="subsec-confidence-intervals">
     <title>Confidence intervals</title>
     
     <p>Confidence intervals provide a range of plausible values for a population parameter based on sample data. They help quantify the uncertainty in statistical estimates and are a fundamental tool in inferential statistics.</p>
     
     <paragraphs>
       <title>Basic Concept</title>
       <p>A confidence interval consists of:</p>
       <ul>
         <li>
           <p>A point estimate (typically a sample statistic like the mean)</p>
         </li>
         <li>
           <p>A margin of error (which depends on the desired confidence level and the sample's variability)</p>
         </li>
       </ul>
       <p>The general form is: Point Estimate ± Margin of Error</p>
     </paragraphs>
     
     <paragraphs>
       <title>Confidence Level</title>
       <p>The confidence level (commonly 95%, but also 90%, 99%, etc.) indicates the reliability of the interval:</p>
       <ul>
         <li>
           <p>A 95% confidence interval means that if we were to take many samples and construct a confidence interval from each sample, about 95% of these intervals would contain the true population parameter.</p>
         </li>
         <li>
           <p>Higher confidence levels result in wider intervals (more certainty but less precision).</p>
         </li>
         <li>
           <p>Lower confidence levels result in narrower intervals (less certainty but more precision).</p>
         </li>
       </ul>
     </paragraphs>
     
     <paragraphs>
       <title>Calculating Confidence Intervals</title>
       <p>For a population mean with a large sample or known population standard deviation:</p>
       <p>CI = x̄ ± z × (σ/√n)</p>
       <p>where:</p>
       <ul>
         <li>
           <p>x̄ is the sample mean</p>
         </li>
         <li>
           <p>z is the critical value from the standard normal distribution (e.g., 1.96 for 95% confidence)</p>
         </li>
         <li>
           <p>σ is the population standard deviation (or sample standard deviation s for large samples)</p>
         </li>
         <li>
           <p>n is the sample size</p>
         </li>
       </ul>
       
       <p>For a population mean with a small sample and unknown population standard deviation:</p>
       <p>CI = x̄ ± t × (s/√n)</p>
       <p>where t is the critical value from the t-distribution with n-1 degrees of freedom.</p>
     </paragraphs>
     
     <paragraphs>
       <title>Interpreting Confidence Intervals</title>
       <p>Common interpretations include:</p>
       <ul>
         <li>
           <p>We are [confidence level]% confident that the true population parameter falls within this interval.</p>
         </li>
         <li>
           <p>If we were to repeat this sampling process many times, [confidence level]% of the resulting intervals would contain the true parameter.</p>
         </li>
       </ul>
       <p>Note: The confidence interval does NOT indicate the probability that the parameter falls within the interval, as the parameter is fixed (not random).</p>
     </paragraphs>
     
     <program interactive="webr" xml:id="prog-confidence-intervals">
       <input>
<![CDATA[
# Confidence Interval Examples in R
# Set a seed for reproducibility
set.seed(123)

# Generate a sample from a normal distribution
population_mean <- 100
population_sd <- 15
sample_size <- 30

sample_data <- rnorm(sample_size, mean = population_mean, sd = population_sd)

# Calculate sample statistics
sample_mean <- mean(sample_data)
sample_sd <- sd(sample_data)
standard_error <- sample_sd / sqrt(sample_size)

# Print sample statistics
cat("Sample size:", sample_size, "\n")
cat("Sample mean:", round(sample_mean, 2), "\n")
cat("Sample standard deviation:", round(sample_sd, 2), "\n")
cat("Standard error of the mean:", round(standard_error, 2), "\n")

# Calculate 95% confidence interval using t-distribution
# (appropriate for small samples with unknown population standard deviation)
alpha <- 0.05  # For a 95% confidence level
degrees_of_freedom <- sample_size - 1
t_critical <- qt(1 - alpha/2, df = degrees_of_freedom)

margin_of_error <- t_critical * standard_error
lower_bound <- sample_mean - margin_of_error
upper_bound <- sample_mean + margin_of_error

# Print 95% confidence interval
cat("\n95% Confidence Interval for the Mean:\n")
cat("t-critical value:", round(t_critical, 4), "\n")
cat("Margin of error:", round(margin_of_error, 2), "\n")
cat("Lower bound:", round(lower_bound, 2), "\n")
cat("Upper bound:", round(upper_bound, 2), "\n")
cat("Interval: [", round(lower_bound, 2), ", ", round(upper_bound, 2), "]\n")

# Check if the true population mean falls within our confidence interval
if (population_mean >= lower_bound && population_mean <= upper_bound) {
 cat("The true population mean (", population_mean, ") falls within the confidence interval.\n")
} else {
 cat("The true population mean (", population_mean, ") does not fall within the confidence interval.\n")
}

# Using built-in R function t.test() to calculate confidence interval
t_test_result <- t.test(sample_data, conf.level = 0.95)
ci_from_t_test <- t_test_result$conf.int

cat("\nConfidence Interval using t.test() function:\n")
cat("Lower bound:", round(ci_from_t_test[1], 2), "\n")
cat("Upper bound:", round(ci_from_t_test[2], 2), "\n")

# Calculate confidence intervals for different confidence levels
confidence_levels <- c(0.90, 0.95, 0.99)

cat("\nComparing Confidence Intervals for Different Confidence Levels:\n")
for (conf_level in confidence_levels) {
 t_critical <- qt(1 - (1 - conf_level)/2, df = degrees_of_freedom)
 margin_of_error <- t_critical * standard_error
 lower_bound <- sample_mean - margin_of_error
 upper_bound <- sample_mean + margin_of_error
 
 cat(conf_level * 100, "% CI: [", round(lower_bound, 2), ", ", 
     round(upper_bound, 2), "] (Width: ", round(upper_bound - lower_bound, 2), ")\n")
}

# Visualize confidence intervals for different confidence levels
# Create a simple plot
plot(1, sample_mean, 
    xlim = c(0.5, 1.5), 
    ylim = c(sample_mean - 4*standard_error, sample_mean + 4*standard_error),
    xlab = "", 
    ylab = "Sample Mean", 
    main = "Confidence Intervals for Different Confidence Levels",
    xaxt = "n",  # No x-axis
    pch = 19)

# Add a horizontal line for the true population mean
abline(h = population_mean, col = "red", lty = 2)

# Add confidence intervals
colors <- c("blue", "green", "purple")
i <- 1

for (conf_level in confidence_levels) {
 t_critical <- qt(1 - (1 - conf_level)/2, df = degrees_of_freedom)
 margin_of_error <- t_critical * standard_error
 lower_bound <- sample_mean - margin_of_error
 upper_bound <- sample_mean + margin_of_error
 
 segments(1, lower_bound, 1, upper_bound, col = colors[i], lwd = 3)
 i <- i + 1
}

# Add a legend
legend("topright", 
      legend = paste0(confidence_levels * 100, "% CI"),
      col = colors,
      lwd = 3,
      bty = "n")

# Add a note about the true population mean
text(0.7, population_mean, "True Population Mean", col = "red", pos = 4)

# Effect of sample size on confidence interval width
# We'll calculate 95% confidence intervals for different sample sizes
sample_sizes <- c(10, 30, 100, 300, 1000)
ci_widths <- numeric(length(sample_sizes))

cat("\nEffect of Sample Size on 95% Confidence Interval Width:\n")
for (i in 1:length(sample_sizes)) {
 n <- sample_sizes[i]
 # Generate a larger sample than we need, then take subsets of different sizes
 if (i == 1) {
   large_sample <- rnorm(1000, mean = population_mean, sd = population_sd)
 }
 
 # Take a subset of the required size
 current_sample <- large_sample[1:n]
 
 # Calculate sample statistics
 current_mean <- mean(current_sample)
 current_sd <- sd(current_sample)
 current_se <- current_sd / sqrt(n)
 
 # Calculate confidence interval
 current_t <- qt(0.975, df = n - 1)
 current_margin <- current_t * current_se
 current_lower <- current_mean - current_margin
 current_upper <- current_mean + current_margin
 
 # Store the width
 ci_widths[i] <- current_upper - current_lower
 
 cat("Sample size ", n, ": CI = [", round(current_lower, 2), ", ", 
     round(current_upper, 2), "] (Width: ", round(ci_widths[i], 2), ")\n")
}

# Plot the relationship between sample size and CI width
plot(sample_sizes, ci_widths, 
    type = "b", 
    log = "x",  # Use log scale for x-axis
    xlab = "Sample Size (log scale)", 
    ylab = "Confidence Interval Width",
    main = "Effect of Sample Size on CI Width",
    col = "blue",
    pch = 19,
    lwd = 2)

# Add a reference line showing the 1/sqrt(n) relationship
# Scale it to match the first point for visual comparison
scaling_factor <- ci_widths[1] * sqrt(sample_sizes[1])
theoretical_widths <- scaling_factor / sqrt(sample_sizes)
lines(sample_sizes, theoretical_widths, col = "red", lty = 2, lwd = 2)

# Add a legend
legend("topright", 
      legend = c("Actual CI Width", "Theoretical 1/sqrt(n) Relationship"),
      col = c("blue", "red"),
      lty = c(1, 2),
      lwd = 2,
      pch = c(19, NA),
      bty = "n")

# Real-world example: Confidence interval for mpg in the mtcars dataset
data(mtcars)

# Calculate the 95% confidence interval for the mean mpg
mpg_test <- t.test(mtcars$mpg, conf.level = 0.95)
mpg_ci <- mpg_test$conf.int

cat("\nReal-world Example: 95% Confidence Interval for Mean MPG in mtcars dataset:\n")
cat("Sample mean:", round(mean(mtcars$mpg), 2), "\n")
cat("95% CI: [", round(mpg_ci[1], 2), ", ", round(mpg_ci[2], 2), "]\n")
cat("Interpretation: We are 95% confident that the true average MPG for the population\n")
cat("of cars represented by this sample is between", round(mpg_ci[1], 2), "and", round(mpg_ci[2], 2), "mpg.\n")

# Calculate confidence intervals for mpg by cylinder count
cat("\nConfidence Intervals for MPG by Number of Cylinders:\n")
# Convert cylinders to a factor
mtcars$cyl_f <- as.factor(mtcars$cyl)

# For each cylinder group
for (cyl in levels(mtcars$cyl_f)) {
 mpg_subset <- mtcars$mpg[mtcars$cyl == cyl]
 cyl_test <- t.test(mpg_subset, conf.level = 0.95)
 cyl_ci <- cyl_test$conf.int
 
 cat(cyl, "cylinders (n =", length(mpg_subset), "):\n")
 cat("  Mean MPG:", round(mean(mpg_subset), 2), "\n")
 cat("  95% CI: [", round(cyl_ci[1], 2), ", ", round(cyl_ci[2], 2), "]\n")
}

# Visualize confidence intervals for mpg by cylinder count
# Calculate means and confidence intervals
cyl_levels <- sort(unique(mtcars$cyl))
cyl_means <- numeric(length(cyl_levels))
cyl_lower <- numeric(length(cyl_levels))
cyl_upper <- numeric(length(cyl_levels))

for (i in 1:length(cyl_levels)) {
 cyl <- cyl_levels[i]
 mpg_subset <- mtcars$mpg[mtcars$cyl == cyl]
 
 cyl_means[i] <- mean(mpg_subset)
 cyl_test <- t.test(mpg_subset, conf.level = 0.95)
 cyl_lower[i] <- cyl_test$conf.int[1]
 cyl_upper[i] <- cyl_test$conf.int[2]
}

# Plot the means with confidence intervals
plot(cyl_levels, cyl_means, 
    type = "p", 
    xlab = "Number of Cylinders", 
    ylab = "Miles Per Gallon",
    main = "Mean MPG by Cylinder Count with 95% Confidence Intervals",
    ylim = c(min(cyl_lower) - 1, max(cyl_upper) + 1),
    pch = 19,
    col = "blue",
    xaxt = "n")  # Turn off x-axis labels

# Add custom x-axis labels
axis(1, at = cyl_levels)

# Add confidence intervals as error bars
arrows(cyl_levels, cyl_lower, cyl_levels, cyl_upper, 
      angle = 90, code = 3, length = 0.1, col = "blue", lwd = 2)

# Add sample size for each group
for (i in 1:length(cyl_levels)) {
 cyl <- cyl_levels[i]
 n <- sum(mtcars$cyl == cyl)
 text(cyl, cyl_lower[i] - 0.8, paste("n =", n), cex = 0.8)
}

# Add another perspective - compare with boxplots
boxplot(mpg ~ cyl, 
       data = mtcars,
       main = "MPG Distribution by Cylinder Count",
       xlab = "Number of Cylinders",
       ylab = "Miles Per Gallon",
       col = "lightblue")

# Add the means and confidence intervals to the boxplot
points(1:length(cyl_levels), cyl_means, pch = 19, col = "red")
for (i in 1:length(cyl_levels)) {
 arrows(i, cyl_lower[i], i, cyl_upper[i], 
        angle = 90, code = 3, length = 0.1, col = "red", lwd = 2)
}

# Add a legend
legend("topright", 
      legend = c("Mean", "95% CI"),
      col = c("red", "red"),
      pch = c(19, NA),
      lwd = c(NA, 2),
      bty = "n")
]]>
       </input>
     </program>
     
     <exercise xml:id="ex-ci-interpretation">
       <title>Confidence Interval Interpretation</title>
       <statement>
         <p>A researcher measures the hemoglobin levels (in g/dL) in a sample of 40 adult women and calculates a 95% confidence interval of [12.3, 13.7] for the population mean.</p>
       </statement>
       <exercise>
         <statement>
           <p>What is the correct interpretation of this confidence interval?</p>
         </statement>
         <answer>
           <p>The correct interpretation of the 95% confidence interval [12.3, 13.7] for hemoglobin levels is:</p>
           <p>We are 95% confident that the true population mean hemoglobin level for adult women is between 12.3 and 13.7 g/dL.</p>
           <p>This means that if we were to take many different samples of 40 adult women and construct 95% confidence intervals using the same method, approximately 95% of those intervals would contain the true population mean hemoglobin level.</p>
           <p>The confidence interval provides a range of plausible values for the unknown population parameter (mean hemoglobin level) based on the sample data. It reflects the precision and uncertainty in our estimate due to sampling variability.</p>
           <p>It's important to note what this does <em>not</em> mean:</p>
           <ul>
             <li><p>It does <em>not</em> mean there's a 95% probability that the true mean is between 12.3 and 13.7 g/dL. The true population mean is fixed (not random), while the confidence interval is random (varies with different samples).</p></li>
             <li><p>It does <em>not</em> mean that 95% of individual women's hemoglobin levels fall within this range. The interval represents plausible values for the <em>mean</em>, not for individual measurements.</p></li>
           </ul>
         </answer>
       </exercise>
       <exercise>
         <statement>
           <p>If the researcher wants to reduce the width of the confidence interval while maintaining the same confidence level (95%), what could they do?</p>
         </statement>
         <answer>
           <p>To reduce the width of the confidence interval while maintaining the same 95% confidence level, the researcher could:</p>
           <ol>
             <li><p><strong>Increase the sample size</strong>: This is the most effective approach. The width of a confidence interval is proportional to 1/√n, where n is the sample size. By increasing the sample from 40 to 160 women, the researcher could reduce the interval width by half (since √160 = 2 × √40).</p></li>
             <li><p><strong>Reduce the variability in measurements</strong>: The width of the confidence interval is directly proportional to the standard deviation of the sample. The researcher could:
               <ul>
                 <li><p>Use more precise measuring instruments to reduce measurement error</p></li>
                 <li><p>Control for confounding variables that contribute to variability (e.g., standardizing the time of day when samples are collected)</p></li>
                 <li><p>Use a more homogeneous sample population (though this would limit generalizability)</p></li>
               </ul>
             </p></li>
             <li><p><strong>Use a more efficient sampling design</strong>: Techniques like stratified sampling can sometimes yield more precise estimates than simple random sampling for the same sample size.</p></li>
             <li><p><strong>Apply statistical techniques</strong> that reduce variance, such as:
               <ul>
                 <li><p>Using paired designs where appropriate</p></li>
                 <li><p>Including relevant covariates in the analysis (e.g., ANCOVA instead of ANOVA)</p></li>
                 <li><p>Using ratio or regression estimators if auxiliary information is available</p></li>
               </ul>
             </p></li>
           </ol>
           <p>Among these options, increasing the sample size is typically the most straightforward and reliable approach, though it may be constrained by practical limitations such as cost, time, and participant availability.</p>
         </answer>
       </exercise>
     </exercise>
   </subsection>
   
   <!-- Linear regression basics -->
   <subsection xml:id="subsec-linear-regression">
     <title>Linear regression basics</title>
     
     <p>Linear regression is a fundamental statistical technique used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables and is widely used for prediction and understanding variable relationships.</p>
     
     <paragraphs>
       <title>Simple Linear Regression</title>
       <p>Simple linear regression models the relationship between two variables:</p>
       <p>y = β₀ + β₁x + ε</p>
       <p>where:</p>
       <ul>
         <li>
           <p>y is the dependent (outcome) variable</p>
         </li>
         <li>
           <p>x is the independent (predictor) variable</p>
         </li>
         <li>
           <p>β₀ is the y-intercept (value of y when x = 0)</p>
         </li>
         <li>
           <p>β₁ is the slope (change in y for a one-unit change in x)</p>
         </li>
         <li>
           <p>ε is the error term (random variation not explained by the model)</p>
         </li>
       </ul>
     </paragraphs>
     
     <paragraphs>
       <title>Multiple Linear Regression</title>
       <p>Multiple linear regression extends the model to include multiple independent variables:</p>
       <p>y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ + ε</p>
       <p>where p is the number of independent variables.</p>
     </paragraphs>
     
     <paragraphs>
       <title>Key Concepts in Linear Regression</title>
       <ul>
         <li>
           <p><term>Least Squares Estimation</term>: The method used to find the line that minimizes the sum of squared differences between observed and predicted values.</p>
         </li>
         <li>
           <p><term>Coefficient of Determination (R²)</term>: Measures the proportion of variance in the dependent variable explained by the independent variable(s). Ranges from 0 to 1, with higher values indicating better fit.</p>
         </li>
         <li>
           <p><term>Adjusted R²</term>: A modified version of R² that adjusts for the number of predictors, making it more suitable for comparing models with different numbers of variables.</p>
         </li>
         <li>
           <p><term>Residuals</term>: The differences between observed and predicted values, used to assess model fit and check assumptions.</p>
         </li>
         <li>
           <p><term>Standard Error</term>: Measures the variability of the coefficient estimates, used for hypothesis testing and confidence intervals.</p>
         </li>
         <li>
           <p><term>t-statistic and p-value</term>: Used to test whether a coefficient is significantly different from zero.</p>
         </li>
       </ul>
     </paragraphs>
     
     <paragraphs>
       <title>Assumptions of Linear Regression</title>
       <ul>
         <li>
           <p><term>Linearity</term>: The relationship between the independent and dependent variables is linear.</p>
         </li>
         <li>
           <p><term>Independence</term>: The observations are independent of each other.</p>
         </li>
         <li>
           <p><term>Homoscedasticity</term>: The variance of the errors is constant across all levels of the independent variables.</p>
         </li>
         <li>
           <p><term>Normality</term>: The errors are normally distributed.</p>
         </li>
         <li>
           <p><term>No multicollinearity</term> (for multiple regression): The independent variables are not highly correlated with each other.</p>
         </li>
       </ul>
     </paragraphs>
     
     <program interactive="webr" xml:id="prog-linear-regression">
       <input>
<![CDATA[
# Linear Regression Examples in R
# Load necessary data
data(mtcars)

# Simple Linear Regression: MPG vs. Weight
# Fit the model
model1 <- lm(mpg ~ wt, data = mtcars)

# Display the model summary
summary(model1)

# Extract key components
coefficients <- coef(model1)
intercept <- coefficients[1]
slope <- coefficients[2]

cat("Simple Linear Regression: MPG vs. Weight\n")
cat("Equation: MPG =", round(intercept, 2), "+", round(slope, 2), "× Weight\n")
cat("Interpretation: For each additional 1,000 lbs of weight,\n")
cat("                MPG decreases by approximately", abs(round(slope, 2)), "units.\n\n")

# R-squared and adjusted R-squared
r_squared <- summary(model1)$r.squared
adj_r_squared <- summary(model1)$adj.r.squared

cat("R-squared:", round(r_squared, 3), "\n")
cat("Adjusted R-squared:", round(adj_r_squared, 3), "\n")
cat("Interpretation: About", round(r_squared * 100), "% of the variation in MPG\n")
cat("                can be explained by the weight of the car.\n\n")

# Confidence intervals for coefficients
conf_intervals <- confint(model1, level = 0.95)
cat("95% Confidence Intervals for Coefficients:\n")
print(round(conf_intervals, 3))

# Plot the data with the regression line
plot(mtcars$wt, mtcars$mpg,
    main = "MPG vs. Weight with Regression Line",
    xlab = "Weight (1000 lbs)",
    ylab = "Miles Per Gallon",
    pch = 19,
    col = "blue")

# Add the regression line
abline(model1, col = "red", lwd = 2)

# Add equation and R-squared to the plot
text(4, 30, paste("MPG =", round(intercept, 2), "+", round(slope, 2), "× Weight"), pos = 4)
text(4, 28, paste("R² =", round(r_squared, 3)), pos = 4)

# Add confidence bands for the regression line
# First, create a sequence of weight values
weight_seq <- seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)

# Predict MPG for these weights with confidence intervals
predictions <- predict(model1, newdata = data.frame(wt = weight_seq),
                     interval = "confidence", level = 0.95)

# Add the confidence bands
lines(weight_seq, predictions[, "lwr"], col = "blue", lty = 2)
lines(weight_seq, predictions[, "upr"], col = "blue", lty = 2)

# Add a legend
legend("topright", 
      legend = c("Data", "Regression Line", "95% Confidence Bands"),
      col = c("blue", "red", "blue"),
      pch = c(19, NA, NA),
      lty = c(NA, 1, 2),
      lwd = c(NA, 2, 1),
      bty = "n")

# Residual Analysis
residuals <- residuals(model1)
fitted_values <- fitted(model1)

# Create residual plots
par(mfrow = c(2, 2))

# Residuals vs. Fitted Values
plot(fitted_values, residuals,
    main = "Residuals vs. Fitted Values",
    xlab = "Fitted Values",
    ylab = "Residuals",
    pch = 19,
    col = "blue")
abline(h = 0, col = "red", lwd = 2, lty = 2)

# Normal Q-Q Plot of Residuals
qqnorm(residuals, main = "Normal Q-Q Plot of Residuals")
qqline(residuals, col = "red", lwd = 2)

# Scale-Location Plot (sqrt of standardized residuals vs. fitted values)
plot(fitted_values, sqrt(abs(residuals/sd(residuals))),
    main = "Scale-Location Plot",
    xlab = "Fitted Values",
    ylab = "Sqrt of Standardized Residuals",
    pch = 19,
    col = "blue")
lines(lowess(fitted_values, sqrt(abs(residuals/sd(residuals)))), col = "red", lwd = 2)

# Residuals vs. Leverage
plot(hatvalues(model1), residuals,
    main = "Residuals vs. Leverage",
    xlab = "Leverage",
    ylab = "Residuals",
    pch = 19,
    col = "blue")
abline(h = 0, col = "red", lwd = 2, lty = 2)

# Reset plotting parameters
par(mfrow = c(1, 1))

# Multiple Linear Regression: MPG vs. Weight and Horsepower
# Fit the model
model2 <- lm(mpg ~ wt + hp, data = mtcars)

# Display the model summary
summary(model2)

# Extract key components
coefficients2 <- coef(model2)
intercept2 <- coefficients2[1]
wt_coef <- coefficients2[2]
hp_coef <- coefficients2[3]

cat("\nMultiple Linear Regression: MPG vs. Weight and Horsepower\n")
cat("Equation: MPG =", round(intercept2, 2), "+", 
    round(wt_coef, 2), "× Weight +", 
    round(hp_coef, 2), "× Horsepower\n")
cat("Interpretation:\n")
cat("- For each additional 1,000 lbs of weight, MPG decreases by approximately", 
    abs(round(wt_coef, 2)), "units, holding horsepower constant.\n")
cat("- For each additional horsepower, MPG decreases by approximately", 
    abs(round(hp_coef, 2)), "units, holding weight constant.\n\n")

# R-squared and adjusted R-squared
r_squared2 <- summary(model2)$r.squared
adj_r_squared2 <- summary(model2)$adj.r.squared

cat("R-squared:", round(r_squared2, 3), "\n")
cat("Adjusted R-squared:", round(adj_r_squared2, 3), "\n")
cat("Interpretation: About", round(r_squared2 * 100), "% of the variation in MPG\n")
cat("                can be explained by weight and horsepower together.\n\n")

# Confidence intervals for coefficients
conf_intervals2 <- confint(model2, level = 0.95)
cat("95% Confidence Intervals for Coefficients:\n")
print(round(conf_intervals2, 3))

# Model comparison
cat("\nModel Comparison: Simple vs. Multiple Regression\n")
anova_result <- anova(model1, model2)
print(anova_result)

cat("\nInterpretation: The p-value in the ANOVA table indicates whether adding horsepower\n")
cat("significantly improves the model compared to using weight alone.\n")
cat("A small p-value (< 0.05) suggests that the multiple regression model is preferable.\n\n")

# Create a 3D visualization of the multiple regression model
# (In a normal R environment, you would use packages like 'plotly' or 'rgl')
# Here, we'll create a 2D visualization with color for the third dimension

# Create a grid of weight and horsepower values
weight_grid <- seq(min(mtcars$wt), max(mtcars$wt), length.out = 30)
hp_grid <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 30)
grid_data <- expand.grid(wt = weight_grid, hp = hp_grid)

# Predict MPG for each combination
grid_data$mpg_pred <- predict(model2, newdata = grid_data)

# Create a scatter plot with contour lines
# Plot the actual data points
plot(mtcars$wt, mtcars$hp, 
     main = "Multiple Regression: MPG as a Function of Weight and Horsepower",
     xlab = "Weight (1000 lbs)",
     ylab = "Horsepower",
     pch = 19,
     col = "blue",
     cex = mtcars$mpg / 10)  # Size proportional to actual MPG

# Add contour lines for predicted MPG
contour(weight_grid, hp_grid, 
        matrix(grid_data$mpg_pred, nrow = length(weight_grid)), 
        add = TRUE,
        levels = seq(10, 35, by = 5),
        col = "red",
        lwd = 2,
        labcex = 0.8)

# Add a legend
legend("topright", 
       legend = c("Car (size proportional to MPG)", "MPG contour lines"),
       col = c("blue", "red"),
       pch = c(19, NA),
       lty = c(NA, 1),
       lwd = c(NA, 2),
       bty = "n")

# Check for multicollinearity
cat("\nChecking for Multicollinearity:\n")
cat("Correlation between Weight and Horsepower:", round(cor(mtcars$wt, mtcars$hp), 3), "\n")
if(abs(cor(mtcars$wt, mtcars$hp)) > 0.7) {
  cat("Note: The high correlation between weight and horsepower indicates\n")
  cat("potential multicollinearity, which could affect the stability and\n")
  cat("interpretation of the regression coefficients.\n\n")
}

# Calculate Variance Inflation Factor (VIF) manually
# VIF = 1/(1-R²), where R² is from regressing the predictor on other predictors
wt_on_hp_model <- lm(wt ~ hp, data = mtcars)
hp_on_wt_model <- lm(hp ~ wt, data = mtcars)

wt_vif <- 1 / (1 - summary(wt_on_hp_model)$r.squared)
hp_vif <- 1 / (1 - summary(hp_on_wt_model)$r.squared)

cat("Variance Inflation Factors (VIF):\n")
cat("- Weight:", round(wt_vif, 2), "\n")
cat("- Horsepower:", round(hp_vif, 2), "\n")
cat("VIF values greater than 5-10 indicate problematic multicollinearity.\n\n")

# Reset plotting parameters
par(mfrow = c(1, 1))

# Summary of linear regression analysis
cat("\nSummary of Linear Regression Analysis:\n")
cat("------------------------------------\n\n")

cat("Linear regression is a powerful and interpretable modeling technique that allows us\n")
cat("to understand and quantify relationships between variables. In our analysis of the\n")
cat("mtcars dataset, we found that both car weight and horsepower are significant\n")
cat("predictors of fuel efficiency (MPG).\n\n")

cat("The simple linear regression model using only weight explained", 
    round(r_squared * 100), "% of the\n")
cat("variation in MPG, while the multiple regression model including both weight and\n")
cat("horsepower explained", round(r_squared2 * 100), "% of the variation.\n\n")

cat("The multiple regression analysis revealed that, holding other factors constant:\n")
cat("1. Each additional 1,000 lbs of weight is associated with a", 
    abs(round(wt_coef, 2)), "MPG decrease\n")
cat("2. Each additional horsepower is associated with a", 
    abs(round(hp_coef, 2)), "MPG decrease\n\n")

cat("These findings can help car manufacturers understand the trade-offs between\n")
cat("performance characteristics and fuel efficiency. For example, reducing vehicle\n")
cat("weight appears to be an effective strategy for improving MPG.\n\n")

cat("However, it's important to note that our analysis has limitations. The mtcars\n")
cat("dataset is relatively small (32 observations) and from the 1970s, so the\n")
cat("relationships may not fully apply to modern vehicles with advanced technologies.\n")
cat("Additionally, we should be cautious about causal interpretation, as other factors\n")
cat("not included in our model might influence both our predictors and MPG.\n\n")

cat("Further analysis could explore additional predictor variables, interaction effects,\n")
cat("and non-linear relationships to build a more comprehensive model of fuel efficiency.\n")
]]>
    </input>
  </program>
  
  <exercise xml:id="ex-multi-regression">
    <title>Multiple Regression Analysis</title>
    <statement>
      <p>Based on the multiple regression model explored above, answer the following questions:</p>
    </statement>
    <exercise>
      <statement>
        <p>How does the relationship between weight and MPG change when horsepower is added to the model? Compare the coefficients from the simple and multiple regression models and explain the differences.</p>
      </statement>
      <answer>
        <p>When horsepower is added to the regression model, the relationship between weight and MPG typically changes in the following ways:</p>
        
        <p>In the simple regression model (MPG ~ weight), the weight coefficient reflects both the direct effect of weight on MPG and the indirect effect through weight's correlation with horsepower. This is because the simple model doesn't account for the fact that heavier cars typically have more powerful engines.</p>
        
        <p>In the multiple regression model (MPG ~ weight + horsepower), the weight coefficient represents the estimated effect of weight on MPG while holding horsepower constant. This provides a more nuanced understanding of each factor's unique contribution.</p>
        
        <p>The weight coefficient usually decreases in magnitude when horsepower is added to the model. This indicates that part of the apparent effect of weight on MPG in the simple model was actually due to the correlation between weight and horsepower. By including both variables, we can separate their individual effects.</p>
        
        <p>For example, if the weight coefficient changes from -5.0 in the simple model to -3.5 in the multiple model, this suggests that about 1.5 units of the apparent weight effect was actually attributable to horsepower differences.</p>
        
        <p>This change in coefficients highlights the importance of considering multiple factors simultaneously when analyzing complex relationships, as simpler models may overestimate the impact of individual variables due to unaccounted correlations.</p>
      </answer>
    </exercise>
    <exercise>
      <statement>
        <p>A new sports car has a weight of 2,800 lbs and 320 horsepower. Using the multiple regression model, predict the MPG for this car and construct a 95% prediction interval. Interpret the results and discuss any limitations of this prediction.</p>
      </statement>
      <answer>
        <p>To predict the MPG for a new sports car with a weight of 2,800 lbs and 320 horsepower, we would substitute these values into our multiple regression equation:</p>
        
        <p>Predicted MPG = β₀ + β₁(weight) + β₂(horsepower)</p>
        
        <p>For example, if our equation is MPG = 37.23 - 3.88(weight) - 0.03(horsepower), the prediction would be:</p>
        <p>Predicted MPG = 37.23 - 3.88(2.8) - 0.03(320) = 37.23 - 10.86 - 9.6 = 16.77 MPG</p>
        
        <p>A 95% prediction interval might be [13.2, 20.3] MPG, indicating we're 95% confident that the actual MPG of this specific car would fall within this range.</p>
        
        <p><strong>Interpretation:</strong> This prediction suggests the sports car would achieve moderate fuel efficiency, which is expected given its relatively light weight but high horsepower. The width of the prediction interval reflects uncertainty in our estimate due to natural variability in MPG even among similar cars.</p>
        
        <p><strong>Limitations:</strong></p>
        <ul>
          <li><p>The prediction involves extrapolation if 320 horsepower is beyond the range of the training data, which reduces reliability</p></li>
          <li><p>The model doesn't account for other relevant factors such as aerodynamics, transmission type, or engine technology</p></li>
          <li><p>Sports cars may have specific design features not represented in the general car population used to build the model</p></li>
          <li><p>The dataset (mtcars) is from the 1970s, so relationships may differ for modern vehicles with advanced technologies</p></li>
          <li><p>The prediction assumes a linear relationship between predictors and MPG, which may not hold at extreme values</p></li>
        </ul>
        
        <p>For more accurate predictions, it would be beneficial to use more recent data specific to similar sports cars and include additional relevant predictors.</p>
      </answer>
    </exercise>
  </exercise>
</subsection>

<subsection xml:id="subsec-hypothesis-testing">
  <title>Hypothesis testing basics</title>
  
  <p>Hypothesis testing is a fundamental statistical method used to make inferences about populations based on sample data. It provides a framework for deciding whether experimental or observational data support a particular claim about a population parameter.</p>
  
  <paragraphs>
    <title>The Hypothesis Testing Framework</title>
    <p>The general process of hypothesis testing follows these steps:</p>
    <ol>
      <li>
        <p>State the null hypothesis (H₀) and alternative hypothesis (H₁ or Hₐ)</p>
      </li>
      <li>
        <p>Choose a significance level (α) - commonly 0.05</p>
      </li>
      <li>
        <p>Collect data and calculate a test statistic</p>
      </li>
      <li>
        <p>Calculate the p-value or compare the test statistic to critical values</p>
      </li>
      <li>
        <p>Make a decision: reject or fail to reject the null hypothesis</p>
      </li>
      <li>
        <p>Interpret the results in the context of the original question</p>
      </li>
    </ol>
  </paragraphs>
  
  <paragraphs>
    <title>Key Concepts in Hypothesis Testing</title>
    <ul>
      <li>
        <p><term>Null Hypothesis (H₀)</term>: A statement of no effect, no difference, or no relationship. It's the default position that is assumed to be true until evidence suggests otherwise.</p>
      </li>
      <li>
        <p><term>Alternative Hypothesis (H₁ or Hₐ)</term>: A statement that contradicts the null hypothesis and represents what the researcher is trying to establish.</p>
      </li>
      <li>
        <p><term>Significance Level (α)</term>: The threshold probability for rejecting the null hypothesis, typically set at 0.05 (5%). It represents the risk of making a Type I error.</p>
      </li>
      <li>
        <p><term>p-value</term>: The probability of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true. A smaller p-value indicates stronger evidence against the null hypothesis.</p>
      </li>
      <li>
        <p><term>Type I Error</term>: Rejecting a true null hypothesis (false positive).</p>
      </li>
      <li>
        <p><term>Type II Error</term>: Failing to reject a false null hypothesis (false negative).</p>
      </li>
      <li>
        <p><term>Power</term>: The probability of correctly rejecting a false null hypothesis. Power increases with sample size and effect size.</p>
      </li>
    </ul>
  </paragraphs>
  
  <paragraphs>
    <title>Common Hypothesis Tests</title>
    <ul>
      <li>
        <p><term>t-test</term>: Used to compare means.
          <ul>
            <li><p>One-sample t-test: Compares a sample mean to a known population mean.</p></li>
            <li><p>Independent samples t-test: Compares means of two independent groups.</p></li>
            <li><p>Paired samples t-test: Compares means of two related groups (before/after, matched pairs).</p></li>
          </ul>
        </p>
      </li>
      <li>
        <p><term>ANOVA (Analysis of Variance)</term>: Extends the t-test to compare means across more than two groups.</p>
      </li>
      <li>
        <p><term>Chi-Square Test</term>: Used to test relationships between categorical variables.</p>
      </li>
      <li>
        <p><term>Correlation Test</term>: Tests the significance of a correlation coefficient.</p>
      </li>
      <li>
        <p><term>Non-parametric Tests</term>: Used when data do not meet the assumptions of parametric tests (e.g., Mann-Whitney U, Wilcoxon rank-sum).</p>
      </li>
    </ul>
  </paragraphs>
  
  <program interactive="webr" xml:id="prog-hypothesis-testing">
    <input>
<![CDATA[
# Hypothesis Testing Examples in R
# Set a seed for reproducibility
set.seed(123)

# Example 1: One-sample t-test
# Suppose we want to test if the mean mpg of cars is different from 20 mpg
data(mtcars)

# Perform the one-sample t-test
t_test_one_sample <- t.test(mtcars$mpg, mu = 20)

# Display the results
cat("One-sample t-test:\n")
print(t_test_one_sample)

# Explain the results
cat("\nInterpretation:\n")
if (t_test_one_sample$p.value < 0.05) {
  cat("The p-value (", round(t_test_one_sample$p.value, 4), ") is less than 0.05, so we reject the null hypothesis.\n")
  cat("There is sufficient evidence to conclude that the mean MPG is different from 20.\n")
} else {
  cat("The p-value (", round(t_test_one_sample$p.value, 4), ") is greater than 0.05, so we fail to reject the null hypothesis.\n")
  cat("There is insufficient evidence to conclude that the mean MPG is different from 20.\n")
}

# Example 2: Two-sample t-test (independent samples)
# Compare MPG between automatic and manual transmission cars
automatic <- mtcars$mpg[mtcars$am == 0]
manual <- mtcars$mpg[mtcars$am == 1]

# Perform the two-sample t-test
t_test_two_sample <- t.test(manual, automatic)

# Display the results
cat("\nTwo-sample t-test (Manual vs Automatic Transmission):\n")
print(t_test_two_sample)

# Visualization of the two groups
boxplot(mpg ~ am, data = mtcars,
        main = "MPG by Transmission Type",
        xlab = "Transmission (0 = Automatic, 1 = Manual)",
        ylab = "Miles Per Gallon",
        col = c("lightblue", "lightgreen"))

# Add the means to the boxplot
points(c(1, 2), c(mean(automatic), mean(manual)), pch = 19, col = "red")

# Example 3: Paired t-test
# Let's simulate before-after measurements
set.seed(456)
before_treatment <- rnorm(20, mean = 10, sd = 2)
after_treatment <- before_treatment + rnorm(20, mean = 1.5, sd = 1)  # Effect of +1.5 with some noise

# Perform the paired t-test
t_test_paired <- t.test(after_treatment, before_treatment, paired = TRUE)

# Display the results
cat("\nPaired t-test (After vs Before Treatment):\n")
print(t_test_paired)

# Visualization of the paired data
plot(1:20, before_treatment, type = "p", col = "blue", pch = 19,
     ylim = c(min(c(before_treatment, after_treatment)) - 1, max(c(before_treatment, after_treatment)) + 1),
     xlab = "Subject", ylab = "Measurement",
     main = "Before-After Treatment Comparison")
points(1:20, after_treatment, col = "red", pch = 19)
legend("topleft", legend = c("Before", "After"), col = c("blue", "red"), pch = 19)

# Connect the paired points with lines
for (i in 1:20) {
  segments(i, before_treatment[i], i, after_treatment[i], col = "gray")
}

# Example 4: Chi-square test of independence
# Create a contingency table (example: gender vs. preference)
gender <- factor(c(rep("Male", 50), rep("Female", 50)))
preference <- factor(c(rep("Option A", 30), rep("Option B", 20), 
                      rep("Option A", 20), rep("Option B", 30)))

# Create a contingency table
contingency_table <- table(gender, preference)
print(contingency_table)

# Perform the chi-square test
chi_sq_test <- chisq.test(contingency_table)

# Display the results
cat("\nChi-square test of independence:\n")
print(chi_sq_test)

# Visualize the contingency table
barplot(contingency_table, beside = TRUE, legend = TRUE,
        main = "Preference by Gender",
        xlab = "Preference",
        ylab = "Count",
        col = c("lightblue", "lightpink"))

# Example 5: Correlation test
# Test if there's a significant correlation between weight and MPG
cor_test <- cor.test(mtcars$wt, mtcars$mpg)

# Display the results
cat("\nCorrelation test (Weight vs MPG):\n")
print(cor_test)

# Visualize the relationship
plot(mtcars$wt, mtcars$mpg,
     main = "Weight vs. MPG with Correlation Results",
     xlab = "Weight (1000 lbs)",
     ylab = "Miles Per Gallon",
     pch = 19,
     col = "blue")

# Add the regression line
abline(lm(mpg ~ wt, data = mtcars), col = "red", lwd = 2)

# Add the correlation value to the plot
text(4, 30, paste("r =", round(cor_test$estimate, 2),
                 "\np-value =", round(cor_test$p.value, 5)),
     pos = 4)
]]>
    </input>
  </program>
  
      <exercise xml:id="ex-hypothesis-testing">
        <title>Hypothesis Testing Practice</title>
        <statement>
          <p>Answer the following questions about hypothesis testing:</p>
        </statement>
        <exercise>
          <statement>
            <p>A researcher wants to test if a new teaching method improves student test scores. The average test score with the traditional method is 75. After implementing the new method with 40 students, the mean score is 78 with a standard deviation of 8. Does the new method significantly improve test scores at α = 0.05?</p>
          </statement>
          <answer>
            <p>To determine if the new teaching method significantly improves test scores, we need to conduct a one-sample t-test.</p>
            <p><strong>Step 1:</strong> State the hypotheses
              <ul>
                <li>H₀: μ ≤ 75 (The new method does not improve scores)</li>
                <li>H₁: μ > 75 (The new method improves scores)</li>
              </ul>
            </p>
            
            <p><strong>Step 2:</strong> Calculate the test statistic (t)
              <ul>
                <li>Sample mean (x̄) = 78</li>
                <li>Population mean under H₀ (μ₀) = 75</li>
                <li>Sample standard deviation (s) = 8</li>
                <li>Sample size (n) = 40</li>
                <li>t = (x̄ - μ₀)/(s/√n) = (78 - 75)/(8/√40) = 3/1.265 = 2.37</li>
              </ul>
            </p>
            
            <p><strong>Step 3:</strong> Find the critical value
              <ul>
                <li>For a one-tailed test with α = 0.05 and df = 39</li>
                <li>Critical t-value = 1.685</li>
              </ul>
            </p>
            
            <p><strong>Step 4:</strong> Make a decision
              <ul>
                <li>Since the calculated t-value (2.37) exceeds the critical value (1.685), we reject the null hypothesis</li>
              </ul>
            </p>
            
            <p><strong>Conclusion:</strong> There is statistically significant evidence (t(39) = 2.37, p &lt; 0.05) that the new teaching method improves student test scores compared to the traditional method.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Explain the difference between statistical significance and practical significance in hypothesis testing, and why it's important to consider both when interpreting results.</p>
          </statement>
          <answer>
            <p><strong>Statistical Significance</strong> refers to the likelihood that an observed effect occurred by chance. A result is statistically significant when the p-value is less than the predetermined significance level (typically α = 0.05), indicating that the observed effect is unlikely to have occurred randomly.</p>
            
            <p><strong>Practical Significance</strong> refers to the real-world importance or meaningfulness of an effect, regardless of its statistical significance. It considers whether the magnitude of the effect is large enough to be relevant in practical applications.</p>
            
            <p><strong>Key Differences:</strong>
              <ul>
                <li><p><strong>Focus:</strong> Statistical significance focuses on probability and reliability of findings; practical significance focuses on magnitude and real-world impact</p></li>
                <li><p><strong>Influenced by:</strong> Statistical significance is heavily influenced by sample size; practical significance is not</p></li>
                <li><p><strong>Measurement:</strong> Statistical significance is measured by p-values; practical significance often evaluated using effect sizes (e.g., Cohen's d, R²)</p></li>
              </ul>
            </p>
            
            <p><strong>Importance of Considering Both:</strong>
              <ul>
                <li><p>With large sample sizes, even tiny, inconsequential differences can become statistically significant</p></li>
                <li><p>A statistically significant result might have such a small effect size that it's not worth implementing in practice</p></li>
                <li><p>Conversely, a result might not reach statistical significance due to small sample size but could have a large effect size suggesting practical importance</p></li>
                <li><p>Considering both provides a more complete understanding of research findings and their applications</p></li>
                <li><p>Helps prevent making important decisions based solely on p-values, which has been criticized in the "p-value crisis" in science</p></li>
                <li><p>Allows for better cost-benefit analysis when implementing interventions or changes based on research findings</p></li>
              </ul>
            </p>
            
            <p>A comprehensive approach to hypothesis testing should always consider both statistical significance (Is the effect real?) and practical significance (Does the effect matter?), especially when making recommendations based on data analysis.</p>
          </answer>
        </exercise>
      </exercise>
    </subsection>
    
    <!-- ANOVA section -->
    <section xml:id="sec-anova-basics">
      <title>ANOVA basics</title>
      <p>Analysis of Variance (ANOVA) is a statistical method used to compare means across three or more groups. It extends the t-test concept to situations with multiple groups, helping to determine if there are any statistically significant differences between the means of independent groups.</p>
      
      <paragraphs>
        <title>Basic Concepts of ANOVA</title>
        <p>ANOVA works by analyzing the variance in the data and partitioning it into different sources:</p>
        <ul>
          <li>
            <p><term>Between-Group Variance</term>: Variation due to differences between group means (treatment effect)</p>
          </li>
          <li>
            <p><term>Within-Group Variance</term>: Variation due to differences within each group (error or residual variance)</p>
          </li>
        </ul>
        <p>The F-statistic is then calculated as the ratio of between-group variance to within-group variance. A large F-value suggests that the between-group variance is more significant than what would be expected by chance, indicating differences between group means.</p>
      </paragraphs>
      
      <paragraphs>
        <title>Types of ANOVA</title>
        <ul>
          <li>
            <p><term>One-Way ANOVA</term>: Analyzes the effect of a single factor (independent variable) on a continuous outcome.</p>
          </li>
          <li>
            <p><term>Two-Way ANOVA</term>: Examines the effect of two factors simultaneously, including their interaction.</p>
          </li>
          <li>
            <p><term>Repeated Measures ANOVA</term>: Used when the same subjects are measured multiple times (like in before-after designs).</p>
          </li>
          <li>
            <p><term>MANOVA (Multivariate ANOVA)</term>: Extends ANOVA to cases with multiple dependent variables.</p>
          </li>
          <li>
            <p><term>ANCOVA (Analysis of Covariance)</term>: Incorporates continuous control variables (covariates) into the ANOVA model.</p>
          </li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>ANOVA Assumptions</title>
        <p>For valid ANOVA results, several assumptions should be met:</p>
        <ul>
          <li>
            <p><term>Independence</term>: Observations are independent of each other.</p>
          </li>
          <li>
            <p><term>Normality</term>: The data within each group are normally distributed.</p>
          </li>
          <li>
            <p><term>Homogeneity of Variance</term>: The variance is similar across all groups (homoscedasticity).</p>
          </li>
        </ul>
      </paragraphs>
      
      <paragraphs>
        <title>Post-hoc Tests</title>
        <p>ANOVA tells us if there are significant differences among groups, but it doesn't tell us which specific groups differ from each other. Post-hoc tests are used to make pairwise comparisons after a significant ANOVA result:</p>
        <ul>
          <li>
            <p><term>Tukey's HSD (Honestly Significant Difference)</term>: Controls the family-wise error rate and is commonly used for all pairwise comparisons.</p>
          </li>
          <li>
            <p><term>Bonferroni Correction</term>: A conservative approach that adjusts the significance level by dividing it by the number of comparisons.</p>
          </li>
          <li>
            <p><term>Scheffe's Method</term>: A flexible but conservative test that allows for complex comparisons.</p>
          </li>
          <li>
            <p><term>Dunnett's Test</term>: Used when comparing multiple groups to a single control group.</p>
          </li>
        </ul>
      </paragraphs>
      
      <program interactive="webr" xml:id="prog-anova">
        <input>
<![CDATA[
# ANOVA Examples in R
# Load necessary data
data(iris)
data(mtcars)

# Example 1: One-Way ANOVA
# Compare sepal lengths across three iris species
one_way_anova <- aov(Sepal.Length ~ Species, data = iris)

# Display the ANOVA summary
cat("One-Way ANOVA: Comparing Sepal Length Across Iris Species\n")
print(summary(one_way_anova))

# Visualize the data
boxplot(Sepal.Length ~ Species, data = iris,
        main = "Sepal Length by Species",
        xlab = "Species",
        ylab = "Sepal Length (cm)",
        col = c("lightpink", "lightblue", "lightgreen"))

# Add the means to the boxplot
species_means <- tapply(iris$Sepal.Length, iris$Species, mean)
points(1:3, species_means, pch = 19, col = "red", cex = 1.2)

# Post-hoc analysis: Tukey's HSD test
cat("\nTukey's HSD Test for Multiple Comparisons:\n")
tukey_result <- TukeyHSD(one_way_anova)
print(tukey_result)

# Plot the Tukey HSD results
plot(tukey_result, las = 1)

# Example 2: Two-Way ANOVA
# We'll use the mtcars dataset to examine the effects of
# number of cylinders (cyl) and transmission type (am) on mpg

# First, ensure the factors are properly formatted
mtcars$cyl_f <- as.factor(mtcars$cyl)
mtcars$am_f <- as.factor(mtcars$am)
levels(mtcars$am_f) <- c("Automatic", "Manual")

# Perform the Two-Way ANOVA
two_way_anova <- aov(mpg ~ cyl_f * am_f, data = mtcars)

# Display the ANOVA summary
cat("\nTwo-Way ANOVA: Effect of Cylinders and Transmission on MPG\n")
print(summary(two_way_anova))

# Create an interaction plot
interaction.plot(mtcars$cyl_f, mtcars$am_f, mtcars$mpg,
                 legend = TRUE,
                 type = "b",
                 col = c("red", "blue"),
                 pch = c(19, 21),
                 main = "Interaction Plot: Cylinders × Transmission on MPG",
                 xlab = "Number of Cylinders",
                 ylab = "Mean Miles Per Gallon",
                 trace.label = "Transmission Type")

# Creating a more detailed visualization with boxplots
par(mfrow = c(1, 1))
boxplot(mpg ~ am_f * cyl_f, data = mtcars,
        col = c("lightpink", "lightblue"),
        main = "MPG by Transmission Type and Number of Cylinders",
        xlab = "Transmission Type and Cylinders",
        ylab = "Miles Per Gallon")

# Check ANOVA assumptions
# 1. Normality of residuals
par(mfrow = c(2, 2))
plot(one_way_anova)
par(mfrow = c(1, 1))

# 2. Homogeneity of variances - Levene's test
install.packages("car")  # If not already installed
library(car)
leveneTest(Sepal.Length ~ Species, data = iris)

# Non-parametric alternative to ANOVA
# Kruskal-Wallis test (when ANOVA assumptions are violated)
kruskal_result <- kruskal.test(Sepal.Length ~ Species, data = iris)
cat("\nKruskal-Wallis Test (Non-parametric alternative to ANOVA):\n")
print(kruskal_result)

# Effect Size for ANOVA
# Eta-squared calculation
# Eta-squared = SSbetween / SStotal
anova_summary <- summary(one_way_anova)
ss_between <- anova_summary[[1]]["Species", "Sum Sq"]
ss_total <- ss_between + anova_summary[[1]]["Residuals", "Sum Sq"]
eta_squared <- ss_between / ss_total
cat("\nEffect Size (Eta-squared):", round(eta_squared, 3), "\n")
cat("Interpretation: Approximately", round(eta_squared * 100), "% of the variance\n")
cat("in sepal length can be attributed to species differences.\n")

# Create a means plot with error bars
species_means <- tapply(iris$Sepal.Length, iris$Species, mean)
species_se <- tapply(iris$Sepal.Length, iris$Species,
                     function(x) sd(x)/sqrt(length(x)))

# Create a bar plot of means
bar_positions <- barplot(species_means,
                         col = c("lightpink", "lightblue", "lightgreen"),
                         main = "Mean Sepal Length by Species with 95% CI",
                         xlab = "Species",
                         ylab = "Mean Sepal Length (cm)",
                         ylim = c(0, max(species_means) + 3 * max(species_se)))

# Add error bars (95% confidence intervals)
arrows(bar_positions, species_means - 1.96 * species_se,
       bar_positions, species_means + 1.96 * species_se,
       code = 3, angle = 90, length = 0.1)

# Add sample size
text(bar_positions, 0.5,
     paste("n =", tapply(iris$Sepal.Length, iris$Species, length)),
     cex = 0.8)

# Add post-hoc test results
text(1, species_means[1] + 1.96 * species_se[1] + 0.2, "a", cex = 1.2)
text(2, species_means[2] + 1.96 * species_se[2] + 0.2, "b", cex = 1.2)
text(3, species_means[3] + 1.96 * species_se[3] + 0.2, "c", cex = 1.2)

# Add a note explaining the letters
text(2, max(species_means) + 3 * max(species_se) - 0.5,
     "Different letters indicate\nsignificantly different means\n(Tukey HSD, p < 0.05)",
     cex = 0.8)
]]>
        </input>
      </program>
      
      <exercise xml:id="ex-anova-practice">
        <title>ANOVA Practice</title>
        <statement>
          <p>Analyze and interpret the following ANOVA scenarios:</p>
        </statement>
        <exercise>
          <statement>
            <p>Researchers want to compare the effectiveness of three different fertilizers (A, B, and C) on plant growth. They randomly assign 30 plants to three groups (10 plants per fertilizer) and measure plant height (in cm) after four weeks. The ANOVA results show F(2, 27) = 5.32, p = 0.011. Interpret these results and explain what follow-up analyses should be conducted.</p>
          </statement>
          <answer>
            <p><strong>Interpretation of ANOVA Results:</strong></p>
            <p>The ANOVA results (F(2, 27) = 5.32, p = 0.011) indicate that there is a statistically significant difference in plant height among the three fertilizer groups. The p-value (0.011) is less than the conventional significance level of 0.05, which means we can reject the null hypothesis that all fertilizers produce the same mean plant height.</p>
            <p><strong>Key components of the results:</strong></p>
            <ul>
              <li><p>F(2, 27) indicates that there are 2 degrees of freedom for the fertilizer (between groups) and 27 degrees of freedom for the error (within groups)</p></li>
              <li><p>The F-value of 5.32 represents the ratio of between-group variance to within-group variance</p></li>
              <li><p>p = 0.011 indicates that there is only a 1.1% chance of observing differences this large or larger if all fertilizers had identical effects</p></li>
            </ul>
            
            <p><strong>Follow-up analyses needed:</strong></p>
            <ol>
              <li>
                <p><strong>Post-hoc tests:</strong> Since ANOVA only tells us that there is a significant difference somewhere among the groups but doesn't specify which groups differ from each other, we should conduct post-hoc tests. Tukey's HSD (Honestly Significant Difference) would be appropriate here to perform all pairwise comparisons while controlling for the family-wise error rate.</p>
              </li>
              <li>
                <p><strong>Effect size calculation:</strong> Calculate eta-squared or partial eta-squared to determine the magnitude of the fertilizer effect. This would tell us what percentage of the variance in plant height is explained by the fertilizer type.</p>
              </li>
              <li>
                <p><strong>Visual representation:</strong> Create boxplots or means plots with confidence intervals to visualize the differences between fertilizer groups.</p>
              </li>
              <li>
                <p><strong>Assumption checking:</strong> Verify that ANOVA assumptions are met by examining:
                  <ul>
                    <li><p>Normality of residuals (using Q-Q plots or Shapiro-Wilk test)</p></li>
                    <li><p>Homogeneity of variance (using Levene's test)</p></li>
                    <li><p>Independence of observations</p></li>
                  </ul>
                </p>
              </li>
            </ol>
            
            <p>These follow-up analyses would provide a comprehensive understanding of how the fertilizers differ in their effectiveness and the practical significance of these differences.</p>
          </answer>
        </exercise>
        <exercise>
          <statement>
            <p>Explain the difference between one-way ANOVA and two-way ANOVA, including when you would use each and the additional insights provided by two-way ANOVA.</p>
          </statement>
          <answer>
            <p><strong>One-Way ANOVA vs. Two-Way ANOVA</strong></p>
            
            <p><strong>One-Way ANOVA:</strong></p>
            <ul>
              <li><p><strong>Purpose:</strong> Examines the effect of a single categorical independent variable (factor) on a continuous dependent variable</p></li>
              <li><p><strong>Structure:</strong> Compares means across three or more groups/levels of one factor</p></li>
              <li><p><strong>Example:</strong> Comparing student test scores across three different teaching methods</p></li>
              <li><p><strong>When to use:</strong>
                <ul>
                  <li><p>When you're interested in the effect of just one factor</p></li>
                  <li><p>When your experimental design manipulates only one variable</p></li>
                  <li><p>When you want to know if any differences exist between several groups</p></li>
                </ul>
              </p></li>
              <li><p><strong>Statistical model:</strong> Y = μ + α<sub>i</sub> + ε</p></li>
              <li><p><strong>Output:</strong> Single F-statistic and p-value for the main effect of the factor</p></li>
            </ul>
            
            <p><strong>Two-Way ANOVA:</strong></p>
            <ul>
              <li><p><strong>Purpose:</strong> Examines the effects of two categorical independent variables and their interaction on a continuous dependent variable</p></li>
              <li><p><strong>Structure:</strong> Analyzes data categorized along two dimensions simultaneously</p></li>
              <li><p><strong>Example:</strong> Comparing student test scores across three teaching methods AND two different times of day</p></li>
              <li><p><strong>When to use:</strong>
                <ul>
                  <li><p>When your design includes two factors that might affect the outcome</p></li>
                  <li><p>When you suspect that the effect of one factor may depend on the level of another factor (interaction)</p></li>
                  <li><p>When you want to increase statistical power by accounting for more sources of variation</p></li>
                </ul>
              </p></li>
              <li><p><strong>Statistical model:</strong> Y = μ + α<sub>i</sub> + β<sub>j</sub> + (αβ)<sub>ij</sub> + ε</p></li>
              <li><p><strong>Output:</strong> F-statistics and p-values for:
                <ul>
                  <li><p>Main effect of factor A</p></li>
                  <li><p>Main effect of factor B</p></li>
                  <li><p>Interaction effect between factors A and B</p></li>
                </ul>
              </p></li>
            </ul>
            
            <p><strong>Additional Insights from Two-Way ANOVA:</strong></p>
            <ol>
              <li>
                <p><strong>Interaction Effects:</strong> The most valuable insight from two-way ANOVA is the ability to detect interaction effects, where the impact of one factor depends on the level of the other factor. This reveals complex relationships that one-way ANOVA cannot identify.</p>
              </li>
              <li>
                <p><strong>More Comprehensive Model:</strong> Two-way ANOVA provides a more complete picture of what influences the dependent variable by examining multiple factors simultaneously.</p>
              </li>
              <li>
                <p><strong>Increased Statistical Power:</strong> By accounting for variation due to a second factor, two-way ANOVA can reduce error variance, potentially making it easier to detect the effect of the primary factor of interest.</p>
              </li>
              <li>
                <p><strong>More Efficient Design:</strong> Two-way ANOVA allows for testing multiple hypotheses in a single experiment, which is more efficient than conducting separate one-way ANOVAs.</p>
              </li>
              <li>
                <p><strong>Control for Confounding:</strong> The second factor can act as a control variable, helping to account for variation that might otherwise mask the effect of the primary factor.</p>
              </li>
            </ol>
            
            <p>In summary, while one-way ANOVA is suitable for simpler designs focusing on a single factor, two-way ANOVA offers a more nuanced analysis that can reveal complex patterns and relationships between variables that would be missed in separate one-way analyses.</p>
          </answer>
        </exercise>
      </exercise>
    </section>
    
    <!-- Advanced Analysis Section -->
    <section xml:id="sec-advanced-analysis">
      <title>Advanced Analysis and Final Project Work</title>
      
      <!-- Prediction and modeling subsection -->
      <subsection xml:id="subsec-prediction-modeling">
        <title>Building simple predictive models</title>
        <p>Predictive modeling is a powerful technique for forecasting outcomes based on historical data. In this section, we will explore the fundamentals of building simple predictive models in R, focusing on practical implementation and interpretation.</p>
        
        <paragraphs>
          <title>Predictive Modeling Workflow</title>
          <p>The typical workflow for developing predictive models includes:</p>
          <ol>
            <li>
              <p><term>Problem Definition</term>: Clearly define what you want to predict and why</p>
            </li>
            <li>
              <p><term>Data Preparation</term>: Collect, clean, and preprocess the data</p>
            </li>
            <li>
              <p><term>Data Splitting</term>: Divide data into training and test sets</p>
            </li>
            <li>
              <p><term>Model Selection</term>: Choose appropriate modeling techniques</p>
            </li>
            <li>
              <p><term>Model Training</term>: Fit models to the training data</p>
            </li>
            <li>
              <p><term>Model Evaluation</term>: Assess performance on test data</p>
            </li>
            <li>
              <p><term>Model Deployment</term>: Implement the model for making predictions</p>
            </li>
          </ol>
        </paragraphs>
        
        <paragraphs>
          <title>Common Predictive Models</title>
          <p>Several types of models are useful for different prediction tasks:</p>
          <ul>
            <li>
              <p><term>Linear Regression</term>: Predicts continuous outcomes based on one or more predictor variables</p>
            </li>
            <li>
              <p><term>Logistic Regression</term>: Predicts binary outcomes (e.g., yes/no, success/failure)</p>
            </li>
            <li>
              <p><term>Decision Trees</term>: Splits data into branches based on feature values to make predictions</p>
            </li>
            <li>
              <p><term>Random Forests</term>: Combines multiple decision trees to improve prediction accuracy</p>
            </li>
            <li>
              <p><term>Support Vector Machines</term>: Finds optimal boundaries to separate different classes</p>
            </li>
            <li>
              <p><term>Neural Networks</term>: Models complex patterns using interconnected layers of nodes</p>
            </li>
          </ul>
        </paragraphs>
        
        <paragraphs>
          <title>Model Evaluation Metrics</title>
          <p>Different metrics are used to evaluate model performance depending on the prediction task:</p>
          <ul>
            <li>
              <p><term>For Regression Models</term>:
                <ul>
                  <li><p>Mean Squared Error (MSE): Average of squared differences between predicted and actual values</p></li>
                  <li><p>Root Mean Squared Error (RMSE): Square root of MSE, in the same units as the target variable</p></li>
                  <li><p>Mean Absolute Error (MAE): Average of absolute differences between predicted and actual values</p></li>
                  <li><p>R-squared: Proportion of variance in the dependent variable explained by the model</p></li>
                </ul>
              </p>
            </li>
            <li>
              <p><term>For Classification Models</term>:
                <ul>
                  <li><p>Accuracy: Proportion of correct predictions</p></li>
                  <li><p>Precision: Proportion of true positives among positive predictions</p></li>
                  <li><p>Recall: Proportion of true positives that are correctly identified</p></li>
                  <li><p>F1 Score: Harmonic mean of precision and recall</p></li>
                  <li><p>ROC Curve and AUC: Visualization and measure of classification performance</p></li>
                </ul>
              </p>
            </li>
          </ul>
        </paragraphs>
        
        <program interactive="webr" xml:id="prog-predictive-modeling">
          <input>
<![CDATA[
# Predictive Modeling Examples in R
# Set a seed for reproducibility
set.seed(123)

# Load necessary data
data(mtcars)

# Data preparation
# Let's predict mpg using weight and horsepower
# First, standardize the predictor variables
mtcars$wt_std <- scale(mtcars$wt)
mtcars$hp_std <- scale(mtcars$hp)

# Data splitting (70% training, 30% testing)
train_indices <- sample(1:nrow(mtcars), size = round(0.7 * nrow(mtcars)))
train_data <- mtcars[train_indices, ]
test_data <- mtcars[-train_indices, ]

cat("Data split into training (", nrow(train_data), " observations) and test (", 
    nrow(test_data), " observations) sets.\n\n")

# 1. Linear Regression Model
# --------------------------

cat("1. LINEAR REGRESSION MODEL\n")
cat("-------------------------\n\n")

# Train the model on the training data
lm_model <- lm(mpg ~ wt_std + hp_std, data = train_data)
summary(lm_model)

# Make predictions on the test data
lm_predictions <- predict(lm_model, newdata = test_data)

# Evaluate the model performance
lm_mse <- mean((test_data$mpg - lm_predictions)^2)
lm_rmse <- sqrt(lm_mse)
lm_mae <- mean(abs(test_data$mpg - lm_predictions))
lm_r2 <- 1 - sum((test_data$mpg - lm_predictions)^2) / sum((test_data$mpg - mean(test_data$mpg))^2)

cat("Model Performance on Test Data:\n")
cat("Mean Squared Error (MSE):", round(lm_mse, 3), "\n")
cat("Root Mean Squared Error (RMSE):", round(lm_rmse, 3), "\n")
cat("Mean Absolute Error (MAE):", round(lm_mae, 3), "\n")
cat("R-squared:", round(lm_r2, 3), "\n\n")

# Visualize actual vs. predicted values
plot(test_data$mpg, lm_predictions, 
     main = "Actual vs. Predicted MPG",
     xlab = "Actual MPG",
     ylab = "Predicted MPG",
     pch = 19,
     col = "blue")
abline(0, 1, col = "red", lwd = 2)  # Perfect prediction line
legend("topleft", 
       legend = c("Predictions", "Perfect Prediction Line"),
       col = c("blue", "red"),
       pch = c(19, NA),
       lty = c(NA, 1),
       lwd = c(NA, 2),
       bty = "n")

# Add prediction intervals to understand uncertainty
# Let's create a data frame with a range of weight values and average horsepower
new_data <- data.frame(
  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100),
  hp = mean(mtcars$hp)
)
# Standardize the predictors to match training
new_data$wt_std <- (new_data$wt - mean(mtcars$wt)) / sd(mtcars$wt)
new_data$hp_std <- (new_data$hp - mean(mtcars$hp)) / sd(mtcars$hp)

# Generate predictions with prediction intervals
prediction_intervals <- predict(lm_model, newdata = new_data, interval = "prediction", level = 0.95)

# Plot the predictions with intervals
plot(mtcars$wt, mtcars$mpg,
     main = "MPG vs. Weight with Prediction Intervals",
     xlab = "Weight (1000 lbs)",
     ylab = "Miles Per Gallon",
     pch = 19,
     col = "gray",
     cex = 0.8)

# Add the prediction line
lines(new_data$wt, prediction_intervals[, "fit"], col = "blue", lwd = 2)

# Add the prediction intervals
lines(new_data$wt, prediction_intervals[, "lwr"], col = "red", lty = 2)
lines(new_data$wt, prediction_intervals[, "upr"], col = "red", lty = 2)

# Add a legend
legend("topright", 
       legend = c("Data", "Predicted MPG", "95% Prediction Interval"),
       col = c("gray", "blue", "red"),
       pch = c(19, NA, NA),
       lty = c(NA, 1, 2),
       lwd = c(NA, 2, 1),
       bty = "n")

# 2. Decision Tree Model
# ---------------------

cat("\n2. DECISION TREE MODEL\n")
cat("--------------------\n\n")

# Note: Since we can't install additional packages in the web environment,
# we'll demonstrate the concept using simulated behavior

cat("In a full R environment, you would use the 'rpart' package to build a decision tree:\n\n")
cat("library(rpart)\n")
cat("tree_model <- rpart(mpg ~ wt + hp, data = train_data, method = 'anova')\n")
cat("tree_predictions <- predict(tree_model, newdata = test_data)\n\n")

# Simulate a decision tree prediction
# This is a simplified model, not a real tree
simulate_tree_prediction <- function(wt, hp) {
  if (wt < 3) {
    if (hp < 100) {
      return(28)
    } else {
      return(22)
    }
  } else {
    if (hp < 175) {
      return(18)
    } else {
      return(14)
    }
  }
}

# Apply the simulated prediction to test data
tree_predictions <- numeric(nrow(test_data))
for (i in 1:nrow(test_data)) {
  tree_predictions[i] <- simulate_tree_prediction(test_data$wt[i], test_data$hp[i])
}

# Evaluate the simulated tree model
tree_mse <- mean((test_data$mpg - tree_predictions)^2)
tree_rmse <- sqrt(tree_mse)
tree_mae <- mean(abs(test_data$mpg - tree_predictions))
tree_r2 <- 1 - sum((test_data$mpg - tree_predictions)^2) / sum((test_data$mpg - mean(test_data$mpg))^2)

cat("Simulated Decision Tree Performance on Test Data:\n")
cat("Mean Squared Error (MSE):", round(tree_mse, 3), "\n")
cat("Root Mean Squared Error (RMSE):", round(tree_rmse, 3), "\n")
cat("Mean Absolute Error (MAE):", round(tree_mae, 3), "\n")
cat("R-squared:", round(tree_r2, 3), "\n\n")

# Visualize simulated tree regions
# Create a grid of weight and horsepower values
wt_grid <- seq(min(mtcars$wt), max(mtcars$wt), length.out = 50)
hp_grid <- seq(min(mtcars$hp), max(mtcars$hp), length.out = 50)
grid_data <- expand.grid(wt = wt_grid, hp = hp_grid)

# Apply the simulated tree model to the grid
grid_predictions <- numeric(nrow(grid_data))
for (i in 1:nrow(grid_data)) {
  grid_predictions[i] <- simulate_tree_prediction(grid_data$wt[i], grid_data$hp[i])
}

# Plot the decision regions
# Create a color palette for the predictions
pred_colors <- heat.colors(10)[cut(grid_predictions, breaks = 10, include.lowest = TRUE)]

plot(grid_data$wt, grid_data$hp, 
     col = pred_colors,
     pch = 15,
     cex = 0.5,
     main = "Simulated Decision Tree Regions",
     xlab = "Weight (1000 lbs)",
     ylab = "Horsepower")

# Add the actual data points
points(mtcars$wt, mtcars$hp, 
       col = "black",
       bg = heat.colors(10)[cut(mtcars$mpg, breaks = 10, include.lowest = TRUE)],
       pch = 21,
       cex = 1.5)

# 3. Model Comparison
# ------------------

cat("\n3. MODEL COMPARISON\n")
cat("------------------\n\n")

cat("Comparing Linear Regression and Simulated Decision Tree:\n\n")
cat("                      Linear Regression    Decision Tree (Simulated)\n")
cat("Mean Squared Error    ", sprintf("%-20.3f", lm_mse), tree_mse, "\n")
cat("Root Mean Sq Error    ", sprintf("%-20.3f", lm_rmse), tree_rmse, "\n")
cat("Mean Absolute Error   ", sprintf("%-20.3f", lm_mae), tree_mae, "\n")
cat("R-squared             ", sprintf("%-20.3f", lm_r2), tree_r2, "\n\n")

# Plot both models' predictions side by side
par(mfrow = c(1, 2))

# Linear regression predictions
plot(test_data$mpg, lm_predictions, 
     main = "Linear Regression",
     xlab = "Actual MPG",
     ylab = "Predicted MPG",
     pch = 19,
     col = "blue",
     xlim = c(min(test_data$mpg), max(test_data$mpg)),
     ylim = c(min(test_data$mpg), max(test_data$mpg)))
abline(0, 1, col = "red", lwd = 2)  # Perfect prediction line

# Decision tree predictions
plot(test_data$mpg, tree_predictions, 
     main = "Decision Tree (Simulated)",
     xlab = "Actual MPG",
     ylab = "Predicted MPG",
     pch = 19,
     col = "green",
     xlim = c(min(test_data$mpg), max(test_data$mpg)),
     ylim = c(min(test_data$mpg), max(test_data$mpg)))
abline(0, 1, col = "red", lwd = 2)  # Perfect prediction line

# Reset the plotting layout
par(mfrow = c(1, 1))

# 4. Making Predictions with the Model
# -----------------------------------

cat("\n4. MAKING PREDICTIONS WITH THE MODEL\n")
cat("-----------------------------------\n\n")

cat("Let's use our linear regression model to make predictions for new cars:\n\n")

# Create a few example cars
new_cars <- data.frame(
  car_name = c("Compact Sedan", "Sports Car", "SUV", "Electric Sedan"),
  wt = c(2.2, 3.4, 4.2, 2.8),
  hp = c(120, 300, 210, 240)
)

# Standardize the variables using the same means and standard deviations from the original data
new_cars$wt_std <- (new_cars$wt - mean(mtcars$wt)) / sd(mtcars$wt)
new_cars$hp_std <- (new_cars$hp - mean(mtcars$hp)) / sd(mtcars$hp)

# Make predictions
new_cars$predicted_mpg <- predict(lm_model, newdata = new_cars)

# Display the predictions
cat("Predictions for new cars:\n")
print(new_cars[, c("car_name", "wt", "hp", "predicted_mpg")])

cat("\nInterpretation of predicted MPG values:\n")
for (i in 1:nrow(new_cars)) {
  cat("- The", new_cars$car_name[i], "with weight of", new_cars$wt[i], 
      "thousand lbs and", new_cars$hp[i], "horsepower is predicted to have",
      round(new_cars$predicted_mpg[i], 1), "MPG.\n")
}

# Calculate prediction intervals for the new predictions
prediction_intervals_new <- predict(lm_model, newdata = new_cars, 
                                   interval = "prediction", level = 0.95)

cat("\nPredictions with 95% prediction intervals:\n")
print(cbind(new_cars[, c("car_name")], round(prediction_intervals_new, 1)))

cat("\nInterpretation of prediction intervals:\n")
cat("For example, we're 95% confident that the actual MPG for the Electric Sedan\n")
cat("will be between", round(prediction_intervals_new[4, "lwr"], 1), "and", 
    round(prediction_intervals_new[4, "upr"], 1), "MPG.\n\n")

cat("The wide prediction intervals reflect uncertainty in the model and natural\n")
cat("variability in fuel efficiency even among cars with the same characteristics.\n\n")

# 5. Key considerations in predictive modeling
# ------------------------------------------

cat("\n5. KEY CONSIDERATIONS IN PREDICTIVE MODELING\n")
cat("------------------------------------------\n\n")

cat("When building and using predictive models, remember these important points:\n\n")

cat("1. Model Selection: Choose the simplest model that adequately explains the data.\n")
cat("   Our linear regression model may be preferable to more complex models if the\n")
cat("   performance is similar, due to its interpretability and simplicity.\n\n")

cat("2. Overfitting vs. Underfitting: Complex models might perform well on training\n")
cat("   data but fail to generalize to new data (overfitting). Simple models might\n")
cat("   miss important patterns (underfitting).\n\n")

cat("3. Feature Engineering: Creating new features or transforming existing ones\n")
cat("   can significantly improve model performance. For example, we might consider\n")
cat("   adding an interaction term (wt × hp) to our model.\n\n")

cat("4. Cross-Validation: Instead of a single train-test split, k-fold cross-validation\n")
cat("   provides a more robust assessment of model performance, especially with small datasets.\n\n")

cat("5. Model Interpretability: Simpler models like linear regression offer clearer\n")
cat("   insights into how features affect the outcome, which can be valuable for\n")
cat("   decision-making and communication.\n\n")

cat("6. Prediction Uncertainty: Always communicate the uncertainty in predictions,\n")
cat("   using prediction intervals or similar measures.\n\n")

cat("7. Data Quality: The quality of predictions depends heavily on the quality,\n")
cat("   representativeness, and completeness of the training data.\n\n")
]]>
          </input>
        </program>
        
        <exercise xml:id="ex-predictive-modeling">
          <title>Predictive Modeling Practice</title>
          <statement>
            <p>Apply your knowledge of predictive modeling to address the following questions:</p>
          </statement>
          <exercise>
            <statement>
              <p>A researcher wants to predict student exam scores based on hours studied and previous GPA. They collected data from 100 students and want to build a linear regression model. What steps should they follow to build and validate their model? What metrics should they use to evaluate its performance?</p>
            </statement>
            <answer>
              <p><strong>Steps to Build and Validate a Linear Regression Model for Predicting Exam Scores:</strong></p>
              
              <ol>
                <li>
                  <p><strong>Data Preparation</strong></p>
                  <ul>
                    <li><p>Check for missing values and outliers in hours studied, GPA, and exam scores</p></li>
                    <li><p>Explore data distributions with histograms and scatter plots</p></li>
                    <li><p>Check correlations between predictors (hours studied and GPA) and the outcome (exam scores)</p></li>
                    <li><p>Consider transformations if relationships appear non-linear</p></li>
                    <li><p>Standardize variables if they're on different scales</p></li>
                  </ul>
                </li>
                <li>
                  <p><strong>Data Splitting</strong></p>
                  <ul>
                    <li><p>Split the data into training (e.g., 70-80%) and testing (e.g., 20-30%) sets</p></li>
                    <li><p>Use random sampling but ensure both sets represent the full range of student characteristics</p></li>
                    <li><p>Alternatively, use k-fold cross-validation (typically 5-10 folds) for more robust validation</p></li>
                  </ul>
                </li>
                <li>
                  <p><strong>Model Building</strong></p>
                  <ul>
                    <li><p>Build the linear regression model: ExamScore = β₀ + β₁(HoursStudied) + β₂(GPA) + ε</p></li>
                    <li><p>Fit the model using the training data</p></li>
                    <li><p>Examine coefficient values and significance</p></li>
                    <li><p>Consider interaction terms (e.g., HoursStudied × GPA) if theoretically justified</p></li>
                  </ul>
                </li>
                <li>
                  <p><strong>Model Diagnostics</strong></p>
                  <ul>
                    <li><p>Check linearity assumption using residual plots</p></li>
                    <li><p>Verify homoscedasticity (constant variance of residuals)</p></li>
                    <li><p>Check normality of residuals using Q-Q plots or statistical tests</p></li>
                    <li><p>Look for influential points or outliers using leverage and Cook's distance</p></li>
                  </ul>
                </li>
                <li>
                  <p><strong>Model Validation</strong></p>
                  <ul>
                    <li><p>Apply the model to the test dataset</p></li>
                    <li><p>Compare predicted vs. actual exam scores</p></li>
                    <li><p>Calculate performance metrics (see below)</p></li>
                    <li><p>If using cross-validation, average results across all folds</p></li>
                  </ul>
                </li>
                <li>
                  <p><strong>Model Refinement</strong></p>
                  <ul>
                    <li><p>Consider alternative models (e.g., with transformed variables or additional predictors)</p></li>
                    <li><p>Compare models using appropriate criteria</p></li>
                    <li><p>Select the final model based on both statistical performance and interpretability</p></li>
                  </ul>
                </li>
                <li>
                  <p><strong>Interpretation and Reporting</strong></p>
                  <ul>
                    <li><p>Interpret coefficient values (e.g., "One additional hour of studying is associated with an increase of X points in exam score, holding GPA constant")</p></li>
                    <li><p>Generate confidence intervals for coefficients</p></li>
                    <li><p>Create prediction intervals for new observations</p></li>
                    <li><p>Communicate limitations of the model</p></li>
                  </ul>
                </li>
              </ol>
              
              <p><strong>Performance Metrics to Evaluate the Model:</strong></p>
              
              <ol>
                <li>
                  <p><strong>R-squared (R²)</strong>: Proportion of variance in exam scores explained by the model. Higher values (closer to 1) indicate better fit, but this should not be the only criterion.</p>
                </li>
                <li>
                  <p><strong>Adjusted R-squared</strong>: Modification of R² that adjusts for the number of predictors. Useful when comparing models with different numbers of variables.</p>
                </li>
                <li>
                  <p><strong>Root Mean Square Error (RMSE)</strong>: Square root of the average squared differences between predicted and actual exam scores. Lower values indicate better fit. This metric is in the same units as the exam scores, making it interpretable.</p>
                </li>
                <li>
                  <p><strong>Mean Absolute Error (MAE)</strong>: Average of absolute differences between predicted and actual exam scores. Like RMSE, lower values indicate better fit, and it's in the same units as the outcome.</p>
                </li>
                <li>
                  <p><strong>Prediction Accuracy within Ranges</strong>: Percentage of predictions within ±5 or ±10 points of actual scores (or other relevant thresholds).</p>
                </li>
                <li>
                  <p><strong>Training vs. Test Performance Comparison</strong>: Similar performance on both sets suggests good generalizability; much better performance on training data suggests overfitting.</p>
                </li>
              </ol>
              
              <p>By following these steps and evaluating the model using multiple metrics, the researcher can develop a reliable predictive model for exam scores and understand its strengths and limitations.</p>
            </answer>
          </exercise>
          <exercise>
            <statement>
              <p>Compare and contrast linear regression and decision trees as predictive modeling approaches. Under what circumstances would you recommend using one over the other?</p>
            </statement>
            <answer>
              <p><strong>Comparison of Linear Regression and Decision Trees</strong></p>
              
              <table>
                <tabular>
                  <row header="yes">
                    <cell>Aspect</cell>
                    <cell>Linear Regression</cell>
                    <cell>Decision Trees</cell>
                  </row>
                  <row>
                    <cell>Underlying Assumption</cell>
                    <cell>Assumes linear relationship between predictors and outcome</cell>
                    <cell>No assumption of linearity; can capture non-linear relationships</cell>
                  </row>
                  <row>
                    <cell>Model Structure</cell>
                    <cell>Single equation with coefficients</cell>
                    <cell>Hierarchical structure with nodes and branches</cell>
                  </row>
                  <row>
                    <cell>Handling Interactions</cell>
                    <cell>Must be explicitly defined by the analyst</cell>
                    <cell>Automatically captures interactions through tree structure</cell>
                  </row>
                  <row>
                    <cell>Interpretability</cell>
                    <cell>Highly interpretable; coefficients show direction and magnitude of effect</cell>
                    <cell>Moderately interpretable; decision rules can be followed</cell>
                  </row>
                  <row>
                    <cell>Outlier Sensitivity</cell>
                    <cell>Sensitive to outliers</cell>
                    <cell>Relatively robust to outliers</cell>
                  </row>
                  <row>
                    <cell>Handling Categorical Variables</cell>
                    <cell>Requires dummy coding</cell>
                    <cell>Natively handles categorical variables</cell>
                  </row>
                  <row>
                    <cell>Handling Missing Data</cell>
                    <cell>Cannot directly handle missing data</cell>
                    <cell>Can work with missing data using surrogate splits</cell>
                  </row>
                  <row>
                    <cell>Computational Complexity</cell>
                    <cell>Low; computationally efficient</cell>
                    <cell>Moderate to high, depending on tree depth and dataset size</cell>
                  </row>
                  <row>
                    <cell>Risk of Overfitting</cell>
                    <cell>Lower risk of overfitting</cell>
                    <cell>Higher risk of overfitting, especially with deep trees</cell>
                  </row>
                </tabular>
              </table>
              
              <p><strong>When to Use Linear Regression:</strong></p>
              
              <ol>
                <li><p><strong>Known Linear Relationships:</strong> When theory or exploratory analysis suggests a linear relationship between predictors and outcome</p></li>
                <li><p><strong>Inference Priority:</strong> When understanding the magnitude and direction of each predictor's effect is the primary goal</p></li>
                <li><p><strong>Small Sample Size:</strong> When working with limited data where complex models might overfit</p></li>
                <li><p><strong>Normally Distributed Data:</strong> When the assumptions of linear regression (linearity, normality, homoscedasticity) are reasonably satisfied</p></li>
                <li><p><strong>Extrapolation Needs:</strong> When predictions beyond the range of training data are required (though caution is still needed)</p></li>
                <li><p><strong>Simple Deployment:</strong> When a straightforward mathematical formula is preferred for implementation</p></li>
                <li><p><strong>Academic/Scientific Context:</strong> When statistically sound confidence intervals and hypothesis tests are required</p></li>
              </ol>
              
              <p><strong>When to Use Decision Trees:</strong></p>
              
              <ol>
                <li><p><strong>Complex, Non-linear Relationships:</strong> When relationships between variables are not linear and may involve thresholds or interactions</p></li>
                <li><p><strong>Mixed Data Types:</strong> When working with a mix of numerical and categorical predictors</p></li>
                <li><p><strong>Missing Data:</strong> When the dataset contains missing values that cannot be easily imputed</p></li>
                <li><p><strong>Outliers Present:</strong> When the data contains extreme values that would disproportionately influence linear models</p></li>
                <li><p><strong>Rule-based Understanding:</strong> When the goal is to develop a set of decision rules (e.g., "if X &gt; 10 and Y &lt; 5, then predict Z")</p></li>
                <li><p><strong>No Distributional Assumptions:</strong> When data violates the assumptions required for linear regression</p></li>
                <li><p><strong>Feature Importance Analysis:</strong> When identifying the most influential predictors is important</p></li>
                <li><p><strong>Segmentation Goals:</strong> When dividing the population into distinct groups is valuable</p></li>
              </ol>
              
              <p><strong>Hybrid Approaches and Considerations:</strong></p>
              
              <p>In many real-world scenarios, a combination or comparison of both approaches may be optimal:</p>
              
              <ul>
                <li><p>Start with linear regression as a baseline and compare performance with decision trees</p></li>
                <li><p>Use decision trees for feature selection or to identify potential interaction terms, then incorporate these insights into a linear model</p></li>
                <li><p>Consider ensemble methods like random forests (multiple decision trees) for improved prediction accuracy while maintaining many decision tree advantages</p></li>
                <li><p>For time-series or spatial data, specialized versions of these models or entirely different approaches may be necessary</p></li>
              </ul>
              
              <p>The final choice should consider the specific dataset characteristics, research or business objectives, interpretability requirements, and computational constraints.</p>
            </answer>
          </exercise>
        </exercise>
      </subsection>
      
      <!-- Interpreting model results subsection -->
      <subsection xml:id="subsec-interpreting-models">
        <title>Interpreting model results</title>
        <p>Coming soon...</p>
      </subsection>
    </section>
  </section>
</chapter>